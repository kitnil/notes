:PROPERTIES:
:ID:       6b80aed2-b76c-4d92-98d3-1491429cb6e4
:END:
- [[https://www.youtube.com/watch?v=SfYaAQ9-RnE][(97) 1. –ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. –í–≤–µ–¥–µ–Ω–∏–µ | –¢–µ—Ö–Ω–æ—Å—Ç—Ä–∏–º - YouTube]]
- [[https://github.com/amacneil/dbmate][amacneil/dbmate: A lightweight, framework-agnostic database migration tool.]]
- [[https://github.com/arangodb/arangodb][arangodb/arangodb: ü•ë ArangoDB is a native multi-model database with flexible data models for documents, graphs, and key-values. Build high performance applications using a convenient SQL-like query language or JavaScript extensions.]]
- [[https://github.com/CeresDB/ceresdb][CeresDB/ceresdb: CeresDB is a high-performance, distributed, schema-less, cloud native time-series database that can handle both time-series and analytics workloads.]]
- [[https://github.com/codenotary/immudb][codenotary/immudb: immudb - immutable database based on zero trust, SQL and Key-Value, tamperproof, data change history]]
- [[https://mickael.kerjean.me/2016/04/08/database-cheat-sheet/][Database Cheat Sheet]]
- [[https://github.com/datazip-inc/olake][datazip-inc/olake: Fastest open-source tool for replicating Databases to Data Lake in Open Table Formats like Apache Iceberg. ‚ö° Efficient, quick and scalable data ingestion for real-time analytics. Supporting Postgres, MongoDB , MySQL and Oracle]]
- [[https://github.com/dbgate/dbgate][dbgate/dbgate: Database manager for MySQL, PostgreSQL, SQL Server, MongoDB, SQLite and others. Runs under Windows, Linux, Mac or as web application]]
- [[https://holistic.dev/][DB OPTIMIZATION SERVICE - Holistic.dev]]
- [[https://github.com/delta-io/delta][delta-io/delta: An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.]]
- [[https://github.com/dolthub/dolt][dolthub/dolt: Dolt ‚Äì It's Git for Data]]
- [[https://github.com/elkirrs/dumper][elkirrs/dumper: Dumper ‚Äî This is a CLI utility for creating backups databases of various types (PostgreSQL, MySQL and etc.)]]
- [[https://github.com/grepTimeTeam/greptimedb/][GreptimeTeam/greptimedb: An open-source, cloud-native, distributed time-series database with PromQL/SQL/Python supported.]]
- [[https://github.com/k1LoW/tbls][k1LoW/tbls: tbls is a CI-Friendly tool for document a database, written in Go.]]
- [[https://github.com/liweiyi88/onedump][liweiyi88/onedump: database dump with one command and configuration.]]
- [[https://github.com/mbucc/shmig][mbucc/shmig: Database migration tool written in BASH.]]
- [[https://github.com/pingcap/awesome-database-learning][pingcap/awesome-database-learning: A list of learning materials to understand databases internals]]
- [[https://github.com/Qovery/Replibyte][Qovery/Replibyte: Seed your development database with real data]]
- [[https://github.com/flower-corp/rosedb/discussions][rosedb - high performance NoSQL database based on bitcask, supports string, list, hash, set, and sorted set]]
- [[https://github.com/tarantool/tarantool][tarantool/tarantool: Get your data in RAM. Get compute close to data. Enjoy the performance.]]
- [[https://github.com/tigerbeetle/tigerbeetle][tigerbeetle/tigerbeetle: The financial transactions database designed for mission critical safety and performance.]]
- [[https://github.com/ydb-platform/ydb][ydb-platform/ydb: YDB server (daemon) source code]]
- [[https://habr.com/ru/company/arenadata/blog/576418/][–§–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã –≤ Greenplum - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –°–£–ë–î, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ PostgreSQL –∏ –∑–∞—Ç–æ—á–µ–Ω–Ω–æ–π –ø–æ–¥ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å –Ω–∞–≥—Ä—É–∑–∫–∏ / –•–∞–±—Ä]]

* Learning
- [[https://fosdem.org/2023/schedule/event/db/][FOSDEM 2023 - Monitor your databases with Open Source tools]]
- [[https://en.wikipedia.org/wiki/Object%E2%80%93relational_mapping][Object‚Äìrelational mapping - Wikipedia]]
  - [[https://habr.com/ru/articles/667078/][ORM ‚Äî –æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω—Ç–∏-–ø–∞—Ç—Ç–µ—Ä–Ω / –•–∞–±—Ä]]
  - [[http://citforum.ru/database/articles/vietnam/][–í—å–µ—Ç–Ω–∞–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–π –Ω–∞—É–∫–∏]]
- [[https://github.com/lonng/db-papers][lonng/db-papers: A list papers of learning how to building database system]]

* REST
- [[https://github.com/exist-db/exist/][eXist-db/exist: eXist Native XML Database and Application Platform]]
- [[https://github.com/openlink/virtuoso-opensource][openlink/virtuoso-opensource: Virtuoso is a high-performance and scalable Multi-Model RDBMS, Data Integration Middleware, Linked Data Deployment, and HTTP Application Server Platform]]
- [[https://github.com/PostgREST/postgrest][PostgREST/postgrest: REST API for any Postgres database]]

* [[https://github.com/datafold/data-diff][datafold/data-diff: Compare tables within or across databases]]

Validate data replication pipelines with data-diff

Learn to replicate data from Postgres to Snowflake with Airbyte, and compare replicated data with data-diff.
Madison Schott
Published on Nov 25, 2022
What is data-diff?
Replicate data from Postgres to Snowflake with Airbyte
Validate the replication process with data-diff
Conclusion 

There are many different reasons why you may want to replicate your data. I‚Äôve personally replicated databases in order to move them to a different region, use them as a testing environment and combine it with other sources in a data warehouse. The reasons are endless! 

With data replication often comes a painful validation process. You need to ensure your data was replicated accurately and all of it is securely stored in the new location. Data validation is the process of ensuring that the replicated data is correct and complete in the destination. Validation may look like writing aggregate queries to check whether certain data groups match or directly comparing values row by row. As an analytics engineer, this is probably one of my least favorite parts of the job. It‚Äôs tedious and time-consuming and doesn‚Äôt guarantee you won‚Äôt miss something important. 

Often times something during the replication process can cause source data and its destination to not completely sync. I‚Äôve seen the following occur:

    replication jobs using incremental replication halt midway leading to data missing in the destination
    replication jobs where changes in data at the source aren‚Äôt reflected downstream, causing major discrepancies

Luckily, there is now an easier way to ensure these problems don‚Äôt go undetected.  
What is data-diff?

Data-diff is an open-source tool created by Datafold that allows you to compare tables between relational databases and data warehouses. It efficiently compares all of your rows for you, without you having to do so manually. It does so by splitting the tables in each database into small segments and then checksumming each segment. When the checksums aren‚Äôt equal, it divides the segment into smaller segments, checksumming those until getting to the differing row(s).

I recently started using it in replace of my manual validation process and it‚Äôs saved me hours' worth of work. Not to mention it‚Äôs way more accurate! Because of the checksum approach, its performance has a magnitude of count(*) when there are little or no variations between tables while still outputting the differing rows. Data-diff catches discrepancies in your datasets before they have the opportunity to manifest downstream. 
Replicate data from Postgres to Snowflake with Airbyte

In this article, I‚Äôm going to walk through how to replicate data from a Postgres database hosted in AWS to Snowflake using Airbyte. This is common when wanting a copy of a database for analytics purposes when you don‚Äôt want to slow down database performance. 

First, we are going to start by setting up our Postgres source and Snowflake destination within Airbyte.
Set up your Postgres source

On Airbyte Cloud or Airbyte Open Source, click ‚Äúnew connection‚Äù. This will bring you to a screen where you can select your data source. Choose ‚ÄúPostgres‚Äù as your source type.

Now you will be brought to a screen where you need to enter some specific information about your Postgres database. This includes host, port, database name, and a list of the schemas you wish to sync. 

I kept the default port and added my database named `development`, `customers` schema, and the login information for my Airbyte user. It is best practice to create users specific to the tools you are connecting to your database.
Set up your Snowflake destination

Now let‚Äôs set up our Snowflake destination where we will be replicating our Postgres data to. Start by clicking on ‚Äúnew destination‚Äù in the top right corner. Then select ‚ÄúSnowflake‚Äù as your destination type.

‚Äç

This is where you will input the information for the Snowflake database that you are copying your Postgres data. Make sure you enter the right location information! 

I also recommend setting up a role that is specific for loading data in your destination as well. This will help keep your environment secure and all you to closely monitor different metrics on the replication process.
Set up a Postgres to Snowflake connection

Now that you‚Äôve created both your source in Postgres and your destination in Snowflake, you can set up a connection between the two to replicate your data from Postgres. Select ‚Äúconnections‚Äù on the left panel.

Select your Postgres source you created from the dropdown, then select Snowflake as your destination.

Now you‚Äôll want to give the connection a good name and choose how often it replicates. I‚Äôm going to call mine ‚Äúpostgres_snowflake_replication‚Äù and set it t replicate every 24 hours. 

I also recommend selecting ‚Äúmirror source structure‚Äù for the ‚Äúdestination namespace‚Äù. This will allow you to easily compare the differences between the source table and the destination table. Your data will be replicated to the database ‚Äúdevelopment‚Äù and the schema ‚Äúcustomers‚Äù, with the same table names. If the naming convention were different, it may get confusing down the line. 

When you choose the streams you wish to activate, be sure that you are selecting ‚Äúfull refresh | overwrite‚Äù. This will capture any deletes or updates in your source data table through the replication process. 

Select ‚ÄúNormalized tabular data‚Äù and create your connection. Once you set up your connection, you should see your data syncing. 

‚Äç

When your connection is finished syncing and reads ‚Äúsuccessful‚Äù, you are ready to begin the validation process! 
Validate the replication process with data-diff

Now that you have your Postgres database replicated to Snowflake, you want to validate that your data looks as expected. Normally you would have to go through a long process of performing different discovery queries on each data source and comparing values. And, unless you looked at every single row and did a row-by-row comparison, you wouldn‚Äôt be confident that your data replicated as expected. This is where data-diff comes in. Let‚Äôs walk through how to set this up.

First, you‚Äôll want to install the open-source tool. You can do this with the following command:


pip install data-diff

Since you are using both Postgres and Snowflake, you want to install the drivers for these specific databases:


pip install 'data-diff[postgresql]'
pip install 'data-diff[snowflake]'

After you‚Äôve installed data-diff, you‚Äôll have to focus on creating your connection strings. These contain key information about the database you are connecting to like the database user, password, host, account, database, schema, warehouse, and role. 

For Snowflake, your connection string will follow the following format:


"snowflake://[username]:[password]@[account]/[DATABASE]/[SCHEMA]?warehouse=[WAREHOUSE]&role=[ROLE]"

Because the table I replicated into Snowflake is in the `development` database and `customers` schema, I‚Äôll input this information into the string. I‚Äôll also be using `external_user` for my user, the `validator_wh` for my warehouse, and the `validator‚Äô role here. These are different from what I used in the loading process since this is a different step in the data pipeline. 

Make sure whatever role you are using has the appropriate permissions to access the table you are comparing! The user you are inputting in the string also has to be assigned the role you specified. 

My final connection string looks like so:


"snowflake://external_user:[password]@[account]/development/customers?warehouse=validator_wh&role=validator"

For Postgres, the connection string will be formatted like so:


postgresql://[username]:[password]@localhost:5432/[database]

I‚Äôm using the username and password that I typically use to connect to my Postgres database. Unfortunately, this is a personal username and password which is not a best practice. It‚Äôs always best to use a user that is created for the use of external tools like data-diff. This helps keep your databases secure and ensures you are always in tight control of access.

My final connection string looks like so:


postgresql://madison:[password]@localhost:5432/development

Now that you‚Äôve built your connection strings, you can use them in the data-diff command, which looks like this:


data-diff DB1_URI TABLE1_NAME DB2_URI TABLE2_NAME

You will input Snowflake‚Äôs connection string as DB1_URI and Postgres‚Äôs as DB2_URI. 

Although I‚Äôm choosing to do Snowflake first and Postges second, the order doesn‚Äôt matter. Then be sure to write the appropriate table names for each of the sources that you are comparing. Note that data-diff only allows you to compare two tables at one time rather than all of your tables in a schema. However, you could easily create a Python script using data-diff Python‚Äôs SDK that loops through a list of table names, running the command for every pair of tables specified and outputting the results in a nicely formatted CSV file. 

Let‚Äôs start by comparing the `customer_contacts` table in our customers schemas. Because the source and destination have the same table names, I would put `customer_contacts` as my Snowflake table and `customer_contacts` as my Postgres table. 

The command would look like this:


data-diff \
"snowflake://external_user:[password]@[account]/development/customers?warehouse=validator_wh&role=validator" customer_contacts \
"postgresql://madison:[password]@localhost:5432/development" customer_contacts

Because we are activating Airbyte normalization and Airbyte creates extra metadata columns (_airbyte_ab_id, _airbyte_emitted_at, _airbyte__hashid, airbyte_normalized_at), on the Snowflake destination, we will have to specify the specific column names that we wish to compare. Otherwise, data-diff would mark all rows as varying due to the differing columns. In order to specify columns to include, we need to use the flag `columns` (or just `c`) in the command.


data-diff \
"snowflake://external_user:[password]@[account]/development/customers?warehouse=validator_wh&role=validator" customer_contacts \
"postgresql://madison'[password]@localhost:5432/development" customer_contacts \
-c customer_contact_id customer_name phone_number email_address updated_at

Another thing to note, if your primary keys are not `id`, then you‚Äôll have to specify another parameter called `key column` in your connection string. This just lets data-diff know which column acts as a primary key, allowing the two tables to be compared using that.

Since the primary key of these tables is `customer_contact_id` I would add that to the `key-columns` (or just `k`)  flag in my command. Like this:


data-diff \
"snowflake://external_user:[password]@[password]/development/customers?warehouse=validator_wh&role=validator" customer_contacts \
"postgresql://madison:[password]@localhost:5432/development" customer_contacts \
-columns customer_contact_id customer_name phone_number email_address updated_at \
-key-columns customer_contact_id

And, lastly, if you ever want to filter the column values that you are comparing, you can specify a `where` (or just `w`) flag in the command. This acts as a where clause for your query, allowing you to filter for certain conditions. This is particularly helpful if you have a fast-changing source and need to validate a new batch of data that has been ingested after a previous validation check. Using this will prevent false positives of rows that have been updated since being ingested when checking for differing rows. We can add this to our command like so:


data-diff \
"snowflake://external_user:[password]@[account]/development/customers?warehouse=validator_wh&role=validator" customer_contacts \
"postgresql://madison:[password]@localhost:5432/development" customer_contacts \
-columns customer_contact_id customer_name phone_number email_address updated_at \
-key-columns customer_contact_id \
-where ‚Äúupdated_at <= '10-31-2022'"

Now we are ready to run data-diff!
Understanding data-diff output

Once you run the command, data-diff will give you the primary key values of the rows in the tables that differ. Each value will have a + or - in front of it. A + signifies that the rows exists in the second table but not the first. A - sign indicates that the row exists in the first table but not the second. Now, you can dig deeper into your validation process and explore why these primary keys are in one database and not the other.

Also, note that one primary key can be present with a + and a - sign in the output. This means that the row exists in both tables but has a column value that varies. So not only does data-diff let you know when one table is missing a primary key, but it also lets you know when the column values for the rows with that primary key differ. 

I can see here that the row with a `customer_contact_id` of 14339589 is present in Postgres but not Snowflake. I‚Äôll want to look into why this row wasn‚Äôt replicated from my source table to my destination. This will involve a deeper dive into your data and understanding its behavior. You may want to look into the following:

    Did my connection stop mid-way through its sync?
    Is there a condition in this particular row that‚Äôs now being met?
    Was this row created after the latest sync time?

If the opposite were to occur, where a row is present in Snowflake but not Postgres, you should ask yourself the following questions:

    Was this record deleted in my source table? Is that accurate? Did this happen after the latest sync?

And, if you have a row whose column values vary between the two tables, you should be thinking about these questions:

    Was this row updated after the latest sync time?
    What conditions could cause this value to be one thing in one table and something else in the other? 

Data-diff frees up the most time-consuming, tedious task of finding the rows that differ in column values. It also allows you to fix these issues right then and there rather than reloading all of the data. Instead, it allows you to focus on what really matters- the why behind the discrepancies. Together, Airbyte and data-diff give you the tools to successfully replicate your data from one database to another and validate that the process went smoothly.
Conclusion 

Knowing how to use open-source tools in combination with one another is a powerful skill for any analyst, analytics engineer, or data engineer. Utilizing a data quality tool like data-diff skips the guessing game, leaving more time for the most impactful work. It tells you the exact rows to investigate further making it so no aggregated queries or direct comparisons need to be made.

Not only is it great for the initial replication process, but data-diff is always available for you to use to check the health of your Airbyte connectors. It is quick and easy to run whenever you suspect an issue, or as a monthly data quality maintenance check.

Better yet, you could automate the data-diff CLI commands we just used in this tutorial to run after every Airbyte sync using an orchestrating tool like Prefect. Then, you could set up Slack alerts based on what information you would want to be sent to you when your data pipeline runs. I‚Äôd recommend getting the primary keys of the varying rows sent to you directly as a Slack message.

The possibilities for automating ingestion and data-quality tools are endless. Open-source tools like Airbyte and data-diff make a great pair, working together to create a data environment that puts data quality above all else. 

* Tools
- [[https://github.com/adaptive-scale/dbchaos][adaptive-scale/dbchaos: Stress-test your database with pre-defined queries. Validate slow and expensive queries that breaks your database.]]
- [[https://github.com/clement-tourriere/dbcrust][clement-tourriere/dbcrust: The modern database CLI that speaks your language ‚Äî PostgreSQL, MySQL, SQLite with zero compromises.]]
