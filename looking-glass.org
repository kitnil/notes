* Windows VM with GPU Passthrough, Part 1: Creating a basic Windows VM
Apr 20, 2022
Desktop
9 minutes
Quantum
qt.ax/vg1

Last time, we introduced a series on running a Windows VM with native-level graphics performance via GPU passthrough and integrating it seamlessly into your Linux desktop via Looking Glass. We start this journey by creating a basic Windows virtual machine, which will form the foundation of all future work.

For this example, I decided to use Windows 11 for fun, since I did it quite a few times with Windows 10 already. However, given that Windows 11 is basically a renamed Windows 10 with some additional hardware requirements, there is not much of a difference anyway.

On the Linux side, we will be using the standard QEMU/KVM setup, managing our virtual machines with libvirt. Let’s begin!
Preparation

No good journey can start without sufficient preparation. To start, you need to enable hardware-assisted virtualization in the UEFI firmware (commonly still called “BIOS”) if you haven’t already done so. This is buried in the settings under names like SVM, Intel VT-x, AMD-V, or just “virtualization”. Unfortunately, this is highly specific to your motherboard and I cannot provide more help. Once done, grep -oE 'vmx|svm' /proc/cpuinfo | sort -u should produce output.

On the Linux side, you want to install the following packages (the names depend on the distribution, but I’ll use Debian as an example):

    libvirt: a virtual machine management daemon
    virt-manager: a GUI for libvirt
    qemu-system-x86: an emulator capable of using KVM
    ovmf: a UEFI firmware implementation
    swtpm: a software TPM emulator (Windows 11 only)

Once you have these packages, you’ll also need a Windows ISO. You can download this from Microsoft:

    Windows 11
    Windows 10

For optimal VM performance, you’ll want to use virtio devices. These are designed for virtual machine use and have far less overhead than emulating real hardware. The Linux kernel usually contains drivers for these devices, but the same is not true for Windows. To have a good time, you should download this ISO file as well.

You’ll also need a chunk of free disk space to store your virtual machine. Given the disk requirements of Windows, I would suggest at least 40 GB.
Creating the Virtual Machine

We start by launching virt-manager, and then
→

. You want a QEMU/KVM connection. If none exist, you should go back and add a connection to QEMU/KVM on the local machine.

A screenshot of virt-manager's VM creation screen

Then, continue and select the Windows ISO file (you likely want to use the

option in the “Locate ISO media volume” dialog). It is okay if Windows 11 is detected as Windows 10 instead, as libvirt does not have a Windows 11 option at the time of writing.

Once that’s done, you should decide how much RAM and CPU you want to give to the virtual machine. I would suggest at least 8 GB and 4 CPU cores if not more.

You’ll then be prompted to configure the storage. To start, it’s probably the easiest to use a disk image. We can change this later into some more optimal form, e.g. an LVM volume, a full block device, or using VFIO to pass through an NVMe SSD. This will be explored in a later part.

Then, in the last step of the wizard, you will be prompted to name the VM. At this point, you likely want to configure the network to something sane. I recommend that you start with the default NAT network, as that requires the least amount of effort to get working. Depending on your distro, this NAT network may not be running by default (you can check to see if it’s active in virsh net-list). If it’s not, you need to run virsh net-start default to get it running now, and virsh net-autostart default to get it running on startup. You may also need to install dnsmasq before it would work. For optimal performance, you can use bridge networking instead of NAT, but that is out of the scope for this series.

A screenshot of virt-manager's final VM creation screen

Then, remember to check the Customize configuration before install box before clicking

, as we will be making some changes that would be very difficult to change later.

Here are the changes that need to be done before we begin the installation:

    In the “Overview” tab, change the firmware to 

. This is necessary to enable Secure Boot to properly run Windows 11. On Windows 10, the plain OVMF_CODE_4M.fd would also work. Remember to
your changes before you switch to a different tab. Note that some distributions do not come with OVMF_CODE_4M.ms.fd. Don’t panic! There is a way to bypass the TPM requirement. A screenshot of virt-manager's hardware overview screen
In the “SATA Disk 1” tab, you want to change the disk bus to virtio for the best performance.
In the NIC tab, you want to change the device model to virtio.
Click the
button, select “Storage” and select custom storage, then use then
to select the virtio driver ISO. Remember to set the device type to CDROM and the bus type to SATA. A screenshot of virt-manager's add CD screen
Finally, if you are running Windows 11, you want to add a TPM. Press

    and select the TPM option, using the CRB model, the “emulated device” backend, and version 2.0.

You are now ready to continue! Press

to start the process.
Installing Windows

The virtual machine should boot. Press any key when prompted, and eventually, you will land on the Windows installer welcome screen:

A screenshot of Windows installer welcome page

Select the language and then click the

button. Enter your product key if you have one, otherwise, click I don’t have a product key. Select the edition you want to install, read and agree to the license agreement, then select the custom installation type.

A screenshot of Windows installer drive selection page

You will notice that you can’t see any drives. This is because Windows doesn’t have the virtio driver. Click Load driver and browse to the amd64/win11 (or win10) directory under the virtio-win disc. Select viostor.inf and continue. You should now see an unallocated space. Select that to continue the installation.

Wait a few minutes, and you’ll be prompted to select your country and keyboard layout:

A screenshot of Windows keyboard selection page

At this point, just follow on-screen instructions, except selecting
and

when asked about networking. This is because we haven’t installed the virtio network driver yet.

Once that’s done, you should end up on the Windows desktop.
No TPM / Secure Boot

If you don’t have secure boot enabled, you’ll see this screen when installing Windows 11:

A screenshot of no secure boot

At this point, press Shift+F10, then run regedit.

Under HKEY_LOCAL_MACHINE\SYSTEM\Setup, create a new key called LabConfig. Inside this new key, create the following DWORD values and set their values to 1: BypassTPMCheck, BypassRAMCheck, BypassSecureBootCheck.

A screenshot of registry hack

Now close regedit and the command prompt, go back one step, and click next. You can now continue the installation as usual.
Installing Drivers

In the start menu, search for “Device Manager” and launch it. You should see a bunch of “other devices” with no drivers. These are virtio devices.
A screenshot of device manager showing other devices A screenshot of device manager showing display adapters

For all these, as well as “Microsoft Basic Display Adapter” under “display adapters”, you want to update the drivers. To do this, right-click the device, select “update driver”, and then browse your computer for drivers (in the “Search for drivers in this location” section). Select the following directory under the virtio-win drive (most likely E:) based on the device:

    For “Ethernet Controller”, select NetKVM\w11\amd64 (or NetKVM\w10\amd64 on Windows 10).
    For “PCI Device”, select Balloon\w11\amd64 (or Balloon\w10\amd64 on Windows 10). Alternatively, you can ignore this device, since it is useless once we set up GPU passthrough and may harm performance. A future part will mention how to disable this device.
    For “PCI Simple Communications Controller”, select vioserial\w11\amd64 (or vioserial\w10\amd64 on Windows 10).
    For “Microsoft Basic Display Adapter”, select qxldod\win11\amd on Windows 11 if it exists (it doesn’t at the time of writing), otherwise select qxldod\win10\amd.

Then press

. Windows should find the drivers and install them. If prompted, reboot.

Once this is done, you should be able to change the screen resolution and enjoy a proper desktop experience.

A screenshot of completed Windows desktop

At this point, your basic Windows virtual machine is complete. Next time, we shall discuss the process of passing a GPU through to the virtual machine.
You may also be interested in…
Windows VM with GPU Passthrough, Part 3: Setting up Looking Glass
Last time, we learned how to passthrough a GPU. This time, we'll learn how to set up Looking Glass to integrate it into the Linux desktop.
September 18, 2022
Windows VM with GPU Passthrough, Part 2: Passing through PCIe Devices
Last time, we learned how to create a Windows virtual machine. This time, we shall examine how we might give it a real GPU.
May 12, 2022
Windows VM with GPU Passthrough, Part 0: Introduction
This post introduces a new series about running a Windows VM with native-level graphics performance with GPU passthrough and integrating it into your Linux desktop with Looking Glass.
April 18, 2022
About me

My name is Guanzhong Chen, also known by my username, quantum.

I am a software developer at Stripe, a software engineering graduate from the University of Waterloo, and co-founder of DMOJ, the most popular programming contest platform in Canada.

    me@quantum5.ca
    quantum5
    quantum5
    quantum
    3DC5 5F49 1A67 0BED
    RSS Feed

Support me

If you like my content and would like to show your appreciation, feel free to support me and offset some of my costs:

    quantum5
    quantum5
    quantum
    quantum2048

Categories

    Code
    Desktop
    Electronics
    Science
    Security
    Sysadmin
    Year Review

Archives

    2023
    2022
    2021
    2020
    2019
    2018
    2017

Copyright © 2017 – 2023 Guanzhong Chen. All Rights Reserved. Served from Amsterdam, the Netherlands.

* Windows VM with GPU Passthrough, Part 2: Passing through PCIe Devices
May 12, 2022
Desktop
9 minutes
Quantum
qt.ax/vg2

Last time, we discussed how we might create a Windows virtual machine as part of a series on running a Windows VM with native-level graphics performance via GPU passthrough and integrating it seamlessly into your Linux desktop via Looking Glass. Today, we shall turn that normal Windows virtual machine into something far more interesting by giving it a real GPU.

As far as Windows is concerned, the GPU is real hardware and can be treated as normal, so we will not go into too much depth. Most of the work lies on the Linux side, where we must do some work to make sure the GPU is free for the VM to use, and then instruct the hypervisor to use it. Again, we will be using the standard QEMU/KVM setup, managing our virtual machines with libvirt.

Naturally, the same procedure here can be used for any other PCIe device, such as NVMe SSDs. Let’s begin!
Enabling IOMMU

To begin, we must first enable the feature known as IOMMU. This is essentially a translation layer for device-visible virtual addresses to real physical memory addresses. The IOMMU can be used to remap the addresses seen by PCIe devices to be the same as that of the virtual machine’s memory addresses, allowing it to function in a virtual machine as if it was directly plugged into it. This enables us to pass PCIe devices through to the virtual machine.

IOMMU is a hardware feature and requires support from both the CPU and the system firmware. If you have anything remotely recent, like AMD Bulldozer or Ryzen CPUs and Intel Core i3/i5/i7 CPUs, you almost certainly have support.

This setting is controlled by the UEFI firmware (commonly still called “BIOS”). Usually, you want to go into the UEFI firmware settings and enable the feature. It may be listed under names like IOMMU, Intel VT-d, AMD-Vi, or any other such names. It’s often buried deep in some menus, so you have systematically search through all the menus. Unfortunately, like enabling hardware-assisted virtualization, this is highly dependent on the motherboard, and oftentimes, also undocumented. You are on your own here, but it should not be too hard.

Once the feature is enabled in the UEFI firmware, you’ll need to enable it via a kernel command line flag. Typically, this is done by adding the flags to the GRUB_CMDLINE_LINUX line in /etc/default/grub. For Intel CPUs, you want to add intel_iommu=on, and for AMD CPUs, you want to add amd_iommu=on. You often want to add iommu=pt as well to avoid issues with devices that you are not passing through. Once done, the line should look something like (where ... represents other flags, if you have them):

GRUB_CMDLINE_LINUX="... amd_iommu=on iommu=pt ..."

Once this is done, you’ll need to regenerate the grub configuration. On Debian and derivatives, this would be sudo update-grub. On Arch, this would be sudo grub-mkconfig -o /boot/grub/grub.cfg. If you are using any other distribution or bootloader, follow its documentation.
Identifying the device

After enabling IOMMU, we must identify the PCIe device we want to pass to the virtual machine. We also need to check which IOMMU group it is in. To do this, you can use this script:

#!/bin/bash
echo 'PCIe devices'
shopt -s nullglob
for g in $(find /sys/kernel/iommu_groups/ -mindepth 1 -maxdepth 1 -type d | sort -V); do
    echo "IOMMU Group ${g##*/}:"
    for d in "$g/devices/"*; do
        echo -e "\t$(lspci -nns ${d##*/})"
    done
done

echo
echo 'USB Controllers'
for usb_ctrl in /sys/bus/pci/devices/*/usb*; do
    pci_path="${usb_ctrl%/*}"
    iommu_group="$(readlink $pci_path/iommu_group)"
    echo "Bus $(cat $usb_ctrl/busnum) --> ${pci_path##*/} (IOMMU group ${iommu_group##*/})"
    lsusb -s "${usb_ctrl#*/usb}:"
    echo
done

Save this as iommu.sh and execute it via bash iommu.sh. For example, this is what my GTX 1060 show up as:

$ bash iommu.sh
...
IOMMU Group 29:
  0c:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP106 [GeForce GTX 1060 6GB] [10de:1c03] (rev a1)
  0c:00.1 Audio device [0403]: NVIDIA Corporation GP106 High Definition Audio Controller [10de:10f1] (rev a1)
...

As you can see, the GPU and its corresponding audio device are the only devices listed in IOMMU group 29. This is the ideal scenario, and this GPU can be passed through without any issues. If you are passing through any other device, it should also be in its own IOMMU group. Sometimes, you’ll see a “PCI bridge” device in the IOMMU group. This is fine and you can also proceed.

If you see any other devices in the IOMMU group, you may need to go into your UEFI firmware settings and enable a feature called ACS. This may result in a better IOMMU group. If this is not possible or did not help, then you will need the ACS override patch. Using it degrades system security and is out-of-scope for this post, though instructions are readily available via Google. Once this patch is applied, all the devices will be broken up into their own IOMMU groups.

Once the device you wish to pass through is in its own IOMMU group, note down their device IDs. In the case of my 1060, these are 10de:1c03 and 10de:10f1 as seen in the output above.

Note that you may have some trouble if you have multiple devices with the same IDs, for example, if you have two GPUs of the same model. In such cases, you can attempt to distinguish them by subsystem ID, which you may view via lspci -s [PCIe address] -nnv, where the address is something like 0c:00.0 in the example. If you can’t, then you are on your own. This section on the Arch wiki may prove helpful, if you know how to adapt it to your distribution.
Isolating the device

To give any PCIe device, such as a second GPU, to a virtual machine, it must first be unused by the host machine. Otherwise, sheer chaos ensues as two sets of drivers attempt to control the same device. While libvirt has decent support for unloading the host machine drivers before passing a device to the virtual machine, this is has some quirks. For example, if the GPU is currently being used (sometimes unwittingly), the process that’s using it must be stopped before the virtual machine can start. This is not worth the trouble, so it’s usually easier to mark the device for passthrough at boot time, preventing anything else from using it.

To achieve this, we isolate the device at boot time by binding the device to the vfio-pci kernel module. This is a four-step process:

    You need to use a kernel parameter to tell vfio-pci to bind to your desired device. This is done via the vfio-pci.ids parameter, which takes a comma separated list of device IDs, each of which follows the format vendor:device[:subvendor[:subdevice[:class[:class_mask]]]]. For example, with my 1060 example, it would be vfio-pci.ids=10de:1c03,10de:10f1. Follow the same instructions as above to add the parameter.

    You will then need to tell your initramfs tool to include the vfio-pci module. On most distros using initramfs-tools, such as Debian and derivatives, you can add the following lines to /etc/initramfs-tools/modules:

    # VFIO
    vfio_pci
    vfio
    vfio_iommu_type1
    vfio_virqfd

    If you are using another initramfs tool, this section on the Arch wiki might prove helpful.

    If you are using initramfs-tools (and potentially others, I haven’t tested them), you will also need to declare a soft dependency for the usual kernel module for your device on vfio-pci to ensure that vfio-pci binds to the device first. For GPUs, adding the following lines to /etc/modprobe.d/vfio.conf (you may need to create this file) is sufficient:

    # Make vfio-pci a pre-dependency of the usual video modules
    softdep amdgpu pre: vfio-pci
    softdep radeon pre: vfio-pci
    softdep nouveau pre: vfio-pci
    softdep snd_intel_hda pre: vfio-pci

    Finally, you will need to regenerate your initramfs. On Debian and derivatives, this can be done via sudo update-initramfs. For other initramfs tools, follow its documentation.

Adding the device to the virtual machine

At this point, all the hard work is out of the way. All you need to do now is to start virt-manager and edit the virtual machine you created in part 1:

A screenshot of virt-manager's VM edit screen

Click

, then go to “PCI Host Device” and select the desired device. In our example, this is the 1060:

A screenshot of virt-manager's add device screen with 1060 selected

For GPUs, remember to repeat the process for its corresponding audio device.

Once you are done, you can start the virtual machine. The GPU should be detected as a new hardware device, just as if you had a Windows machine with a new GPU plugged in. At this point, you install drivers for it:

A screenshot of nvidia drivers finishing to install.

Once that is done, if you plug in a monitor, you should be able to use the Windows desktop on that monitor and play games with hardware acceleration.

Next time, we shall look into installing Looking Glass and integrate the virtual machine into the Linux desktop properly.
You may also be interested in…
Windows VM with GPU Passthrough, Part 3: Setting up Looking Glass
Last time, we learned how to passthrough a GPU. This time, we'll learn how to set up Looking Glass to integrate it into the Linux desktop.
September 18, 2022
Windows VM with GPU Passthrough, Part 1: Creating a basic Windows VM
Last time, we introduced a series on running a Windows VM with native-level graphics performance. We start by creating a basic Windows virtual machine.
April 20, 2022
Windows VM with GPU Passthrough, Part 0: Introduction
This post introduces a new series about running a Windows VM with native-level graphics performance with GPU passthrough and integrating it into your Linux desktop with Looking Glass.
April 18, 2022
About me

My name is Guanzhong Chen, also known by my username, quantum.

I am a software developer at Stripe, a software engineering graduate from the University of Waterloo, and co-founder of DMOJ, the most popular programming contest platform in Canada.

    me@quantum5.ca
    quantum5
    quantum5
    quantum
    3DC5 5F49 1A67 0BED
    RSS Feed

Support me

If you like my content and would like to show your appreciation, feel free to support me and offset some of my costs:

    quantum5
    quantum5
    quantum
    quantum2048

Categories

    Code
    Desktop
    Electronics
    Science
    Security
    Sysadmin
    Year Review

Archives

    2023
    2022
    2021
    2020
    2019
    2018
    2017

Copyright © 2017 – 2023 Guanzhong Chen. All Rights Reserved. Served from Amsterdam, the Netherlands.

* Windows VM with GPU Passthrough, Part 3: Setting up Looking Glass
Sep 18, 2022
Desktop
13 minutes
Quantum
qt.ax/vg3

Last time, we discussed how we might add a real GPU to our Windows virtual machine. Today, we’ll discuss how to view this virtual machine without using a dedicated monitor or switching inputs, but instead integrating it into the Linux desktop like a normal application.

There are three steps:

    Configuring the virtual machine.
    Installing the Looking Glass client on the host machine.
    Setting up Looking Glass host application on the virtual machine.

Without further ado, let’s begin.
Configuring the virtual machine

To begin, we must prepare the virtual machine for optimal performance. The process here is beyond what virt-manager can do, so we instead resort to editing the XML via virsh. It is recommended that you make the changes in each step separately, as virsh validates the XML but gives you really awful error messages.

To edit the XML configuration, run virsh edit [vm name]. In our example, the VM is called win11, so we run virsh edit win11. It is worth noting that most of these optimizations benefit all virtual machines.
Step 1: Disable the memory balloon

The memory balloon doesn’t work while a GPU is passed through but nevertheless carries a performance penalty. For obvious reasons, we remove it.

To do this, search the XML for <memballon. It might look like this:

    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x05' slot='0x00' function='0x0'/>
    </memballoon>

Deleting this is insufficient, as virsh will simply add it back. Instead, we replace the entire block with:

    <memballoon model='none'/>

Step 2: Enable all Hyper-V enlightenments

By default, the configuration file doesn’t have all possible Hyper-V optimizations (“enlightenments”) enabled. We should enable all of it. In the XML, search for <features>.

For example, virt-manager created this block for me:

  <features>
    <acpi/>
    <apic/>
    <hyperv mode='custom'>
      <relaxed state='on'/>
      <vapic state='on'/>
      <spinlocks state='on' retries='8191'/>
    </hyperv>
    <vmport state='off'/>
  </features>

We replace it with something like this:

  <features>
    <acpi/>
    <apic/>
    <hyperv mode='custom'>
      <relaxed state='on'/>
      <vapic state='on'/>
      <spinlocks state='on' retries='8191'/>
      <vpindex state='on'/>
      <synic state='on'/>
      <stimer state='on'>
        <direct state='on'/>
      </stimer>
      <reset state='on'/>
      <vendor_id state='on' value='quantum5.ca'/>
      <frequencies state='on'/>
      <reenlightenment state='on'/>
      <tlbflush state='on'/>
      <ipi state='on'/>
      <evmcs state='off'/>
    </hyperv>
    <kvm>
      <hidden state='on'/>
    </kvm>
    <vmport state='off'/>
    <ioapic driver='kvm'/>
  </features>

The vendor_id string ensures certain GPU drivers do not detect our virtual machine and disable some features. Please feel free to replace it with another string that’s no longer than 12 characters.
Step 3: Enable CPU pinning

This is crucial to system performance, as the kernel by default will switch virtual CPU cores between threads, resulting in poor cache performance as the scheduler inside the virtual machine is unaware of this.

This is also not easy. Essentially, you want to select a real core for each virtual core the VM has. However, if your physical hardware has SMT (a.k.a hyperthreading), you should instead define twice vCPUs in the VM. You can check for SMT by running lscpu-e and see if the values in the CORE column are duplicated.

For example, if you wish your virtual machine to have 4 real CPU cores, you want to select 4 distinct core numbers from lscpu -e and pick all the CPU numbers associated with those cores. Then, you create a <cputune> block like this and place it immediately before </domain>:

  <cputune>
    <vcpupin vcpu='0' cpuset='[cpu number here]'/>
    <vcpupin vcpu='1' cpuset='[cpu number here]'/>
    <vcpupin vcpu='2' cpuset='[cpu number here]'/>
    <vcpupin vcpu='3' cpuset='[cpu number here]'/>
    ...
  </cputune>

Note that if you have SMT, it is important that vCPU 0 and vCPU 1 be on the same physical core, and vCPU 2 and 3, and so on.

On recent AMD, if your CPU has multiple core complexes (CCXes), you want to ensure that the cores you select don’t unnecessarily straddle between CCXes. The exact detail is specific to your CPU model. You can identify these by looking at the L3 column in lscpu -e.

For example, I am passing through 6 cores from the second CCX (L3=1) of my Ryzen 9 5950X (with SMT) to my VM. My lscpu -e looks like this:

CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE    MAXMHZ    MINMHZ
  0    0      0    0 0:0:0:0          yes 5083.3979 2200.0000
  1    0      0    1 1:1:1:0          yes 5083.3979 2200.0000
  2    0      0    2 2:2:2:0          yes 5083.3979 2200.0000
  3    0      0    3 3:3:3:0          yes 5083.3979 2200.0000
  4    0      0    4 4:4:4:0          yes 5083.3979 2200.0000
  5    0      0    5 5:5:5:0          yes 5083.3979 2200.0000
  6    0      0    6 6:6:6:0          yes 5083.3979 2200.0000
  7    0      0    7 7:7:7:0          yes 5083.3979 2200.0000
  8    0      0    8 8:8:8:1          yes 5083.3979 2200.0000
  9    0      0    9 9:9:9:1          yes 5083.3979 2200.0000
 10    0      0   10 10:10:10:1       yes 5083.3979 2200.0000
 11    0      0   11 11:11:11:1       yes 5083.3979 2200.0000
 12    0      0   12 12:12:12:1       yes 5083.3979 2200.0000
 13    0      0   13 13:13:13:1       yes 5083.3979 2200.0000
 14    0      0   14 14:14:14:1       yes 5083.3979 2200.0000
 15    0      0   15 15:15:15:1       yes 5083.3979 2200.0000
 16    0      0    0 0:0:0:0          yes 5083.3979 2200.0000
 17    0      0    1 1:1:1:0          yes 5083.3979 2200.0000
 18    0      0    2 2:2:2:0          yes 5083.3979 2200.0000
 19    0      0    3 3:3:3:0          yes 5083.3979 2200.0000
 20    0      0    4 4:4:4:0          yes 5083.3979 2200.0000
 21    0      0    5 5:5:5:0          yes 5083.3979 2200.0000
 22    0      0    6 6:6:6:0          yes 5083.3979 2200.0000
 23    0      0    7 7:7:7:0          yes 5083.3979 2200.0000
 24    0      0    8 8:8:8:1          yes 5083.3979 2200.0000
 25    0      0    9 9:9:9:1          yes 5083.3979 2200.0000
 26    0      0   10 10:10:10:1       yes 5083.3979 2200.0000
 27    0      0   11 11:11:11:1       yes 5083.3979 2200.0000
 28    0      0   12 12:12:12:1       yes 5083.3979 2200.0000
 29    0      0   13 13:13:13:1       yes 5083.3979 2200.0000
 30    0      0   14 14:14:14:1       yes 5083.3979 2200.0000
 31    0      0   15 15:15:15:1       yes 5083.3979 2200.0000

Therefore, my <cputune> block looks like this:

  <cputune>
    <vcpupin vcpu='0' cpuset='8'/>
    <vcpupin vcpu='1' cpuset='24'/>
    <vcpupin vcpu='2' cpuset='9'/>
    <vcpupin vcpu='3' cpuset='25'/>
    <vcpupin vcpu='4' cpuset='10'/>
    <vcpupin vcpu='5' cpuset='26'/>
    <vcpupin vcpu='6' cpuset='11'/>
    <vcpupin vcpu='7' cpuset='27'/>
    <vcpupin vcpu='8' cpuset='12'/>
    <vcpupin vcpu='9' cpuset='28'/>
    <vcpupin vcpu='10' cpuset='13'/>
    <vcpupin vcpu='11' cpuset='29'/>
  </cputune>

Note that if you are changing the number of cores, you also want to change the <vcpu> element to reflect this:

<vcpu placement='static'>12</vcpu>

You also want to set the topology under the CPU element and set the mode to host-passthrough (replace the topology with your own):

  <cpu mode='host-passthrough' check='none' migratable='off'>
    <topology sockets='1' dies='1' cores='6' threads='2'/>
    <cache mode='passthrough'/>
    <feature policy='require' name='topoext'/>
  </cpu>

On AMD, <feature policy='require' name='topoext'/> allows SMT to work. On Intel, remove that line.

For further details, you can read the Arch Wiki page.
Step 4: Create a shared memory device

(Note that this is specific to Looking Glass.)

We need to create a shared memory device for Looking Glass to send the VM’s framebuffer to the host machine. The size must be a power of two, but the exact size is dependent on the resolution you want to use.

Let w be the width and h be the height of the framebuffer. Let b, the number of bytes per pixel, be 4 unless you have HDR enabled, in which case it should be 8. (At the time of writing, HDR is pointless as Linux can’t display it.)

Then, the required size in megabytes should be:
w×h×b×21 048 576+10

Round this up to the nearest power of two.

Then, before the </devices>, add the following block, replacing the size placeholder:

    <shmem name='looking-glass'>
      <model type='ivshmem-plain'/>
      <size unit='M'>[power of two size here]</size>
    </shmem>

By default, QEMU will create this file as its user and deny your user access. You can workaround this by creating the file before you start the VM and granting QEMU write access via the group. For example, you can do this with systemd-tmpfiles by creating /etc/tmpfiles.d/10-looking-glass.conf (replacing user with your username and kvm with the group that libvirt uses for qemu):

f /dev/shm/looking-glass 0660 user kvm -

Alternatively, you can run the following command after starting the VM (every single time):

sudo chown $USER /dev/shm/looking-glass 
sudo chmod 660 /dev/shm/looking-glass

Step 5: Use virtio input devices and disable the tablet input

In the XML, find the <input> elements. There should be some like the following:

    <input type='tablet' bus='usb'>
      <address type='usb' bus='0' port='1'/>
    </input>
    <input type='mouse' bus='ps2'/>
    <input type='keyboard' bus='ps2'/>

Looking Glass doesn’t interact well with tablet devices, so you should replace it with a virtio mouse. While you are at it, replace the keyboard with a virtio model as it also helps with issues like key repeating. Don’t bother removing the PS/2 devices, as virsh will add them back in.

It should look something like this when done:

    <input type='mouse' bus='ps2'/>
    <input type='keyboard' bus='ps2'/>
    <input type='mouse' bus='virtio'/>
    <input type='keyboard' bus='virtio'/>

Step 6: Additional optimizations

There are things like hugepages that could benefit performance, but are rather non-trivial to set up. I may write about these later. If you are interested, you can try to read the Arch Wiki page or Debian Wiki page. You can also visit the VFIO Discord, especially the #kvm-support channel.
Installing the Looking Glass client

We can now install the Looking Glass client. Simply follow these steps:

    Go to the downloads page. Please download the source code for the latest stable release if it’s B6 or newer. (At the time of writing, B6 has not been released, but it has significant improvements that make the setup much easier.) Otherwise, download that of the latest bleeding-edge build.
    Unzip the source code tarball where convenient, e.g.

    quantum@[redacted]:~/lg-test$ tar xzf ~/Downloads/looking-glass-B5-433-3b16fb1b.tar.gz

    Switch to the client subdirectory in the unpacked source tree, create a build subdirectory, and go inside:

    quantum@[redacted]:~/lg-test$ cd looking-glass-B5-433-3b16fb1b/client/
    quantum@[redacted]:~/lg-test/looking-glass-B5-433-3b16fb1b/client$ mkdir build
    quantum@[redacted]:~/lg-test/looking-glass-B5-433-3b16fb1b/client$ cd build/

    Please install the build dependencies. If you are using the stable release, it’s here. Otherwise, use this link. If you are using a non-Debian-based distribution, please check this wiki page instead.
    Now, simply run the following commands to build Looking Glass:

    quantum@[redacted]:~/lg-test/looking-glass-B5-433-3b16fb1b/client/build$ cmake ..
    ...
    -- Configuring done
    -- Generating done
    -- Build files have been written to: /home/quantum/lg-test/looking-glass-B5-433-3b16fb1b/client/build
    quantum@[redacted]:~/lg-test/looking-glass-B5-433-3b16fb1b/client/build$ make -j$(nproc)
    ...
    [100%] Linking CXX executable looking-glass-client
    [100%] Built target looking-glass-client

    At this point, you can run ./looking-glass-client or make install to put it somewhere more convenient.

To use the Looking Glass client, first start the VM, e.g. via virsh start win11. Then, run the client via ./looking-glass-client (or looking-glass if you make installed it). You should see the Windows desktop at this point. However, there will be a broken monitor icon on the top-right, indicating that we are just using spice video (the same thing that virt-manager uses).

To interact with the virtual machine, press the ScrollLock key. If you don’t have this key, you can pass the -m KEY_RIGHTCTRL flag to change it to the right Ctrl. Pass any invalid value to see the full list.
Setting up the Looking Glass host application

In the virtual machine, navigate to the Looking Glass downloads page and download the “Windows Host Application” for the exact same version as the client you downloaded earlier. After extracting the zip file, run looking-glass-host-setup.exe. Follow the setup wizard. When complete, the broken monitor icon will disappear and everything will just work.

You can now press Windows+P to cycle through the modes to disable output to the virtual screen that we were using earlier. (For me, the “PC screen only” option worked.) Alternatively, you can disable the Red Hat QXL controller under Display adapters in Device Manager.

And that’s it, Looking Glass now works. There is just one last step to complete the experience.
Installing drivers for virtio input devices

The procedure here is exactly the same as how we installed the other drivers in the first part. The drivers in question are under \vioinput in the virtio driver CD. Update the drivers of the PCI Keyboard Controller and the PCI Mouse Controller, and you are done!

At this point, your virtual machine is basically complete and you can play games on Windows inside a window on Linux with perfect desktop integration. Enjoy!
You may also be interested in…
Windows VM with GPU Passthrough, Part 2: Passing through PCIe Devices
Last time, we learned how to create a Windows virtual machine. This time, we shall examine how we might give it a real GPU.
May 12, 2022
Windows VM with GPU Passthrough, Part 1: Creating a basic Windows VM
Last time, we introduced a series on running a Windows VM with native-level graphics performance. We start by creating a basic Windows virtual machine.
April 20, 2022
Windows VM with GPU Passthrough, Part 0: Introduction
This post introduces a new series about running a Windows VM with native-level graphics performance with GPU passthrough and integrating it into your Linux desktop with Looking Glass.
April 18, 2022
About me

My name is Guanzhong Chen, also known by my username, quantum.

I am a software developer at Stripe, a software engineering graduate from the University of Waterloo, and co-founder of DMOJ, the most popular programming contest platform in Canada.

    me@quantum5.ca
    quantum5
    quantum5
    quantum
    3DC5 5F49 1A67 0BED
    RSS Feed

Support me

If you like my content and would like to show your appreciation, feel free to support me and offset some of my costs:

    quantum5
    quantum5
    quantum
    quantum2048

Categories

    Code
    Desktop
    Electronics
    Science
    Security
    Sysadmin
    Year Review

Archives

    2023
    2022
    2021
    2020
    2019
    2018
    2017

Copyright © 2017 – 2023 Guanzhong Chen. All Rights Reserved. Served from Amsterdam, the Netherlands.
