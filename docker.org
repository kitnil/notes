* Cheat sheet

- Remote docker host
  : export DOCKER_HOST=ssh://sammy@your_server_ip

- Compose
  : docker-compose --project-name pxe --file pxe.yml up -d --force

- List running docker containers with image hashes
  : docker inspect --format='{{.Id}} {{.Name}} {{.Image}}' $(docker ps -aq)

- exit from interactive shell without killing container
  : c-p-q

- xorg
  #+BEGIN_SRC sh
    docker run -it \
           -w /opt/tome4 \
           -v /tmp/.X11-unix:/tmp/.X11-unix \
           -v /opt/tome4/rootfs/opt/tome4:/opt/tome4 \
           -v /opt/tome4/rootfs/home/user:/home/user \
           -v /home/oleg/.t-engine:/root/.t-engine \
           -v /etc/localtime:/etc/localtime:ro \
           -v "/srv/lib/Tales of Maj'Eyal - GOG Linux":/install \
           -e DISPLAY \
           --rm -u1000: \
           --network=host \
           --name tome4 \
           --hostname tome4 \
           --device /dev/snd \
           --device /dev/input \
           --device /dev/dri \
           --env PULSE_SERVER=unix:/tmp/pulseaudio.socket \
           --env PULSE_COOKIE=/tmp/pulseaudio.cookie \
           --volume /tmp/pulseaudio.socket:/tmp/pulseaudio.socket \
           --volume /tmp/pulseaudio.client.conf:/etc/pulse/client.conf \
           tome4:1.6.0 ./start.sh
  #+END_SRC

* Awesome

- https://github.com/hadolint/hadolint

* Katacoda

** Getting Started With Swarm Mode

Learn how to initialise a two-node Swarm Cluster and deploy a service

*** What is Swarm Mode
   
 In this scenario, you will learn how to initialise a Docker Swarm Mode cluster and deploy networked containers using the built-in Docker Orchestration. The environment has been configured with two Docker hosts.

 In 1.12, Docker introduced Swarm Mode. Swarm Mode enables the ability to deploy containers across multiple Docker hosts, using overlay networks for service discovery with a built-in load balancer for scaling the services.

 Swarm Mode is managed as part of the Docker CLI, making it a seamless experience to the Docker ecosystem.

 Key Concepts
 Docker Swarm Mode introduces three new concepts which we'll explore in this scenario.

 Node: A Node is an instance of the Docker Engine connected to the Swarm. Nodes are either managers or workers. Managers schedules which containers to run where. Workers execute the tasks. By default, Managers are also workers.

 Services: A service is a high-level concept relating to a collection of tasks to be executed by workers. An example of a service is an HTTP Server running as a Docker Container on three nodes.

 Load Balancing: Docker includes a load balancer to process requests across all containers in the service.

 This scenario will help you learn how to deploy these new concepts.

*** Step 1 - Initialise Swarm Mode
 Turn single host Docker host into a Multi-host Docker Swarm Mode. Becomes Manager By default, Docker works as an isolated single-node. All containers are only deployed onto the engine. Swarm Mode turns it into a multi-host cluster-aware engine.

 The first node to initialise the Swarm Mode becomes the manager. As new nodes join the cluster, they can adjust their roles between managers or workers. You should run 3-5 managers in a production environment to ensure high availability.

 Task: Create Swarm Mode Cluster
 Swarm Mode is built into the Docker CLI. You can find an overview the possibility commands via docker swarm --help

 The most important one is how to initialise Swarm Mode. Initialisation is done via init.

 docker swarm init

 After running the command, the Docker Engine knows how to work with a cluster and becomes the manager. The results of an initialisation is a token used to add additional nodes in a secure fashion. Keep this token safe and secure for future use when scaling your cluster.

 In the next step, we will add more nodes and deploy containers across these hosts.

*** Step 2 - Join Cluster
 With Swarm Mode enabled, it is possible to add additional nodes and issues commands across all of them. If nodes happen to disappear, for example, because of a crash, the containers which were running on those hosts will be automatically rescheduled onto other available nodes. The rescheduling ensures you do not lose capacity and provides high-availability.

 On each additional node, you wish to add to the cluster, use the Docker CLI to join the existing group. Joining is done by pointing the other host to a current manager of the cluster. In this case, the first host.

 Docker now uses an additional port, 2377, for managing the Swarm. The port should be blocked from public access and only accessed by trusted users and nodes. We recommend using VPNs or private networks to secure access.

 Task
 The first task is to obtain the token required to add a worker to the cluster. For demonstration purposes, we'll ask the manager what the token is via swarm join-token. In production, this token should be stored securely and only accessible by trusted individuals.

 token=$(ssh -o StrictHostKeyChecking=no 172.17.0.49 "docker swarm join-token -q worker") && echo $token

 On the second host, join the cluster by requesting access via the manager. The token is provided as an additional parameter.

 docker swarm join 172.17.0.49:2377 --token $token

 By default, the manager will automatically accept new nodes being added to the cluster. You can view all nodes in the cluster using docker node ls

*** Step 3 - Create Overlay Network
 Swarm Mode also introduces an improved networking model. In previous versions, Docker required the use of an external key-value store, such as Consul, to ensure consistency across the network. The need for consensus and KV has now been incorporated internally into Docker and no longer depends on external services.

 The improved networking approach follows the same syntax as previously. The overlay network is used to enable containers on different hosts to communicate. Under the covers, this is a Virtual Extensible LAN (VXLAN), designed for large scale cloud based deployments.

 Task
 The following command will create a new overlay network called skynet. All containers registered to this network can communicate with each other, regardless of which node they are deployed onto.

 docker network create -d overlay skynet

*** Step 4 - Deploy Service
 By default, Docker uses a spread replication model for deciding which containers should run on which hosts. The spread approach ensures that containers are deployed across the cluster evenly. This means that if one of the nodes is removed from the cluster, the instances would be already running on the other nodes. The workload on the removed node would be rescheduled across the remaining available nodes.

 A new concept of Services is used to run containers across the cluster. This is a higher-level concept than containers. A service allows you to define how applications should be deployed at scale. By updating the service, Docker updates the container required in a managed way.

 Task
 In this case, we are deploying the Docker Image katacoda/docker-http-server. We are defining a friendly name of a service called http and that it should be attached to the newly created skynet network.

 For ensuring replication and availability, we are running two instances, of replicas, of the container across our cluster.

 Finally, we load balance these two containers together on port 80. Sending an HTTP request to any of the nodes in the cluster will process the request by one of the containers within the cluster. The node which accepted the request might not be the node where the container responds. Instead, Docker load-balances requests across all available containers.

 docker service create --name http --network skynet --replicas 2 -p 80:80 katacoda/docker-http-server

 You can view the services running on the cluster using the CLI command docker service ls

 As containers are started you will see them using the ps command. You should see one instance of the container on each host.

 List containers on the first host - docker ps

 List containers on the second host - docker ps

 If we issue an HTTP request to the public port, it will be processed by the two containers curl host01.

*** Step 5 - Inspect State
 The Service concept allows you to inspect the health and state of your cluster and the running applications.

 Task
 You can view the list of all the tasks associated with a service across the cluster. In this case, each task is a container docker service ps http

 You can view the details and configuration of a service via docker service inspect --pretty http

 On each node, you can ask what tasks it is currently running. Self refers to the manager node Leader: docker node ps self

 Using the ID of a node you can query individual hosts docker node ps $(docker node ls -q | head -n1)

 In the next step, we will scale the service to run more instances of the container.

*** Step 6 - Scale Service
 A Service allows us to scale how many instances of a task is running across the cluster. As it understands how to launch containers and which containers are running, it can easily start, or remove, containers as required. At the moment the scaling is manual. However, the API could be hooked up to an external system such as a metrics dashboard.

 Task
 At present, we have two load-balanced containers running, which are processing our requests curl host01

 The command below will scale our http service to be running across five containers.

 docker service scale http=5

 On each host, you will see additional nodes being started docker ps

 The load balancer will automatically be updated. Requests will now be processed across the new containers. Try issuing more commands via curl host01

 Try scaling the service down to see the result.

** Add Healthcheck for Containers

Learn how to add a Healthcheck instruction for containers

*** Step 1 - Creating Service
The new Healthcheck functionality is created as an extension to the Dockerfile and defined when a Docker image is built.

Create HTTP Service with a Healthcheck
The Dockerfile below extends an existing HTTP service and adds a healthcheck.

The healthcheck will curl the HTTP server running every second to ensure it's up. If the server responds with a non-200 request, curl will fail and an exit code 1 will be returned. After three failures, Docker will mark the container as unhealthy.

The format of the instruction is HEALTHCHECK [OPTIONS] CMD command.

Copy to EditorFROM katacoda/docker-http-server:health
HEALTHCHECK --timeout=1s --interval=1s --retries=3 \
  CMD curl -s --fail http://localhost:80/ || exit 1
Currently, Healthcheck supports three different options:

interval=DURATION (default: 30s). This is the time interval between executing the healthcheck.

timeout=DURATION (default: 30s). If the check does not finish before the timeout, consider it failed.

retries=N (default: 3). How many times to recheck before marking a container as unhealthy.

The command executing must be installed as part of the container deployment. Under the covers, Docker will use docker exec to execute the command.

Build and Run
Before continuing, build and run the HTTP service.

docker build -t http .

By default it will start in a healthy state.

docker run -d -p 80:80 --name srv http

In the next steps we'll cause the HTTP Server to start throwing errors.

*** Step 2 - Crash Service
With the HTTP server running as a container, the Docker Daemon will automatically check the healthcheck based on the options. It will return the status when you list all the running containers, for example docker ps.

Set Unhealthy
The HTTP server has a special endpoint which will cause it to start reporting errors.

Make a http request to curl http://docker/unhealthy

The service will now go into error mode. In the next step, we'll look at how Docker handles this.

*** Step 3 - Verify Status
As the HTTP server is in an error state, the healthcheck should fail. Docker will report this as part of the metadata.

Detecting Errors
Docker will report the health status in various different places. To get the raw text stream, useful during automation, use Docker Inspect to pull out the Health Status field.

docker inspect --format "{{json .State.Health.Status }}" srv

The Health state stores a log of all the failures and any output from the command. This is useful for debugging why a container is considered unhealthy.

docker inspect --format "{{json .State.Health }}" srv

The status of all the containers can be viewed using docker ps

*** Step 4 - Fix Service
Use an extra HTTP endpoint to make the service healthy again. curl http://docker/healthy

View Healthy Status
Once the service is healthy again, Docker will update the status.

docker ps

docker inspect --format "{{json .State.Health.Status }}" srv

*** Step 5 - Healthchecks with Swarm
Docker Swarm can use these health checks to understand when services need to be restarted/recreated.

Initialise a Swarm cluster and deploy the newly created image as a service with two replicas.

docker rm -f $(docker ps -qa); 
docker swarm init
docker service create --name http --replicas 2 -p 80:80 http
You should see two containers responding curl host01

Randomly cause one of the nodes to be unhealthy with curl host01/unhealthy

You should only see one node processing requests as Swarm has automatically removed it from the load balancer: curl host01

Swarm will now restart the unhealthy service automatically. docker ps

After Swarm has restarted the service you should see two nodes again: curl host01

** Deploying Portainer to Docker Swarm Cluster

Portainer is a simple management solution for Docker. It consists of a web UI that allows you to easily manage your Docker containers, images, networks and volumes.

In this scenario, you'll deploy Portainer and use the UI to manage a Docker Swarm cluster.

*** Step 2 - Deploy Portainer
With the cluster configured, the next stage is to deploy Portainer. Portainer is deployed as a container running on a Docker Swarm cluster or a Docker host.

Task: Deploy as Swarm Service
To complete this scenario, deploy Portainer as a Docker Service. By deploying as a Docker Service, Swarm will ensure that the service is always running on a manager, even if the host goes down.

The service exposes the port 9000 and stores the internal Portainer data in the directory /host/data. When Portainer starts, it connects using the docker.sock file to the Docker Swarm Manger.

There is an added constraint that the container should only run on a manager node.

docker service create \
    --name portainer \
    --publish 9000:9000 \
    --constraint 'node.role == manager' \
    --mount type=bind,src=/host/data,dst=/data \
     --mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \
    portainer/portainer \
    -H unix:///var/run/docker.sock
Deploy as Container
An alternative way of running Portainer is directly on a host. In this case, the command exposes the Portainer dashboard on port 9000, persists data to the host and connects to the Docker host it's running on via the docker.sock file.

docker run -d -p 9000:9000 --name=portainer \
  -v "/var/run/docker.sock:/var/run/docker.sock" \
  -v /host/data:/data \
  portainer/portainer

** Deploy Swarm Services with Compose v3

In this scenario, you will learn how to use Docker Compose and Stacks to deploy services on a Docker Swarm Mode cluster. The new Stacks features were added as part of the Docker Compose version 3 (v3) improvements.

Environment
The environment has been configured with two Docker machines that can communicate with each over TCP.

*** Step 1 - Initialise Swarm Mode
By default, Docker works as an isolated single-node. All containers are only deployed onto the engine. Swarm Mode turns it into a multi-host cluster-aware engine.

Task: Initialise Swarm Mode
To use the secrets functionality, Docker has to be in "Swarm Mode". This is enabled via docker swarm init

Join Swarm Mode
Execute the command below on the second host to add it as a worker to the cluster.

token=$(ssh -o StrictHostKeyChecking=no 172.17.0.12 "docker swarm join-token -q worker") && docker swarm join 172.17.0.12:2377 --token $token

*** Step 2 - Create Docker Compose file
Using Docker Compose v3, it's possible to define a Docker deployment along with production details. This provides a central location for managing your application deployments that can be deployed onto a Swarm Mode cluster.

A Docker Compose file has been created that defines deploying a Redis server with a web front end.

View the file using cat docker-compose.yml
#+BEGIN_SRC yaml
  version: "3"
  services:
    redis:
      image: redis:alpine
      volumes:
        - db-data:/data
      networks:
        appnet1:
          aliases:
            - db
      deploy:
        placement:
          constraints: [node.role == manager]

    web:
      image: katacoda/redis-node-docker-example
      networks:
        - appnet1
      depends_on:
        - redis
      deploy:
        mode: replicated
        replicas: 2
        labels: [APP=WEB]
        resources:
          limits:
            cpus: '0.25'
            memory: 512M
          reservations:
            cpus: '0.25'
            memory: 256M
        restart_policy:
          condition: on-failure
          delay: 5s
          max_attempts: 3
          window: 120s
        update_config:
          parallelism: 1
          delay: 10s
          failure_action: continue
          monitor: 60s
          max_failure_ratio: 0.3
        placement:
          constraints: [node.role == worker]

  networks:
      appnet1:

  volumes:
    db-data:
#+END_SRC

The file has been extended to utilize Swarm deployment options.

The first configuration option uses depends_on. This states that Redis must be deployed before the web and allows us to control the order of services being started.

The next configuration options define how the application should be deployed using the new deploy options.

Firstly, mode: replicated and replicas: 2 determine how many replicas of the service should be started.

Secondly, resources are define. The limits are hard limits that the application cannot exceed, the reservations is a guide to Docker Swarm to indicate the resources the applications requires.

Third, restart_policy indicates what should happen if the process crashes.

Fourth, update_config defines how updates should be applied and rolled out.

Finally, placement allows us to add constraints to determine where the service should be deployed.

More details can be found at https://docs.docker.com/compose/compose-file/#deploy
