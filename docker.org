
* Tools
- [[https://github.com/genuinetools/reg][genuinetools/reg: Docker registry v2 command line client and repo listing generator with security checks.]]
- [[https://github.com/mattn/goreman][mattn/goreman: foreman clone written in go language]]

* Containers
- [[https://github.com/shuhaoliu/docker-clion-dev][shuhaoliu/docker-clion-dev: Debugging C++ in a Docker Container with CLion IDE]]
- [[https://github.com/sagemathinc/cocalc-docker][sagemathinc/cocalc-docker: Docker setup for running CoCalc as downloadable software on your own computer]]
- [[https://github.com/wahyd4/aria2-ariang-docker][wahyd4/aria2-ariang-docker: The Docker image for Aria2 + AriaNg + File Browser]]

* Dockerfile
- [[https://github.com/HariSekhon/Dockerfiles][Dockerfiles]]

* Bastion
- [[https://github.com/moul/ssh2docker/][moul/ssh2docker: standalone SSH server that connects you to your Docker containers]]
- [[https://github.com/ml-tooling/ssh-proxy][ml-tooling/ssh-proxy: üê≥ Dockerized SSH bastion to proxy SSH connections to arbitrary containers.]]

* Security

- [[https://tech.paulcz.net/blog/secure-docker-with-tls/][Securing Docker with TLS certificates]]
- [[https://github.com/Tecnativa/docker-socket-proxy][Tecnativa/docker-socket-proxy: Proxy over your Docker socket to restrict which requests it accepts]]

* Misc

- [[https://github.com/aquasecurity/tracee][aquasecurity/tracee: Container and system event tracing using eBPF]]
- [[https://github.com/pfrayer/docker-browser][pfrayer/docker-browser: Visualize your containers/images/volumes/networks and see which ones uses which ones]]
- [[https://github.com/plexsystems/sinker][plexsystems/sinker: A tool to sync images from one container registry to another]]
- [[https://github.com/p8952/bocker][p8952/bocker: Docker implemented in around 100 lines of bash]]
- [[https://github.com/artagnon/rhine-ml][artagnon/rhine-ml: üèû an OCaml compiler for an untyped lisp]]

* Libs
- [[https://github.com/qiniu/qmgo][Qmgo - The MongoDB driver for Go . It‚Äòs based on official mongo-go-driver but easier to use like Mgo.]]
- [[https://github.com/testcontainers/testcontainers-go][testcontainers/testcontainers-go: Testcontainers is a Golang library that providing a friendly API to run Docker container. It is designed to create runtime environment to use during your automatic tests.]]

* Security
- [[https://github.com/Ullaakut/Gorsair][Ullaakut/Gorsair: Gorsair hacks its way into remote docker containers that expose their APIs]]

* Programms

- [[https://github.com/Trendyol/docker-shell][Trendyol/docker-shell: A simple interactive prompt for docker]]
- [[https://github.com/Yash-Handa/logo-ls][Yash-Handa/logo-ls: Modern ls command with vscode like File Icon and Git Integrations. Written in Golang]]
- [[https://github.com/lucasepe/jumble][lucasepe/jumble: Create (not just) diagrams stitching, connecting and labelling images on a grid using HCL syntax (like terraform!).]]
- [[https://github.com/lucasepe/draft][lucasepe/draft: Generate High Level Cloud Architecture diagrams using YAML syntax.]]
- [[https://github.com/lucasepe/crumbs][lucasepe/crumbs: Turn asterisk-indented text lines into mind maps]]
- [[https://github.com/lucasepe/modgv][lucasepe/modgv: Converts 'go mod graph' output into Graphviz's DOT language]]
- [[https://github.com/elsaland/elsa][elsaland/elsa: ‚ùÑÔ∏è Elsa is a minimal, fast and secure runtime for Javascript and Typescript written in Go]]
- [[https://github.com/nakabonne/ali][nakabonne/ali: Generate HTTP load and plot the results in real-time]]
- [[https://github.com/blushft/go-diagrams][blushft/go-diagrams: Create beautiful system diagrams with Go]]
- [[https://github.com/norouter/norouter][norouter/norouter: The easiest multi-host & multi-cloud networking ever. No root privilege is required.]]
- [[https://github.com/traefik/traefik][traefik/traefik: The Cloud Native Edge Router]]
- [[https://github.com/rosineygp/mkdkr][rosineygp/mkdkr: Make + Docker + Shell = CI Pipeline]]
- [[https://github.com/asottile/dockerfile][asottile/dockerfile: Parse a dockerfile into a high-level representation using the official go parser]]
- [[https://github.com/docker/awesome-compose][docker/awesome-compose: Awesome Docker Compose samples]]
- [[https://github.com/nicolaka/netshoot][nicolaka/netshoot: a Docker + Kubernetes network trouble-shooting swiss-army container]]
- [[https://github.com/swarmpit/swarmpit][swarmpit/swarmpit: Lightweight mobile-friendly Docker Swarm management UI]]
- [[https://github.com/docker-library/repo-info][docker-library/repo-info: Extended information (especially license and layer details) about the published Official Images]]
- [[https://github.com/facebook/infer][facebook/infer: A static analyzer for Java, C, C++, and Objective-C]]
- [[https://github.com/moby/datakit][moby/datakit: Connect processes into powerful data pipelines with a simple git-like filesystem interface]]
- [[https://github.com/moby/vpnkit][moby/vpnkit: A toolkit for embedding VPN capabilities in your application]]
- [[https://github.com/metrue/fx][metrue/fx: A Function as a Service tool makes a function as a container-based service in seconds.]]
- [[https://github.com/docker/app#writing-an-app-definition][docker/app: Make your Docker Compose applications reusable, and share them on Docker Hub]]
- [[https://developers.redhat.com/blog/2016/09/13/running-systemd-in-a-non-privileged-container/][Running systemd in a non-privileged container - Red Hat Developer]]
- [[https://github.com/docker/awesome-compose][docker / awesome-compose]]
- [[https://github.com/moby/buildkit][moby/buildkit: concurrent, cache-efficient, and Dockerfile-agnostic builder toolkit]]
- [[https://github.com/genuinetools/img][genuinetools/img: Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder.]]
- [[https://github.com/skanehira/docui][skanehira/docui: TUI Client for Docker]]
- [[https://github.com/pyouroboros/ouroboros][pyouroboros/ouroboros: Automatically update running docker containers with newest available image]]
- [[https://github.com/uber/kraken][uber/kraken: P2P Docker registry capable of distributing TBs of data in seconds]]
- [[https://github.com/uber/makisu][uber/makisu: Fast and flexible Docker image building tool, works in unprivileged containerized environments like Mesos and Kubernetes.]]
- [[https://github.com/jesseduffield/lazydocker][jesseduffield/lazydocker: The lazier way to manage everything docker]]
- [[https://github.com/goodwithtech/dockle][goodwithtech/dockle: Container Image Linter for Security, Helping build the Best-Practice Docker Image, Easy to start]]
- [[https://github.com/aquasecurity/trivy][aquasecurity/trivy: A Simple and Comprehensive Vulnerability Scanner for Containers, Suitable for CI]]
- [[https://github.com/coord-e/magicpak][coord-e/magicpak: Build minimal docker images without static linking]]
- [[https://www.linuxserver.io/][LinuxServer]]
- [[https://github.com/P3GLEG/Whaler][P3GLEG/Whaler: Program to reverse Docker images into Dockerfiles]]
- [[https://github.com/AliyunContainerService/log-pilot][AliyunContainerService/log-pilot: Collect logs for docker containers]]

* Cheat sheet

- Remote docker host
  : export DOCKER_HOST=ssh://sammy@your_server_ip

- Compose
  : docker-compose --project-name pxe --file pxe.yml up -d --force

- List running docker containers with image hashes
  : docker inspect --format='{{.Id}} {{.Name}} {{.Image}}' $(docker ps -aq)

- exit from interactive shell without killing container
  : c-p-q

- xorg
  #+BEGIN_SRC sh
    docker run -it \
           -w /opt/tome4 \
           -v /tmp/.X11-unix:/tmp/.X11-unix \
           -v /opt/tome4/rootfs/opt/tome4:/opt/tome4 \
           -v /opt/tome4/rootfs/home/user:/home/user \
           -v /home/oleg/.t-engine:/root/.t-engine \
           -v /etc/localtime:/etc/localtime:ro \
           -v "/srv/lib/Tales of Maj'Eyal - GOG Linux":/install \
           -e DISPLAY \
           --rm -u1000: \
           --network=host \
           --name tome4 \
           --hostname tome4 \
           --device /dev/snd \
           --device /dev/input \
           --device /dev/dri \
           --env PULSE_SERVER=unix:/tmp/pulseaudio.socket \
           --env PULSE_COOKIE=/tmp/pulseaudio.cookie \
           --volume /tmp/pulseaudio.socket:/tmp/pulseaudio.socket \
           --volume /tmp/pulseaudio.client.conf:/etc/pulse/client.conf \
           tome4:1.6.0 ./start.sh
  #+END_SRC

* Awesome

- https://github.com/hadolint/hadolint

* Katacoda

** Getting Started With Swarm Mode

Learn how to initialise a two-node Swarm Cluster and deploy a service

*** What is Swarm Mode
   
 In this scenario, you will learn how to initialise a Docker Swarm Mode cluster and deploy networked containers using the built-in Docker Orchestration. The environment has been configured with two Docker hosts.

 In 1.12, Docker introduced Swarm Mode. Swarm Mode enables the ability to deploy containers across multiple Docker hosts, using overlay networks for service discovery with a built-in load balancer for scaling the services.

 Swarm Mode is managed as part of the Docker CLI, making it a seamless experience to the Docker ecosystem.

 Key Concepts
 Docker Swarm Mode introduces three new concepts which we'll explore in this scenario.

 Node: A Node is an instance of the Docker Engine connected to the Swarm. Nodes are either managers or workers. Managers schedules which containers to run where. Workers execute the tasks. By default, Managers are also workers.

 Services: A service is a high-level concept relating to a collection of tasks to be executed by workers. An example of a service is an HTTP Server running as a Docker Container on three nodes.

 Load Balancing: Docker includes a load balancer to process requests across all containers in the service.

 This scenario will help you learn how to deploy these new concepts.

*** Step 1 - Initialise Swarm Mode
 Turn single host Docker host into a Multi-host Docker Swarm Mode. Becomes Manager By default, Docker works as an isolated single-node. All containers are only deployed onto the engine. Swarm Mode turns it into a multi-host cluster-aware engine.

 The first node to initialise the Swarm Mode becomes the manager. As new nodes join the cluster, they can adjust their roles between managers or workers. You should run 3-5 managers in a production environment to ensure high availability.

 Task: Create Swarm Mode Cluster
 Swarm Mode is built into the Docker CLI. You can find an overview the possibility commands via docker swarm --help

 The most important one is how to initialise Swarm Mode. Initialisation is done via init.

 docker swarm init

 After running the command, the Docker Engine knows how to work with a cluster and becomes the manager. The results of an initialisation is a token used to add additional nodes in a secure fashion. Keep this token safe and secure for future use when scaling your cluster.

 In the next step, we will add more nodes and deploy containers across these hosts.

*** Step 2 - Join Cluster
 With Swarm Mode enabled, it is possible to add additional nodes and issues commands across all of them. If nodes happen to disappear, for example, because of a crash, the containers which were running on those hosts will be automatically rescheduled onto other available nodes. The rescheduling ensures you do not lose capacity and provides high-availability.

 On each additional node, you wish to add to the cluster, use the Docker CLI to join the existing group. Joining is done by pointing the other host to a current manager of the cluster. In this case, the first host.

 Docker now uses an additional port, 2377, for managing the Swarm. The port should be blocked from public access and only accessed by trusted users and nodes. We recommend using VPNs or private networks to secure access.

 Task
 The first task is to obtain the token required to add a worker to the cluster. For demonstration purposes, we'll ask the manager what the token is via swarm join-token. In production, this token should be stored securely and only accessible by trusted individuals.

 token=$(ssh -o StrictHostKeyChecking=no 172.17.0.49 "docker swarm join-token -q worker") && echo $token

 On the second host, join the cluster by requesting access via the manager. The token is provided as an additional parameter.

 docker swarm join 172.17.0.49:2377 --token $token

 By default, the manager will automatically accept new nodes being added to the cluster. You can view all nodes in the cluster using docker node ls

*** Step 3 - Create Overlay Network
 Swarm Mode also introduces an improved networking model. In previous versions, Docker required the use of an external key-value store, such as Consul, to ensure consistency across the network. The need for consensus and KV has now been incorporated internally into Docker and no longer depends on external services.

 The improved networking approach follows the same syntax as previously. The overlay network is used to enable containers on different hosts to communicate. Under the covers, this is a Virtual Extensible LAN (VXLAN), designed for large scale cloud based deployments.

 Task
 The following command will create a new overlay network called skynet. All containers registered to this network can communicate with each other, regardless of which node they are deployed onto.

 docker network create -d overlay skynet

*** Step 4 - Deploy Service
 By default, Docker uses a spread replication model for deciding which containers should run on which hosts. The spread approach ensures that containers are deployed across the cluster evenly. This means that if one of the nodes is removed from the cluster, the instances would be already running on the other nodes. The workload on the removed node would be rescheduled across the remaining available nodes.

 A new concept of Services is used to run containers across the cluster. This is a higher-level concept than containers. A service allows you to define how applications should be deployed at scale. By updating the service, Docker updates the container required in a managed way.

 Task
 In this case, we are deploying the Docker Image katacoda/docker-http-server. We are defining a friendly name of a service called http and that it should be attached to the newly created skynet network.

 For ensuring replication and availability, we are running two instances, of replicas, of the container across our cluster.

 Finally, we load balance these two containers together on port 80. Sending an HTTP request to any of the nodes in the cluster will process the request by one of the containers within the cluster. The node which accepted the request might not be the node where the container responds. Instead, Docker load-balances requests across all available containers.

 docker service create --name http --network skynet --replicas 2 -p 80:80 katacoda/docker-http-server

 You can view the services running on the cluster using the CLI command docker service ls

 As containers are started you will see them using the ps command. You should see one instance of the container on each host.

 List containers on the first host - docker ps

 List containers on the second host - docker ps

 If we issue an HTTP request to the public port, it will be processed by the two containers curl host01.

*** Step 5 - Inspect State
 The Service concept allows you to inspect the health and state of your cluster and the running applications.

 Task
 You can view the list of all the tasks associated with a service across the cluster. In this case, each task is a container docker service ps http

 You can view the details and configuration of a service via docker service inspect --pretty http

 On each node, you can ask what tasks it is currently running. Self refers to the manager node Leader: docker node ps self

 Using the ID of a node you can query individual hosts docker node ps $(docker node ls -q | head -n1)

 In the next step, we will scale the service to run more instances of the container.

*** Step 6 - Scale Service
 A Service allows us to scale how many instances of a task is running across the cluster. As it understands how to launch containers and which containers are running, it can easily start, or remove, containers as required. At the moment the scaling is manual. However, the API could be hooked up to an external system such as a metrics dashboard.

 Task
 At present, we have two load-balanced containers running, which are processing our requests curl host01

 The command below will scale our http service to be running across five containers.

 docker service scale http=5

 On each host, you will see additional nodes being started docker ps

 The load balancer will automatically be updated. Requests will now be processed across the new containers. Try issuing more commands via curl host01

 Try scaling the service down to see the result.

** Add Healthcheck for Containers

Learn how to add a Healthcheck instruction for containers

*** Step 1 - Creating Service
The new Healthcheck functionality is created as an extension to the Dockerfile and defined when a Docker image is built.

Create HTTP Service with a Healthcheck
The Dockerfile below extends an existing HTTP service and adds a healthcheck.

The healthcheck will curl the HTTP server running every second to ensure it's up. If the server responds with a non-200 request, curl will fail and an exit code 1 will be returned. After three failures, Docker will mark the container as unhealthy.

The format of the instruction is HEALTHCHECK [OPTIONS] CMD command.

Copy to EditorFROM katacoda/docker-http-server:health
HEALTHCHECK --timeout=1s --interval=1s --retries=3 \
  CMD curl -s --fail http://localhost:80/ || exit 1
Currently, Healthcheck supports three different options:

interval=DURATION (default: 30s). This is the time interval between executing the healthcheck.

timeout=DURATION (default: 30s). If the check does not finish before the timeout, consider it failed.

retries=N (default: 3). How many times to recheck before marking a container as unhealthy.

The command executing must be installed as part of the container deployment. Under the covers, Docker will use docker exec to execute the command.

Build and Run
Before continuing, build and run the HTTP service.

docker build -t http .

By default it will start in a healthy state.

docker run -d -p 80:80 --name srv http

In the next steps we'll cause the HTTP Server to start throwing errors.

*** Step 2 - Crash Service
With the HTTP server running as a container, the Docker Daemon will automatically check the healthcheck based on the options. It will return the status when you list all the running containers, for example docker ps.

Set Unhealthy
The HTTP server has a special endpoint which will cause it to start reporting errors.

Make a http request to curl http://docker/unhealthy

The service will now go into error mode. In the next step, we'll look at how Docker handles this.

*** Step 3 - Verify Status
As the HTTP server is in an error state, the healthcheck should fail. Docker will report this as part of the metadata.

Detecting Errors
Docker will report the health status in various different places. To get the raw text stream, useful during automation, use Docker Inspect to pull out the Health Status field.

docker inspect --format "{{json .State.Health.Status }}" srv

The Health state stores a log of all the failures and any output from the command. This is useful for debugging why a container is considered unhealthy.

docker inspect --format "{{json .State.Health }}" srv

The status of all the containers can be viewed using docker ps

*** Step 4 - Fix Service
Use an extra HTTP endpoint to make the service healthy again. curl http://docker/healthy

View Healthy Status
Once the service is healthy again, Docker will update the status.

docker ps

docker inspect --format "{{json .State.Health.Status }}" srv

*** Step 5 - Healthchecks with Swarm
Docker Swarm can use these health checks to understand when services need to be restarted/recreated.

Initialise a Swarm cluster and deploy the newly created image as a service with two replicas.

docker rm -f $(docker ps -qa); 
docker swarm init
docker service create --name http --replicas 2 -p 80:80 http
You should see two containers responding curl host01

Randomly cause one of the nodes to be unhealthy with curl host01/unhealthy

You should only see one node processing requests as Swarm has automatically removed it from the load balancer: curl host01

Swarm will now restart the unhealthy service automatically. docker ps

After Swarm has restarted the service you should see two nodes again: curl host01

** Deploying Portainer to Docker Swarm Cluster

Portainer is a simple management solution for Docker. It consists of a web UI that allows you to easily manage your Docker containers, images, networks and volumes.

In this scenario, you'll deploy Portainer and use the UI to manage a Docker Swarm cluster.

*** Step 2 - Deploy Portainer
With the cluster configured, the next stage is to deploy Portainer. Portainer is deployed as a container running on a Docker Swarm cluster or a Docker host.

Task: Deploy as Swarm Service
To complete this scenario, deploy Portainer as a Docker Service. By deploying as a Docker Service, Swarm will ensure that the service is always running on a manager, even if the host goes down.

The service exposes the port 9000 and stores the internal Portainer data in the directory /host/data. When Portainer starts, it connects using the docker.sock file to the Docker Swarm Manger.

There is an added constraint that the container should only run on a manager node.

docker service create \
    --name portainer \
    --publish 9000:9000 \
    --constraint 'node.role == manager' \
    --mount type=bind,src=/host/data,dst=/data \
     --mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \
    portainer/portainer \
    -H unix:///var/run/docker.sock
Deploy as Container
An alternative way of running Portainer is directly on a host. In this case, the command exposes the Portainer dashboard on port 9000, persists data to the host and connects to the Docker host it's running on via the docker.sock file.

docker run -d -p 9000:9000 --name=portainer \
  -v "/var/run/docker.sock:/var/run/docker.sock" \
  -v /host/data:/data \
  portainer/portainer

** Deploy Swarm Services with Compose v3

In this scenario, you will learn how to use Docker Compose and Stacks to deploy services on a Docker Swarm Mode cluster. The new Stacks features were added as part of the Docker Compose version 3 (v3) improvements.

Environment
The environment has been configured with two Docker machines that can communicate with each over TCP.

*** Step 1 - Initialise Swarm Mode
By default, Docker works as an isolated single-node. All containers are only deployed onto the engine. Swarm Mode turns it into a multi-host cluster-aware engine.

Task: Initialise Swarm Mode
To use the secrets functionality, Docker has to be in "Swarm Mode". This is enabled via docker swarm init

Join Swarm Mode
Execute the command below on the second host to add it as a worker to the cluster.

token=$(ssh -o StrictHostKeyChecking=no 172.17.0.12 "docker swarm join-token -q worker") && docker swarm join 172.17.0.12:2377 --token $token

*** Step 2 - Create Docker Compose file
Using Docker Compose v3, it's possible to define a Docker deployment along with production details. This provides a central location for managing your application deployments that can be deployed onto a Swarm Mode cluster.

A Docker Compose file has been created that defines deploying a Redis server with a web front end.

View the file using cat docker-compose.yml
#+BEGIN_SRC yaml
  version: "3"
  services:
    redis:
      image: redis:alpine
      volumes:
        - db-data:/data
      networks:
        appnet1:
          aliases:
            - db
      deploy:
        placement:
          constraints: [node.role == manager]

    web:
      image: katacoda/redis-node-docker-example
      networks:
        - appnet1
      depends_on:
        - redis
      deploy:
        mode: replicated
        replicas: 2
        labels: [APP=WEB]
        resources:
          limits:
            cpus: '0.25'
            memory: 512M
          reservations:
            cpus: '0.25'
            memory: 256M
        restart_policy:
          condition: on-failure
          delay: 5s
          max_attempts: 3
          window: 120s
        update_config:
          parallelism: 1
          delay: 10s
          failure_action: continue
          monitor: 60s
          max_failure_ratio: 0.3
        placement:
          constraints: [node.role == worker]

  networks:
      appnet1:

  volumes:
    db-data:
#+END_SRC

The file has been extended to utilize Swarm deployment options.

The first configuration option uses depends_on. This states that Redis must be deployed before the web and allows us to control the order of services being started.

The next configuration options define how the application should be deployed using the new deploy options.

Firstly, mode: replicated and replicas: 2 determine how many replicas of the service should be started.

Secondly, resources are define. The limits are hard limits that the application cannot exceed, the reservations is a guide to Docker Swarm to indicate the resources the applications requires.

Third, restart_policy indicates what should happen if the process crashes.

Fourth, update_config defines how updates should be applied and rolled out.

Finally, placement allows us to add constraints to determine where the service should be deployed.

More details can be found at https://docs.docker.com/compose/compose-file/#deploy

*** Step 3 - Deploy Services
The Docker Compose file is referred to as a Docker Compose Stack. Stacks can be deployed to Swarm using the CLI.

Task
The docker stack command is used to deploy a Docker Compose Stack via Swarm. In this case, it will prefix the services with myapp.

docker stack deploy --compose-file docker-compose.yml myapp

Once deployed it's possible to use the CLI to inspect the state.

The command docker stack ls lists all stacks deployed.

Details of the internal services can be discovered via docker stack services myapp

Notice that the command indicates the Desired / Running state for the service. If the service cannot be deployed then this will be different.

The details of each service container can be identified using docker stack ps myapp

All of this information can still be discovered using docker ps

** Keeping Secrets with Docker Swarm

*** Step 1 - Initialise Swarm Mode
By default, Docker works as an isolated single-node. All containers are only deployed onto the engine. Swarm Mode turns it into a multi-host cluster-aware engine.

Task: Initialise Swarm Mode
To use the secrets functionality, Docker has to be in "Swarm Mode". This is enabled via docker swarm init

*** Step 2 - Cluster Based Secret
Create Secret
The following command will first create a random 64 character token, that will be stored in a file for testing purposes. The token file is used to create a secret called deep_thought_answer_secure.

< /dev/urandom tr -dc A-Za-z0-9 | head -c64 > tokenfile
docker secret create deep_thought_answer_secure tokenfile
Creating a secret can also be done using stdin, for example echo "the_answer_is_42" | docker secret create lesssecure -. Note, this approach would leave the value the_answer_is_42 in the users bash history file.

All the secrets names can be viewed using docker secret ls. This will not expose the underlying secret value.

Using Secrets
This secret can be used when deploying services via Swarm. For example, deploy gives the Redis service access to the secret.

docker service create --name="redis" --secret="deep_thought_answer_secure" redis

The secret appears as a file within the secrets directory.

docker exec $(docker ps --filter name=redis -q) ls -l /run/secrets

This can be read as a regular file from disk.

docker exec $(docker ps --filter name=redis -q) cat /run/secrets/deep_thought_answer_secure

*** Step 3 - Create Docker Stack with Compose
The secrets functionality is also available using Docker Compose Stacks. In the example below, the viewer service has access to our Swarm Secret _deep_thoughtanswer. It's being mounted and made available called _deep_thoughtanswer.

Task: Create Docker Compose Stack
Copy the Docker Compose snippet to the file.

Copy to Editorversion: '3.1'
services:
    viewer:
        image: 'alpine'
        command: 'cat /run/secrets/deep_thought_answer_secure'
        secrets:
            - deep_thought_answer_secure

secrets:
    deep_thought_answer_secure:
        external: true
In the next step, the Compose Stack will be deployed.

*** Step 4 - Deploy and Access Secret with Compose
Docker Compose Stack's are deployed using the Docker CLI. As part of the deployment, the stack will be configured with access to the secret.

Task
Deploy the task using the following command:

docker stack deploy -c docker-compose.yml secrets1

View the output with:

docker logs $(docker ps -aqn1 -f status=exited)

If the commands errors with "docker logs" requires exactly 1 argument(s). it means the container has not yet started and returned the secret.

*** Step 5 - File Based Secret
An alternate way of creating secrets is via files. In this case, we have a secret.crt file that needs to be accessed from the container.

Task
First, create the sample .crt file: echo "my-super-secure-cert" > secret.crt

Secondly, update the docker-compose Stack to use the file based secret.

Copy to Editorversion: '3.1'
#+BEGIN_SRC yaml
  services:
      test:
          image: 'alpine'
          command: 'cat /run/secrets/secretcert'
          secrets:
              - secretcert

  secrets:
      secretcert:
          file: ./secret.crt
#+END_SRC

*** Step 6 - Deploy and Access Secret with Compose
Task
As before, deploy the Docker Compose Stack.

docker stack deploy -c docker-compose.yml secrets2

The command below will get the log file of the last container to have exited for the newly created service.

docker logs $(docker ps -aqn1 -f name=secrets2 -f status=exited)

** Enable Maintenance Mode for a Swarm Node
In this scenario, you will learn how to put a Docker Swarm Mode worker node into maintenance mode. By putting a node into maintenance mode, all existing workloads will be restarted on other servers to ensure availability, and no new workloads will be started on the node.

Maintenance mode allows you to perform operations such as security updates or rebooting machines without the loss of availability.

*** Step 1: Create Swarm Cluster
By default, Docker works as an isolated single-node. All containers are only deployed onto the engine. Swarm Mode turns it into a multi-host cluster-aware engine.

Task: Initialise Swarm Mode
To use the secrets functionality, Docker has to be in "Swarm Mode". This is enabled via docker swarm init

Join Swarm Mode
Execute the command below on the second host to add it as a worker to the cluster.

token=$(ssh -o StrictHostKeyChecking=no 172.17.0.12 "docker swarm join-token -q worker") && docker swarm join 172.17.0.12:2377 --token $token

*** Step 2: Deploy Services
Start by deploying a HTTP server with two replicas across the two Swarm Mode nodes. The deployment will result in a container deployed onto each node.

Task
Create the deployment using the command below:

docker service create --name lbapp1 --replicas 2 -p 80:80 katacoda/docker-http-server

Watch the deployment status with docker service ls and docker ps

*** Step 3: Turn on Maintenance Mode
When maintenance is required, it's important to manage the process correctly to ensure reliability. The first action is to remove the node from the load balancer and let all active sessions complete. This will ensure that no requests are being sent to the host. Secondly, workloads on the system need to be redeployed to make sure that capacity is maintained.

Docker Swarm will manage this for you when setting the availability of a node.

Task
Setting the availability requires known the IP of the Swarm Mode. This is done using docker node ls. The command below will store the ID of the worker node.

worker=$(docker node ls | grep -v "Leader" | awk '{print $1}' | tail -n1); echo $worker

Setting the availability is done by updating the node. docker node update $worker --availability=drain

The containers should now be both running on the single manager node. docker ps

When viewing all the nodes, the availability will have changed. docker node ls

*** Step 4: Turn off Maintenance Mode
Once the work has been completed, the node should be made available for future workloads. This is done by settings the availability to active.

docker node update $worker --availability=active

The availability has now changed back.

docker node ls

It's important to note that Docker won't reschedule existing workloads. Looking at the containers, you will see that they're still both running on a single host.

docker ps

Instead, Swarm will only schedule new workloads onto the newly available host. This can be tested by scaling the number of replicas required.

docker service scale lbapp1=3

The new container will be scheduled onto the second node.

docker ps

** Apply Rolling Updates Across Swarm Cluster

In this scenario, you will learn how to apply rolling updates to your Services for configuration changes and new Docker Image versions without any downtime. The environment has been configured with two Docker Hosts.

A service is a high-level concept relating to a collection of tasks to be executed by workers. An example of a service is an HTTP Server running as a Docker Container on three nodes.

*** Step 1 - Update Limits
Services can be updated dynamically to control various settings and options. Internally, Docker manages how the updates should be applied. For certain commands, Docker will stop, remove and re-create the container. Potentially having all containers stopped at once is an important consideration regarding managing connections and uptime.

There are various settings you can control, view the help via docker service update --help

Task
To start, deploy a HTTP service. We will use this to update/modify the container settings.

docker swarm init && docker service create --name http --replicas 2 -p 80:80 katacoda/docker-http-server:v1

Once started, various properties can be updated. For example, adding a new environment variable to the containers. docker service update --env-add KEY=VALUE http

Alternatively, updating the CPU and memory limits. docker service update --limit-cpu 2 --limit-memory 512mb http

Once executed the results will be visible when you inspect the service. docker service inspect --pretty http

However, listing all container, you will see that they have been recreated with every update. docker ps -a.

*** Step 2 - Update Replicas
Not all updates require every container to be re-created. For example, scaling the number of replicas does not effect the existing containers.

Task
As an alternative to docker service scale, it is possible to use the update to define update how many replicas should be running. Below will update the replicas from two to six. Docker will then reschedule the additional four containers to be deployed.

docker service update --replicas=6 http

The number of replicas is viewable when inspecting the service docker service inspect --pretty http

*** Step 3 - Update Image
The most common scenario where updates will be used is when releasing a new version of the application via an updated Docker Image. As the Docker Image is a property of a container, it can be updated like the previous steps.

Task
The following command will re-create the instances of our HTTP service with :v2 tag of the Docker Image.

docker service update --image katacoda/docker-http-server:v2 http

If you open a new terminal window, you will notice that Swarm is performing a rolling update.

docker ps

By having a rolling update with multiple replicas, the application never goes down and you can perform zero-downtime deployments.

curl http://docker

The next step discusses how to control the rollout and zero-downtime deployments.

*** Step 4 - Rolling Updates
The aim is to deploy a new Docker Image without incurring any downtime. Zero downtime can be achieved by setting parallelism and a delay in the rollout. Docker can batch updates and perform them as a rollout across the cluster.

update-parallelism defines how many containers Docker should update at once. Depending on the number of replicas depends on how large you would batch up the requests.

update-delay defines how long to wait in-between each update batch. The delay is useful if you are application has a warm-up time, for example, starting the JVM or CLR. By specifying a delay, you can ensure that requests can still be processed while the process is starting.

Task
The two parameters are applied when running docker service update. In the example it will update one container at a time, waiting 10 seconds in-between each update. The update will be affecting the Docker Image used, but the parameters can apply to any of the possible update values

docker service update --update-delay=10s --update-parallelism=1 --image katacoda/docker-http-server:v3 http

After launching you will slowly see new v3 versions of the containers start and replace the existing v2. docker ps

Issuing HTTP requests to the load balancer will request it them being handled by both v2 and v3 containers resulting in a different output.

curl http://docker

It is important that your application can take this into account and handle two different versions being live concurrently.

** Load Balance and Service Discover in Swarm Mode

In this scenario, you will learn how to use Docker to load balance network traffic to different containers. With the introduction of Swarm Mode and Services, containers can now be logically grouped by a friendly name and port.

Requests to this name/port will be load balanced across all available containers in the cluster. This increases availability and the load distribution.

This functionality is provided as part of Swarm's routing mesh. Internally it's using the Linux IPVS, an in-kernel Layer 4 multi-protocol load balancer.

The environment has been configured with two Docker Hosts.

*** Step 1 - Initialise Cluster
Before beginning, initialise Swarm Mode and add the second host to the cluster.

Click the commands below to execute them.

docker swarm init

docker swarm join 172.17.0.46:2377 --token $(ssh -o StrictHostKeyChecking=no 172.17.0.46 "docker swarm join-token -q worker")

*** Step 2 - Port Load Balance
By default, requests to Services are load balanced based on the public port.

Task
The command below will create a new service called lbapp1 with two containers running. The service is exposed via port 81.

docker service create --name lbapp1 --replicas 2 -p 81:80 katacoda/docker-http-server

When requests are made to a node in our cluster on port 81, it will distribute the load across the two containers.

curl host01:81

The HTTP response indicates which container processed the request. Running the command on the second host has the same results, with it processing the request across both hosts.

curl host01:81

In the next step, we will explore how to use this to deploy a realistic application.

*** Step 3 - Virtual IP and Service Discovery
Docker Swarm Mode includes a Routing Mesh that enables multi-host networking. It allows containers on two different hosts to communicate as if they are on the same host. It does this by creating a Virtual Extensible LAN (VXLAN), designed for cloud-based networking.

The routing works in two different ways. Firstly, based on the public port exposed on the service. Any requests to the port will be distributed. Secondly, the service is given a Virtual IP address that is routable only inside the Docker Network. When requests are made to the IP address, they are distributed to the underlying containers. This Virtual IP is registered with the Embedded DNS server in Docker. When a DNS lookup is made based on the service name, the Virtual IP is returned.

In this step, you will create a load balanced http that is attached to an overlay network and look up it is Virtual IP.

Task
docker network create --attachable -d overlay eg1

This network will be a "swarm-scoped network". This means that only containers launched as a service can attach itself to the network.

docker service create --name http --network eg1 --replicas 2 katacoda/docker-http-server

By calling the service http, Docker adds an entry to it is embedded DNS server. Other containers on the network can use the friendly name to discovery the IP address. Along with ports, it is this IP address which can be used inside the network to reach the load balanced.

Use Dig to find the internal Virtual IP. By using the --attachable flag, a container outside of the Swarm service can access the network.

docker run --name=dig --network eg1 benhall/dig dig http

Pinging the name should also discover the IP address.

docker run --name=ping --network eg1 alpine ping -c5 http

This should match the Virtual IP given to the Service. You can discover this by inspecting the service.

docker service inspect http --format="{{.Endpoint.VirtualIPs}}"

Each container will still be given a unique IP addresses.

docker inspect --format="{{.NetworkSettings.Networks.eg1.IPAddress}}" $(docker ps | grep docker-http-server | head -n1 | awk '{print $1}')

This Virtual IP ensures that the load balancing works as expected within the cluster. While the IP address ensures it works outside the cluster.

*** Step 4 - Multi-Host LB and Service Discovery
Both the Virtual IP and Port Load Balancing and Service Discovery can be used in a multi-host scenario with applications communicating to different services on different hosts.

In this step, we will deploy a replicated Node.js application that communicates with Redis to store data.

Task
To start there needs to be an overlay network that the application and data store can connect to.

docker network create -d overlay app1-network

When deploying Redis, the network can be attached. The application expects to be able to connect to a Redis instance, named Redis. To enable the application to discover the Virtual IP via the Embedded DNS we call the service Redis.

docker service create --name redis --network app1-network redis:alpine

When deploying the application, a public port can be exposed allowing it to load balance the requests between the two containers.

docker service create --name app1-web --network app1-network --replicas 4 -p 80:3000 katacoda/redis-node-docker-example

Each host should have a Node.js container instance with one host storing Redis. docker ps

Calling the HTTP server will store the request in Redis and return the results. This is load balanced, with two containers talking across the overlay network to the Redis container.

curl host01

The application is now distributed across multiple hosts.

** Create Overlay Network

In this scenario you'll learn how to use Overlay Networks as part of Swarm Mode. Overlay networks allow containers to communicate as if they're on the same host. Under the covers they use VxLan features of the Linux Kernel.

Environment
The environment has been configured with two Docker machines that can communicate with each over TCP.

*** Step 1 - Initialise Swarm Mode
By default, Docker works as an isolated single-node. All containers are only deployed onto the engine. Swarm Mode turns it into a multi-host cluster-aware engine.

Task: Initialise Swarm Mode
To use the secrets functionality, Docker has to be in "Swarm Mode". This is enabled via docker swarm init

Join Swarm Mode
Execute the command below on the second host to add it as a worker to the cluster.

token=$(ssh -o StrictHostKeyChecking=no 172.17.0.63 "docker swarm join-token -q worker") && docker swarm join 172.17.0.63:2377 --token $token

*** Step 2 - Create Network
Overlay Networks are created using the Docker CLI, similar to creating a bridge network for connecting between hosts. When creating the network, a driver type of overlay is used. When new services are deployed via Swarm Mode, they can utilise this network allowing containers to communicate.

Task
To create the Overlay Network, use the CLI and define the driver. Networks can only be created via a Swarm Manager node. The network name would be app1-network.

docker network create -d overlay app1-network

All the networks can be viewed using:

docker network ls

Note: It's expected for the network not to appear on the worker nodes. The managers node handles network creation and services being deployed.

docker network ls

*** Step 3 - Deploy Backend
Once the network has been created, services can be deployed and able to communicate with other containers on the network.

Task
The following will deploy a Redis service using the network. The name of the service will be redis that can be used for discovery via DNS.

docker service create --name redis --network app1-network redis:alpine

The next step will deploy a web app on a different node that will interact with Redis over the network.

*** Step 4 - Deploy Frontend
With the overlay network and Redis deployed, it's now possible to deploy a Web App to use Redis to persist data. The application is configured to look up Redis via DNS. The app is configured to listen on port 3000, but the service will be exposed to the public on port 80.

Task
Create the new service will the command below:

docker service create \
    --network app1-network -p 80:3000 \
    --replicas 1 --name app1-web \
    katacoda/redis-node-docker-example

With a two-node deployment, each container will be deployed onto different hosts.

docker ps

They'll use the overlay network and DNS discovery to communicate.

Test
Sending a HTTP request will persist the IP of the client in Redis.

curl host01

As the service has been configured and deployed using Swarm Mode, it will take advantage of the load balancing discussing in our scenario Load Balance and Service Discover in Swarm Mode

curl host01

* Compose

#+begin_src yaml
  version: '3.4'

  x-rabbit: &rabbit
    image: rabbitmq:3.8.5-management-alpine
    ports:
    - 4369:4369
    - 5672:5672
    - 5671:5671
    - 25672:25672
    - 35672-35682:35672-35682
    - 15672:15672
    - 61613:61613
    - 61614:61614
    - 1883:1883
    - 8883:8883
    - 15674:15674
    - 15675:15675
    - 15692:15692
    environment:
    - RABBITMQ_DEFAULT_USER=spring
    - RABBITMQ_DEFAULT_PASS=spring
    - RABBITMQ_NODENAME=rabbit@rabbit-dh
    - RABBITMQ_ERLANG_COOKIE=EJHSDBCQHWCHBHSZPMIE
    extra_hosts:
    - "78.108.86.20 r1"
    - "78.108.87.99 r2"
    - "178.250.246.123 r3"
    volumes:
    - rabbit-data:/var/lib/rabbitmq
    logging:
      driver: json-file
      options:
        max-size: 100m
        max-file: 2

  services:
    rabbit1:
      <<: *rabbit
      hostname: r1
      deploy: 
        placement:
          constraints:
          - node.hostname == r1

    rabbit2:
      <<: *rabbit
      hostname: r2
      deploy: 
        placement:
          constraints:
          - node.hostname == r2

    rabbit3:
      <<: *rabbit
      hostname: r3
      deploy: 
        placement:
          constraints:
          - node.hostname == r3

  volumes:
    rabbit-data:
#+end_src

#+BEGIN_SRC yaml
  version: "2"

  networks:
    gitea:
      external: false

  services:
    server:
      image: gitea/gitea:latest
      extra_hosts:
        - "db:192.168.105.120"
      environment:
        - USER_UID=1000
        - USER_GID=1000
        - DB_TYPE=postgres
        - DB_HOST=db:5432
        - DB_NAME=gitea
        - DB_USER=gitea
        - DB_PASSWD=gitea
        - SSH_DOMAIN=gitea.wugi.info
      restart: always
      networks:
        - gitea
      volumes:
        - /var/lib/gitea:/data
      ports:
        - "3000:3000"
        - "222:22"
  #    depends_on:
  #      - db
  #  db:
  #    image: postgres:9.6
  #    restart: always
  #    environment:
  #      - POSTGRES_USER=gitea
  #      - POSTGRES_PASSWORD=gitea
  #      - POSTGRES_DB=gitea
  #    networks:
  #      - gitea
  #    volumes:
  #      - ./postgres:/var/lib/postgresql/data
#+END_SRC

* systemd containers

https://developers.redhat.com/blog/2016/09/13/running-systemd-in-a-non-privileged-container/
https://developers.redhat.com/blog/2019/04/24/how-to-run-systemd-in-a-container/

1. Create and mount systemd cgroup
#+BEGIN_SRC bash
  mkdir /sys/fs/cgroup/systemd
  mount -t cgroup cgroup -o none,name=systemd /sys/fs/cgroup/systemd
#+END_SRC

2. Run container
#+BEGIN_SRC bash
  docker run                                                              \
          --name fedora                                                   \
          --publish 8085:80 -d                                            \
          --tmpfs /tmp                                                    \
          --tmpfs /run                                                    \
          -v /sys/fs/cgroup:/sys/fs/cgroup:ro                 \
          httpd "$@"

#+END_SRC

1/2 ... dockerfile
#+BEGIN_SRC dockerfile
  FROM fedora:31
  ENV container docker
  RUN dnf -y install httpd; dnf clean all; systemctl enable httpd
  STOPSIGNAL SIGRTMIN+3
  EXPOSE 80
  CMD [ "/sbin/init" ]
#+END_SRC
: docker build -t httpd .

Misc
#+begin_example
  --entrypoint '' \
  -it \
  --tmpfs /sys/fs/cgroup                                          \
  -v /run/j3K4a/systemd:/sys/fs/cgroup/systemd:rw                 \
  -v /sys/fs/cgroup/blkio:/sys/fs/cgroup/blkio:ro                 \
  -v /sys/fs/cgroup/cpu:/sys/fs/cgroup/cpu:ro                     \
  -v /sys/fs/cgroup/cpuacct:/sys/fs/cgroup/cpuacct:ro             \
  -v /sys/fs/cgroup/cpuset:/sys/fs/cgroup/cpuset:ro               \
  -v /sys/fs/cgroup/devices:/sys/fs/cgroup/devices:ro             \
  -v /sys/fs/cgroup/elogind:/sys/fs/cgroup/elogind:ro             \
  -v /sys/fs/cgroup/freezer:/sys/fs/cgroup/freezer:ro             \
  -v /sys/fs/cgroup/memory:/sys/fs/cgroup/memory:ro               \
  -v /sys/fs/cgroup/perf_event:/sys/fs/cgroup/perf_event:ro       \
  -v /sys/fs/cgroup/pids:/sys/fs/cgroup/pids:ro                   \
  -v /sys/fs/cgroup/unified:/sys/fs/cgroup/unified:ro             \
#+end_example

* Swarm

  #+begin_src yaml
    version: '3.4'

    x-rabbit: &rabbit
      image: 178.250.246.123:5000/rabbitmq # rabbitmq:3.8.5-management-alpine
      environment:
      - RABBITMQ_DEFAULT_USER=spring
      - RABBITMQ_DEFAULT_PASS=spring
      - RABBITMQ_USE_LONGNAME=true
      - RABBITMQ_NODENAME={{.Service.Name}}
      - RABBITMQ_ERLANG_COOKIE=EJHSDBCQHWCHBHSZPMIE
      - SERVICE_NAME={{.Service.Name}}
      hostname: "{{.Service.Name}}"
      volumes:
      - rabbit-data:/var/lib/rabbitmq
      logging:
        driver: json-file
        options:
          max-size: 100m
          max-file: 2
      healthcheck:
        test: ["CMD", "nc", "-z", "localhost", "15672"] # TODO: change port
        interval: 1m30s
        timeout: 10s
        retries: 3
        start_period: 40s
      deploy:
        restart_policy:
          condition: on-failure

    services:
      rabbit1:
        <<: *rabbit
        hostname: r1
        ports:
          - 15672:15672
        deploy: 
          placement:
            constraints:
            - node.hostname == r1

      rabbit2:
        <<: *rabbit
        hostname: r2
        deploy: 
          placement:
            constraints:
            - node.hostname == r2

      rabbit3:
        <<: *rabbit
        hostname: r3
        deploy: 
          placement:
            constraints:
            - node.hostname == r3

    volumes:
      rabbit-data:

  #+end_src
