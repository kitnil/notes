:PROPERTIES:
:ID:       e430d654-a7b0-4625-b4be-56d697c0d142
:END:

* Documentation

- [[https://www.elastic.co/guide/en/elasticsearch/reference/6.6/index.html][Elasticsearch Guide [6.6] | Elastic]]

* Cheet sheet

- [[https://www.elastic.co/guide/en/elasticsearch/reference/current/xpack-ccr.html][Cross-cluster replication | Elasticsearch Guide [7.15] | Elastic]]
- [[https://github.com/wfxr/elastic-tunnel][wfxr/elastic-tunnel: Tools for downloading data from elasticsearch]]
- [[https://github.com/wfxr/estunnel][wfxr/estunnel: Tool for downloading data from elasticsearch cluster.]]

- [[https://github.com/cch123/elasticsql][cch123/elasticsql: convert sql to elasticsearch DSL in golang(go)]]

- [[https://github.com/lmangani/sentinl][lmangani/sentinl: Kibana Alert & Report App for Elasticsearch]]

- [[https://archivy.github.io/][Index - Archivy]]

- https://gist.github.com/ruanbekker/e8a09604b14f37e8d2f743a87b930f93

- [[https://github.com/outflanknl/RedELK/][outflanknl/RedELK: Red Team's SIEM - tool for Red Teams used for tracking and alarming about Blue Team activities as well as better usability in long term operations.]]

- [[https://github.com/shirosaidev/diskover][shirosaidev/diskover: File system crawler, disk space usage, file search engine and file system analytics powered by Elasticsearch]]

- [[https://github.com/maxyermayank/docker-compose-elasticsearch-kibana][maxyermayank/docker-compose-elasticsearch-kibana: Docker Compose for Elasticsearch and Kibana]]

- [[https://github.com/elastic/examples][elastic/examples: Home for Elasticsearch examples available to everyone. It's a great way to get started.]] 

- [[https://github.com/bitemyapp/bloodhound][bitemyapp / bloodhound Haskell Elasticsearch client and query DSL]]

- root@web15 /etc/filebeat # filebeat -e -d "publish"

- [[https://www.elastic.co/guide/en/elasticsearch/reference/6.8/array.html][Arrays | Elasticsearch Reference [6.8] | Elastic]]

- list hot threads
  : curl '127.0.0.1:9200/_nodes/hot_threads'
  : e.g.: xsdCCWz

- kill threads
  : for thread in $(curl '127.0.0.1:9200/_cat/tasks?detailed' | grep xsdCCWz | grep read | awk '{ print $2 }'); do curl -XPOST "127.0.0.1:9200/_tasks/$thread/_cancel"; done

- [[https://www.elastic.co/guide/en/elasticsearch/reference/6.8/indices-templates.html#indices-templates][Index Templates | Elasticsearch Reference [6.8] | Elastic]]

- Disable readonly for all indexes
  : curl -XPUT -H "Content-Type: application/json" http://localhost:9200/_all/_settings -d '{"index.blocks.read_only_allow_delete": null}'
  https://techoverflow.net/2019/04/17/how-to-fix-elasticsearch-forbidden-12-index-read-only-allow-delete-api/

- Disable readonly
  : curl -H 'Content-Type: application/json' -XPUT 'http://localhost:9200/filebeat-*/_settings' --data '{"index":{"blocks": {"read_only_allow_delete": "false"}}}'
  (could use ‚Äú*‚Äù instead of ‚Äúelastalert_status_status‚Äù)

- exclude rc-user
  : service:rc-user AND log_level:ERROR AND -log_message:"ftp"

- https://qbox.io/blog/indexing-emails-to-elasticsearch-logstash-imap

- [[https://github.com/opensearch-project/OpenSearch][opensearch-project/OpenSearch: Open source distributed and RESTful search engine.]]

- [[https://github.com/elastic/elasticsearch-dsl-py][elastic/elasticsearch-dsl-py: High level Python client for Elasticsearch]]

- [[https://youtu.be/HSXuGU6f0yo][Kibana Searches]]

- setup filebeat templates for kibana
  : filebeat setup -e -strict.perms=false -E output.elasticsearch.hosts=[elasticsearch:9200] -E setup.kibana.host="http://172.17.0.1:5601" -E name=guixsd

- Create index
  : curl -XPUT localhost:9200/foo

- List indexes
  : curl 'localhost:9200/_cat/indices?v&pretty'

- Search
  : curl -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?q=server:web37&pretty"
  : curl -H 'Content-Type: application/json' -d '{"query": {"dis_max": {"queries": [{"match": {"server": "web37"}}, {"match": {"OPERATION_IDENTITY": "LOCAL-SCHED"}}]}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty&size=1000" | jq -r '.hits.hits[] | ._source.ACTION_IDENTITY'
  : curl -H 'Content-Type: application/json' -d '{"query": {"dis_max": {"queries": [{"match": {"server": "web37"}}, {"match": {"OPERATION_IDENTITY": "LOCAL-SCHED"}}]}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty&size=2000" | jq --monochrome-output -r '.hits.hits[] | [._source.ACTION_IDENTITY, ._source.log_message] | @tsv' | grep -v 'malware_report\|saved in'
  : curl -H 'Content-Type: application/json' -d '{"query": {"dis_max": {"queries": [{"match": {"server": "web37"}}, {"match": {"OPERATION_IDENTITY": "LOCAL-SCHED"}}, {"match": {"ACTION_IDENTITY": "unix-account.backup.*"}}]}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty&size=10000" | jq --monochrome-output -r '.hits.hits[] | [._source.ACTION_IDENTITY, ._source.log_message] | @tsv' | wc -l

- Map field
  #+BEGIN_SRC sh
    curl -H "Content-Type: application/json" -XPUT --data-binary \
         '{"properties": {"upload_date": {"type": "date", "format": "yyyyMMdd"}, "title": {"type": "text", "fields":{"keyword":{"type":"keyword","ignore_above":256}}}}}' \
         localhost:9200/youtube-2019.02.10/_mapping/_doc
  #+END_SRC

- Import JSON to Elasticsearch
  : cat /tmp/dio.txt | jq -c '.entries[] | { index: { "_index": "youtube", "_type": "_doc", _id: .id }}, { upload_date: .upload_date, channel_id: .channel_id, title: .title, webpage_url: .webpage_url_basename }' | curl -H "Content-Type: application/json" -XPOST localhost:9200/_bulk --data-binary @-

- Download YouTube channel JSON
  : youtube-dl --ignore-errors -J https://www.youtube.com/user/gotbletu/videos > /tmp/gotbletu.txt

- Create backup repository
  : curl -H "Content-Type: application/json" -XPUT 'http://localhost:9200/_snapshot/youtubee": "fs", "settings": {"compress": true, "location": "/mnt/backup"}}'

- Backup Index 
  : curl -H "Content-Type: application/json" -XPUT 'http://localhost:9200/_snapshot/youtube_fs_backup/snapshot_1?wait_for_completion=true' -d '{"indices": "youtube", "ignore_unavailable": true, "include_global_state": false}'

- Create alias
  : curl -X POST "localhost:9200/_aliases" -H 'Content-Type: application/json' -d'{"actions":[{"add":{"index":"yt-game","alias":"yt"}}]}'

- Reindex
  #+begin_example
    import elasticsearch
    import elasticsearch.helpers

    elastic = elasticsearch.Elasticsearch([{"host": "localhost", "port": 9200}])

    elasticsearch.helpers.reindex(client=elastic, target_client=elastic, source_index="youtube-gaming", target_index="yt-game")
  #+end_example
  
* WIP

oleg@guixsd ~$ curl -H 'Content-Type: application/json' -d '{"query": {"match": {"server": "web37", "OPERATION_IDENTITY": "LOCAL-SCHED"}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty" 
{
  "error" : {
    "root_cause" : [
      {
        "type" : "parsing_exception",
        "reason" : "[match] query doesn't support multiple fields, found [server] and [OPERATION_IDENTITY]",
        "line" : 1,
        "col" : 63
      }
    ],
    "type" : "parsing_exception",
    "reason" : "[match] query doesn't support multiple fields, found [server] and [OPERATION_IDENTITY]",
    "line" : 1,
    "col" : 63
  },
  "status" : 400
}

curl -H 'Content-Type: application/json' -d '{"query": {"bool": {"must": {"term": {"server": "web37"}}}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty" 

* Cluster

- health
  : curl -XGET 'http://127.0.0.1:9200/_cluster/health?pretty'

- cluster_uuid
  : curl -XGET 'http://localhost:9200/_cluster/state/master_node?pretty'

- cluster nodes
  : curl -XGET 'http://localhost:9200/_cluster/state/nodes?pretty'

- drain node
  : curl -XPUT localhost:9200/_cluster/settings -H 'Content-Type: application/json' -d '{"transient" :{"cluster.routing.allocation.exclude._ip" : "10.0.0.1"}}'

** es3
#+begin_example
  cluster.name: mjlogger

  http.port: 9200
  transport.tcp.port: 9300

  node.name: "staff-vote-only"
  node.data: false
  node.master: true
  node.ingest: true

  path.repo: ["/home/elasticsearch_backups"]
  xpack.security.enabled: false

  discovery.zen.minimum_master_nodes: 2
  discovery.zen.ping.unicast.hosts: [ "172.16.103.68", "172.16.103.69", "172.16.103.112" ]
#+end_example

* Tools

- [[https://github.com/Cyb3rWard0g/HELK][Cyb3rWard0g/HELK: The Hunting ELK]]
- [[https://github.com/binwiederhier/elastictl][binwiederhier/elastictl: Simple tool to import/export Elasticsearch indices into a file, and/or reshard an index]]

* Alternatives

- [[https://github.com/valeriansaliou/sonic][valeriansaliou/sonic: ü¶î Fast, lightweight & schema-less search backend. An alternative to Elasticsearch that runs on a few MBs of RAM.]]

* Kibana
- [[https://habr.com/ru/company/citymobil/blog/521802/][–°–æ–∑–¥–∞–Ω–∏–µ Dashboard –≤ Kibana –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ª–æ–≥–æ–≤ / –ë–ª–æ–≥ –∫–æ–º–ø–∞–Ω–∏–∏ –°–∏—Ç–∏–º–æ–±–∏–ª / –•–∞–±—Ä]]

* Misc
- [[https://www.google.com/search?q=elastic+list+replicate+specific+shards&hl=en][elastic list replicate specific shards - Google Search]]
- [[https://logz.io/blog/elasticsearch-cheat-sheet/][A Useful Elasticsearch Cheat Sheet in Times of Trouble | Logz.io]]
- [[https://stackoverflow.com/questions/15694724/shards-and-replicas-in-elasticsearch][full text search - Shards and replicas in Elasticsearch - Stack Overflow]]
- [[https://opster.com/blogs/elasticsearch-shards-and-replicas-getting-started-guide/][Elasticsearch Shards and Replicas getting started guide - Opster]]
- [[https://linuxhint.com/elasticsearch-shard-list/][Elasticsearch Shard List]]
- [[https://www.elastic.co/guide/en/elasticsearch/reference/6.6/cat-shards.html][cat shards | Elasticsearch Guide [6.6] | Elastic]]
