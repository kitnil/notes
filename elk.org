- root@web15 /etc/filebeat # filebeat -e -d "publish"

- [https://www.elastic.co/guide/en/elasticsearch/reference/6.8/array.html][Arrays | Elasticsearch Reference [6.8] | Elastic]

- list hot threads
  : curl '127.0.0.1:9200/_nodes/hot_threads'
  : e.g.: xsdCCWz

- kill threads
  : for thread in $(curl '127.0.0.1:9200/_cat/tasks?detailed' | grep xsdCCWz | grep read | awk '{ print $2 }'); do curl -XPOST "127.0.0.1:9200/_tasks/$thread/_cancel"; done

- [https://www.elastic.co/guide/en/elasticsearch/reference/6.8/indices-templates.html#indices-templates][Index Templates | Elasticsearch Reference [6.8] | Elastic]

- Disable readonly for all indexes
  : curl -XPUT -H "Content-Type: application/json" http://localhost:9200/_all/_settings -d '{"index.blocks.read_only_allow_delete": null}'
  https://techoverflow.net/2019/04/17/how-to-fix-elasticsearch-forbidden-12-index-read-only-allow-delete-api/

- Disable readonly
  : curl -H 'Content-Type: application/json' -XPUT 'http://localhost:9200/filebeat-*/_settings' --data '{"index":{"blocks": {"read_only_allow_delete": "false"}}}'
  (could use “*” instead of “elastalert_status_status”)

- exclude rc-user
  : service:rc-user AND log_level:ERROR AND -log_message:"ftp"

- https://qbox.io/blog/indexing-emails-to-elasticsearch-logstash-imap

- [[https://youtu.be/HSXuGU6f0yo][Kibana Searches]]

- setup filebeat templates for kibana
  : filebeat setup -e -strict.perms=false -E output.elasticsearch.hosts=[elasticsearch:9200] -E setup.kibana.host="http://172.17.0.1:5601" -E name=guixsd

- Create index
  : curl -XPUT localhost:9200/foo

- List indexes
  : curl 'localhost:9200/_cat/indices?v&pretty'

- Search
  : curl -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?q=server:web37&pretty"
  : curl -H 'Content-Type: application/json' -d '{"query": {"dis_max": {"queries": [{"match": {"server": "web37"}}, {"match": {"OPERATION_IDENTITY": "LOCAL-SCHED"}}]}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty&size=1000" | jq -r '.hits.hits[] | ._source.ACTION_IDENTITY'

- Map field
  #+BEGIN_SRC sh
    curl -H "Content-Type: application/json" -XPUT --data-binary \
         '{"properties": {"upload_date": {"type": "date", "format": "yyyyMMdd"}, "title": {"type": "text", "fields":{"keyword":{"type":"keyword","ignore_above":256}}}}}' \
         localhost:9200/youtube-2019.02.10/_mapping/_doc
  #+END_SRC

- Import JSON to Elasticsearch
  : cat /tmp/dio.txt | jq -c '.entries[] | { index: { "_index": "youtube", "_type": "_doc", _id: .id }}, { upload_date: .upload_date, channel_id: .channel_id, title: .title, webpage_url: .webpage_url_basename }' | curl -H "Content-Type: application/json" -XPOST localhost:9200/_bulk --data-binary @-

- Download YouTube channel JSON
  : youtube-dl --ignore-errors -J https://www.youtube.com/user/gotbletu/videos > /tmp/gotbletu.txt

- Create backup repository
  : curl -H "Content-Type: application/json" -XPUT 'http://localhost:9200/_snapshot/youtubee": "fs", "settings": {"compress": true, "location": "/mnt/backup"}}'

- Backup Index 
  : curl -H "Content-Type: application/json" -XPUT 'http://localhost:9200/_snapshot/youtube_fs_backup/snapshot_1?wait_for_completion=true' -d '{"indices": "youtube", "ignore_unavailable": true, "include_global_state": false}'

- Create alias
  : curl -X POST "localhost:9200/_aliases" -H 'Content-Type: application/json' -d'{"actions":[{"add":{"index":"yt-game","alias":"yt"}}]}'

- Reindex
  #+BEGIN_SRC python
    import elasticsearch
    import elasticsearch.helpers

    elastic = elasticsearch.Elasticsearch([{"host": "localhost", "port": 9200}])

    elasticsearch.helpers.reindex(client=elastic, target_client=elastic, source_index="youtube-gaming", target_index="yt-game")
  #+END_SRC

* WIP

oleg@guixsd ~$ curl -H 'Content-Type: application/json' -d '{"query": {"match": {"server": "web37", "OPERATION_IDENTITY": "LOCAL-SCHED"}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty" 
{
  "error" : {
    "root_cause" : [
      {
        "type" : "parsing_exception",
        "reason" : "[match] query doesn't support multiple fields, found [server] and [OPERATION_IDENTITY]",
        "line" : 1,
        "col" : 63
      }
    ],
    "type" : "parsing_exception",
    "reason" : "[match] query doesn't support multiple fields, found [server] and [OPERATION_IDENTITY]",
    "line" : 1,
    "col" : 63
  },
  "status" : 400
}

curl -H 'Content-Type: application/json' -d '{"query": {"bool": {"must": {"term": {"server": "web37"}}}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty" 
