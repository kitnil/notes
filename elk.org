:PROPERTIES:
:ID:       e430d654-a7b0-4625-b4be-56d697c0d142
:END:

* Learning

- [[https://habr.com/ru/company/southbridge/blog/510822/][–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ Kubernetes: EFK –ø—Ä–æ—Ç–∏–≤ PLG / –•–∞–±—Ä]]

** Documentation

- [[https://www.elastic.co/guide/en/elasticsearch/reference/6.6/index.html][Elasticsearch Guide [6.6] | Elastic]]

* Cheet sheet

- [[https://www.elastic.co/guide/en/elasticsearch/reference/current/xpack-ccr.html][Cross-cluster replication | Elasticsearch Guide [7.15] | Elastic]]
- [[https://github.com/wfxr/elastic-tunnel][wfxr/elastic-tunnel: Tools for downloading data from elasticsearch]]
- [[https://github.com/wfxr/estunnel][wfxr/estunnel: Tool for downloading data from elasticsearch cluster.]]

- [[https://github.com/cch123/elasticsql][cch123/elasticsql: convert sql to elasticsearch DSL in golang(go)]]

- [[https://github.com/lmangani/sentinl][lmangani/sentinl: Kibana Alert & Report App for Elasticsearch]]

- [[https://archivy.github.io/][Index - Archivy]]

- https://gist.github.com/ruanbekker/e8a09604b14f37e8d2f743a87b930f93

- [[https://github.com/outflanknl/RedELK/][outflanknl/RedELK: Red Team's SIEM - tool for Red Teams used for tracking and alarming about Blue Team activities as well as better usability in long term operations.]]

- [[https://github.com/shirosaidev/diskover][shirosaidev/diskover: File system crawler, disk space usage, file search engine and file system analytics powered by Elasticsearch]]

- [[https://github.com/maxyermayank/docker-compose-elasticsearch-kibana][maxyermayank/docker-compose-elasticsearch-kibana: Docker Compose for Elasticsearch and Kibana]]

- [[https://github.com/elastic/examples][elastic/examples: Home for Elasticsearch examples available to everyone. It's a great way to get started.]] 

- [[https://github.com/bitemyapp/bloodhound][bitemyapp / bloodhound Haskell Elasticsearch client and query DSL]]

- root@web15 /etc/filebeat # filebeat -e -d "publish"

- [[https://www.elastic.co/guide/en/elasticsearch/reference/6.8/array.html][Arrays | Elasticsearch Reference [6.8] | Elastic]]

- list hot threads
  : curl '127.0.0.1:9200/_nodes/hot_threads'
  : e.g.: xsdCCWz

- kill threads
  : for thread in $(curl '127.0.0.1:9200/_cat/tasks?detailed' | grep xsdCCWz | grep read | awk '{ print $2 }'); do curl -XPOST "127.0.0.1:9200/_tasks/$thread/_cancel"; done

- [[https://www.elastic.co/guide/en/elasticsearch/reference/6.8/indices-templates.html#indices-templates][Index Templates | Elasticsearch Reference [6.8] | Elastic]]

- Disable readonly for all indexes
  : curl -XPUT -H "Content-Type: application/json" http://localhost:9200/_all/_settings -d '{"index.blocks.read_only_allow_delete": null}'
  https://techoverflow.net/2019/04/17/how-to-fix-elasticsearch-forbidden-12-index-read-only-allow-delete-api/

- Disable readonly
  : curl -H 'Content-Type: application/json' -XPUT 'http://localhost:9200/filebeat-*/_settings' --data '{"index":{"blocks": {"read_only_allow_delete": "false"}}}'
  (could use ‚Äú*‚Äù instead of ‚Äúelastalert_status_status‚Äù)

- exclude rc-user
  : service:rc-user AND log_level:ERROR AND -log_message:"ftp"

- https://qbox.io/blog/indexing-emails-to-elasticsearch-logstash-imap

- [[https://github.com/opensearch-project/OpenSearch][opensearch-project/OpenSearch: Open source distributed and RESTful search engine.]]

- [[https://github.com/elastic/elasticsearch-dsl-py][elastic/elasticsearch-dsl-py: High level Python client for Elasticsearch]]

- [[https://youtu.be/HSXuGU6f0yo][Kibana Searches]]

- create filebeat index in opensearch-dashboards
  : kubectl -n opensearch exec pod/opensearch-dashboards-xxxxxxxxxx-xxxxx -- curl -u admin:PASSWORD -X POST 'http://opensearch-dashboards.opensearch:5601/api/saved_objects/index-pattern/filebeat-*' -H "osd-xsrf:true" -H "content-type:application/json" -d '{"attributes": {"title": "filebeat-*", "timeFieldName": "@timestamp"}}'

- setup filebeat templates for kibana
  : filebeat setup -e -strict.perms=false -E output.elasticsearch.hosts=[elasticsearch:9200] -E setup.kibana.host="http://172.17.0.1:5601" -E name=guixsd

- Create index
  : curl -XPUT localhost:9200/foo

- List indexes
  : curl 'localhost:9200/_cat/indices?v&pretty'

- List yellow indexes
  : curl 'es.intr:9200/_cat/indices?health=yellow&v&pretty'

- Explain
  : curl -H 'Content-Type: application/json' -d '{"index": "nginx-2022.02.03", "shard": 0, "primary": true}' 'es.intr:9200/_cluster/allocation/explain?pretty' 

- Search
  : curl -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?q=server:web37&pretty"
  : curl -H 'Content-Type: application/json' -d '{"query": {"dis_max": {"queries": [{"match": {"server": "web37"}}, {"match": {"OPERATION_IDENTITY": "LOCAL-SCHED"}}]}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty&size=1000" | jq -r '.hits.hits[] | ._source.ACTION_IDENTITY'
  : curl -H 'Content-Type: application/json' -d '{"query": {"dis_max": {"queries": [{"match": {"server": "web37"}}, {"match": {"OPERATION_IDENTITY": "LOCAL-SCHED"}}]}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty&size=2000" | jq --monochrome-output -r '.hits.hits[] | [._source.ACTION_IDENTITY, ._source.log_message] | @tsv' | grep -v 'malware_report\|saved in'
  : curl -H 'Content-Type: application/json' -d '{"query": {"dis_max": {"queries": [{"match": {"server": "web37"}}, {"match": {"OPERATION_IDENTITY": "LOCAL-SCHED"}}, {"match": {"ACTION_IDENTITY": "unix-account.backup.*"}}]}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty&size=10000" | jq --monochrome-output -r '.hits.hits[] | [._source.ACTION_IDENTITY, ._source.log_message] | @tsv' | wc -l

- Map field
  #+BEGIN_SRC sh
    curl -H "Content-Type: application/json" -XPUT --data-binary \
         '{"properties": {"upload_date": {"type": "date", "format": "yyyyMMdd"}, "title": {"type": "text", "fields":{"keyword":{"type":"keyword","ignore_above":256}}}}}' \
         localhost:9200/youtube-2019.02.10/_mapping/_doc
  #+END_SRC

- Import JSON to Elasticsearch
  : cat /tmp/dio.txt | jq -c '.entries[] | { index: { "_index": "youtube", "_type": "_doc", _id: .id }}, { upload_date: .upload_date, channel_id: .channel_id, title: .title, webpage_url: .webpage_url_basename }' | curl -H "Content-Type: application/json" -XPOST localhost:9200/_bulk --data-binary @-

- Download YouTube channel JSON
  : youtube-dl --ignore-errors -J https://www.youtube.com/user/gotbletu/videos > /tmp/gotbletu.txt

- Create backup repository
  : curl -H "Content-Type: application/json" -XPUT 'http://localhost:9200/_snapshot/youtubee": "fs", "settings": {"compress": true, "location": "/mnt/backup"}}'

- Backup Index 
  : curl -H "Content-Type: application/json" -XPUT 'http://localhost:9200/_snapshot/youtube_fs_backup/snapshot_1?wait_for_completion=true' -d '{"indices": "youtube", "ignore_unavailable": true, "include_global_state": false}'

- Create alias
  : curl -X POST "localhost:9200/_aliases" -H 'Content-Type: application/json' -d'{"actions":[{"add":{"index":"yt-game","alias":"yt"}}]}'

- Reindex
  #+begin_example
    import elasticsearch
    import elasticsearch.helpers

    elastic = elasticsearch.Elasticsearch([{"host": "localhost", "port": 9200}])

    elasticsearch.helpers.reindex(client=elastic, target_client=elastic, source_index="youtube-gaming", target_index="yt-game")
  #+end_example

- Watermark
#+begin_example
  curl -X PUT "es.intr:9200/_cluster/settings?pretty" -H 'Content-Type: application/json' -d'{"transient": {"cluster.routing.allocation.disk.watermark.low": "25gb", "cluster.routing.allocation.disk.watermark.high": "15gb", "cluster.routing.allocation.disk.watermark.flood_stage": "5gb"}}'
#+end_example

- [[https://groups.google.com/g/wazuh/c/lc-NvBVAQcI][Increase number of shards per node]]

  : $ curl -X PUT opensearch.home/_cluster/settings -H "Content-Type: application/json" -d '{ "persistent": { "cluster.max_shards_per_node": "3000" } }'
  # outputs
  : {"acknowledged":true,"persistent":{"cluster":{"max_shards_per_node":"3000"}},"transient":{}}
  
* WIP

oleg@guixsd ~$ curl -H 'Content-Type: application/json' -d '{"query": {"match": {"server": "web37", "OPERATION_IDENTITY": "LOCAL-SCHED"}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty" 
{
  "error" : {
    "root_cause" : [
      {
        "type" : "parsing_exception",
        "reason" : "[match] query doesn't support multiple fields, found [server] and [OPERATION_IDENTITY]",
        "line" : 1,
        "col" : 63
      }
    ],
    "type" : "parsing_exception",
    "reason" : "[match] query doesn't support multiple fields, found [server] and [OPERATION_IDENTITY]",
    "line" : 1,
    "col" : 63
  },
  "status" : 400
}

curl -H 'Content-Type: application/json' -d '{"query": {"bool": {"must": {"term": {"server": "web37"}}}}}' -s -X GET "elastic.intr:9200/logstash-te-2020.02.28/_search?pretty" 

* Cluster

- health
  : curl -XGET 'http://127.0.0.1:9200/_cluster/health?pretty'

- cluster_uuid
  : curl -XGET 'http://localhost:9200/_cluster/state/master_node?pretty'

- cluster nodes
  : curl -XGET 'http://localhost:9200/_cluster/state/nodes?pretty'

- drain node
  : curl -XPUT localhost:9200/_cluster/settings -H 'Content-Type: application/json' -d '{"transient" :{"cluster.routing.allocation.exclude._ip" : "172.16.103.69"}}'
  : {"acknowledged":true,"persistent":{},"transient":{"cluster":{"routing":{"allocation":{"exclude":{"_ip":"172.16.103.69"}}}}}}

#+begin_example
  oleg@guixsd ~$ curl -s es.intr:9200/_cat/shards?pretty  | grep kvm15
  logstash-payment-2021.05.03       0 p STARTED         985    237kb 172.16.103.101 kvm15-master
  logstash-payment-2021.09.17       0 p STARTED        2014  367.4kb 172.16.103.101 kvm15-master
  .monitoring-es-6-2021.04.14       0 r STARTED        8637      3mb 172.16.103.101 kvm15-master
  .monitoring-es-6-2021.11.01       0 r STARTED        8635    3.2mb 172.16.103.101 kvm15-master
  .monitoring-es-6-2020.07.19       0 r STARTED        8639      3mb 172.16.103.101 kvm15-master
  .monitoring-es-6-2021.07.24       0 r STARTED        8637    2.9mb 172.16.103.101 kvm15-master
  cerb_message_content              2 r RELOCATING  2220954      3gb 172.16.103.69  es2-master -> 172.16.103.101 QPHQ1pd4R6qVOuTuXJbwMQ kvm15-master
  .monitoring-es-6-2019.02.19       0 r STARTED        8634    2.9mb 172.16.103.101 kvm15-master
  payment-listeners-2021.11.24      0 p STARTED          16   38.8kb 172.16.103.101 kvm15-master
  juniper-2021.12.08                0 p STARTED        9577  748.3kb 172.16.103.101 kvm15-master
  logstash-mail-2021.12.03          0 p RELOCATING  7697918  915.1mb 172.16.103.69  es2-master -> 172.16.103.101 QPHQ1pd4R6qVOuTuXJbwMQ kvm15-master
  .monitoring-es-6-2020.05.21       0 r STARTED        8639      3mb 172.16.103.101 kvm15-master
#+end_example

- opensearch compatible with filebeat oss
  : curl -XPUT -H 'Content-Type: application/json' -d '{"persistent":{"compatibility":{"override_main_response_version":true}}}' -k -u admin:PASSWORD https://opensearch-cluster-master.opensearch:9200/_cluster/settings

- disk watermark
#+begin_example
  curl -X PUT "es.intr:9200/_cluster/settings?pretty" -H 'Content-Type: application/json' -d'
  {
    "transient": {
      "cluster.routing.allocation.disk.watermark.low": "100gb",
      "cluster.routing.allocation.disk.watermark.high": "50gb",
      "cluster.routing.allocation.disk.watermark.flood_stage": "10gb"

    }
  }
  '
#+end_example

- nodes stats
  : curl es.intr:9200/_nodes/stats

- nodes disks
  : curl -XGET 'http://es.intr:9200/_cat/allocation?v'

** es3
#+begin_example
  cluster.name: mjlogger

  http.port: 9200
  transport.tcp.port: 9300

  node.name: "staff-vote-only"
  node.data: false
  node.master: true
  node.ingest: true

  path.repo: ["/home/elasticsearch_backups"]
  xpack.security.enabled: false

  discovery.zen.minimum_master_nodes: 2
  discovery.zen.ping.unicast.hosts: [ "172.16.103.68", "172.16.103.69", "172.16.103.112" ]
#+end_example

* Tools

- [[https://github.com/binwiederhier/elastictl][binwiederhier/elastictl: Simple tool to import/export Elasticsearch indices into a file, and/or reshard an index]]
- [[https://github.com/codenoid/elastis][codenoid/elastis: Tool for Export / Dump / Import / Copy Elastic/Open Search indexes data]]
- [[https://github.com/Cyb3rWard0g/HELK][Cyb3rWard0g/HELK: The Hunting ELK]]
- [[https://github.com/elastic/cloud-on-k8s][elastic/cloud-on-k8s: Elastic Cloud on Kubernetes]]
- [[https://github.com/flant/elasticsearch-extractor][flant/elasticsearch-extractor: Simple web UI to extract any index from Elasticsearch snapshot into repository.]]
- [[https://github.com/LGUG2Z/elasdx][LGUG2Z/elasdx: An ElasticSearch index template updating, reindexing and cleanup tool]]
- [[https://github.com/medcl/esm][medcl/esm: An Elasticsearch Migration Tool.]]
- [[https://github.com/Netflix/Raigad][Netflix/Raigad: Co-Process for backup/recovery, Auto Deployments and Centralized Configuration management for ElasticSearch]]
- [[https://github.com/StationA/esx][StationA/esx: CLI for streaming I/O with Elasticsearch]]

* Alternatives

- [[https://github.com/valeriansaliou/sonic][valeriansaliou/sonic: ü¶î Fast, lightweight & schema-less search backend. An alternative to Elasticsearch that runs on a few MBs of RAM.]]
- [[https://github.com/prabhatsharma/zinc][prabhatsharma/zinc: Zinc Search engine. A lightweight alternative to elasticsearch that requires minimal resources, written in Go.]]

* Kibana
- [[https://habr.com/ru/company/citymobil/blog/521802/][–°–æ–∑–¥–∞–Ω–∏–µ Dashboard –≤ Kibana –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ª–æ–≥–æ–≤ / –ë–ª–æ–≥ –∫–æ–º–ø–∞–Ω–∏–∏ –°–∏—Ç–∏–º–æ–±–∏–ª / –•–∞–±—Ä]]

* Misc
- [[https://www.google.com/search?q=elastic+list+replicate+specific+shards&hl=en][elastic list replicate specific shards - Google Search]]
- [[https://logz.io/blog/elasticsearch-cheat-sheet/][A Useful Elasticsearch Cheat Sheet in Times of Trouble | Logz.io]]
- [[https://stackoverflow.com/questions/15694724/shards-and-replicas-in-elasticsearch][full text search - Shards and replicas in Elasticsearch - Stack Overflow]]
- [[https://opster.com/blogs/elasticsearch-shards-and-replicas-getting-started-guide/][Elasticsearch Shards and Replicas getting started guide - Opster]]
- [[https://linuxhint.com/elasticsearch-shard-list/][Elasticsearch Shard List]]
- [[https://www.elastic.co/guide/en/elasticsearch/reference/6.6/cat-shards.html][cat shards | Elasticsearch Guide [6.6] | Elastic]]
- [[https://github.com/dadoonet/fscrawler][dadoonet/fscrawler: Elasticsearch File System Crawler (FS Crawler)]]

* Libraries
- [[https://github.com/bitemyapp/bloodhound][bitemyapp/bloodhound: Haskell Elasticsearch client and query DSL]]
