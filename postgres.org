#+title: PostgreSQL

- [[https://github.com/ankane/pgvector][ankane/pgvector: Open-source vector similarity search for Postgres]]
- [[https://github.com/akarki15/dbdot][akarki15/dbdot: Generate DOT description for postgres db schema]]
- [[https://github.com/dimitri/pgloader][dimitri/pgloader: Migrate to PostgreSQL in a single command!]]
- [[https://github.com/PostgREST/postgrest][PostgREST/postgrest: REST API for any Postgres database]]
- [[https://github.com/Paxa/postbird][Paxa/postbird: Open source PostgreSQL GUI client for macOS, Linux and Windows]]
- [[https://www.zombodb.com/][Integrate Postgresql and Elasticsearch | ZomboDB]]
- [[https://github.com/sanpii/explain][sanpii/explain: Transform postgresql explain to a graph]]
- [[https://github.com/wal-g/wal-g][wal-g/wal-g: Archival and Restoration for Postgres]]
- [[https://github.com/supabase/realtime][supabase/realtime: Listen to your to PostgreSQL database in realtime via websockets. Built with Elixir.]]
- [[https://github.com/pllua/pllua][pllua/pllua: Re-implementation of pllua, Lua embedded for postgresql]]

* [[https://postgrespro.com/list/thread-id/1525878][Thread: DB fails to start: "Could not read from file "pg_clog/0003" at offset 212992: No error. : Postgres Professional]]

Please perform below steps:

1. Backup the current pg_clog/0003 file in different directory
2. Create a file  by assumption of  make the uncommitted record as they haven't been committed. command as follows: 

dd if=/dev/zero of=<data directory location>/pg_clog/0003   bs=256K count=1

This is just a 256k zero-byte file. Here's one I made earlier:
http://www.postnewspapers.com.au/~craig/0003.zip

* out of memory query
Вот пример:
SELECT x, COUNT(x), array_agg(x)
  FROM (
       SELECT ((i << 20) | (j << 10) | k)::text::xid AS x
         FROM generate_series(0,1023) AS i,
              generate_series(0,1023) AS j,
              generate_series(0,1023) AS k
       ) s
 GROUP BY x;
И пояснение (всё © RhodiumToad):
Hashaggregate currently has no way to spill to disk. Hashagg won't be planned if the estimated hashtable size exceeds work_mem,
but at runtime, it'll blow past work_mem and use as much memory as it needs.
xid is a useful built-in example of a non-sortable type for sortable types, the query will usually use a sort and therefore be subject to
work_mem limits. But xid can only be grouped by hashing, so it forces a hashagg plan regardless of work_mem. So the query will try and create a hashtable with a billion entries each of which includes an array build state.

