:PROPERTIES:
:ID:       d5f3cdb2-b4c2-46fa-9763-50d0783d2013
:END:
#+title: PostgreSQL

- [[https://prudnitskiy.pro/2018/08/22/repmgr/][Repmgr: управление репликацией postgresql]]
- [[https://github.com/ankane/pgvector][ankane/pgvector: Open-source vector similarity search for Postgres]]
- [[https://github.com/akarki15/dbdot][akarki15/dbdot: Generate DOT description for postgres db schema]]
- [[https://github.com/dimitri/pgloader][dimitri/pgloader: Migrate to PostgreSQL in a single command!]]
- [[https://github.com/PostgREST/postgrest][PostgREST/postgrest: REST API for any Postgres database]]
- [[https://github.com/Paxa/postbird][Paxa/postbird: Open source PostgreSQL GUI client for macOS, Linux and Windows]]
- [[https://www.zombodb.com/][Integrate Postgresql and Elasticsearch | ZomboDB]]
- [[https://github.com/sanpii/explain][sanpii/explain: Transform postgresql explain to a graph]]
- [[https://github.com/wal-g/wal-g][wal-g/wal-g: Archival and Restoration for Postgres]]
- [[https://github.com/supabase/realtime][supabase/realtime: Listen to your to PostgreSQL database in realtime via websockets. Built with Elixir.]]
- [[https://github.com/pllua/pllua][pllua/pllua: Re-implementation of pllua, Lua embedded for postgresql]]
- [[https://gitlab.com/postgres-ai/postgres-checkup][Postgres.ai / postgres-checkup – automated PostgreSQL health checks · GitLab]]
- [[https://www.postgres-xl.org/][Postgres-XL | Open Source Scalable SQL Database Cluster]]
- [[https://habr.com/ru/post/584660/][Храним данные в JSONB, как это влияет на скорость запросов? / Хабр]]
- [[https://github.com/levkk/pgcat][levkk/pgcat: Meow. PgBouncer rewritten in Rust, with sharding, load balancing and failover support.]]
- [[https://github.com/benbjohnson/postlite][benbjohnson/postlite: Postgres wire compatible SQLite proxy.]]
- [[https://github.com/orioledb/orioledb][orioledb/orioledb: OrioleDB – building a modern cloud-native storage engine (... and solving some PostgreSQL wicked problems)]]
- [[https://github.com/aarroyoc/postgresql-prolog][aarroyoc/postgresql-prolog: A Prolog library to connect to PostgreSQL databases]]
- [[https://retool.com/blog/how-we-upgraded-postgresql-database/][How we upgraded our 4 TB main application Postgres database]]
- [[https://github.com/citusdata/citus][citusdata/citus: Distributed PostgreSQL as an extension]]
- [[https://github.com/shayonj/pg-osc][shayonj/pg-osc: Easy CLI tool for making zero downtime schema changes and backfills in PostgreSQL]]
- [[https://github.com/ankane/pgslice][ankane/pgslice: Postgres partitioning as easy as pie]]
- [[https://github.com/ankane/pgsync][ankane/pgsync: Sync data from one Postgres database to another]]
- [[https://github.com/hasura/pgdeltastream][hasura/pgdeltastream: Streaming Postgres logical replication changes atleast-once over websockets]]

* [[https://postgrespro.com/list/thread-id/1525878][Thread: DB fails to start: "Could not read from file "pg_clog/0003" at offset 212992: No error. : Postgres Professional]]

Please perform below steps:

1. Backup the current pg_clog/0003 file in different directory
2. Create a file  by assumption of  make the uncommitted record as they haven't been committed. command as follows: 

dd if=/dev/zero of=<data directory location>/pg_clog/0003   bs=256K count=1

This is just a 256k zero-byte file. Here's one I made earlier:
http://www.postnewspapers.com.au/~craig/0003.zip

* out of memory query
Вот пример:
SELECT x, COUNT(x), array_agg(x)
  FROM (
       SELECT ((i << 20) | (j << 10) | k)::text::xid AS x
         FROM generate_series(0,1023) AS i,
              generate_series(0,1023) AS j,
              generate_series(0,1023) AS k
       ) s
 GROUP BY x;
И пояснение (всё © RhodiumToad):
Hashaggregate currently has no way to spill to disk. Hashagg won't be planned if the estimated hashtable size exceeds work_mem,
but at runtime, it'll blow past work_mem and use as much memory as it needs.
xid is a useful built-in example of a non-sortable type for sortable types, the query will usually use a sort and therefore be subject to
work_mem limits. But xid can only be grouped by hashing, so it forces a hashagg plan regardless of work_mem. So the query will try and create a hashtable with a billion entries each of which includes an array build state.

