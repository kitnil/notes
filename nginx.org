:PROPERTIES:
:ID:       1c53120d-ef8d-4367-8e23-4cc5c3f387d5
:END:
#+title: NGINX

- [[https://github.com/alexazhou/VeryNginx][alexazhou/VeryNginx: A very powerful and friendly nginx base on lua-nginx-module( openresty ) which provide WAF, Control Panel, and Dashboards.]]
- [[https://github.com/blind-oracle/nginx-prometheus][blind-oracle/nginx-prometheus: Turn Nginx logs into Prometheus metrics]]
- [[https://github.com/bunkerity/bunkerized-nginx][bunkerity/bunkerized-nginx: Make your web services secure by default]]
- [[https://github.com/C0nw0nk/Nginx-Lua-Anti-DDoS][C0nw0nk/Nginx-Lua-Anti-DDoS: A Anti-DDoS script to protect Nginx web servers using Lua with a HTML Javascript based authentication puzzle inspired by Cloudflare I am under attack mode an Anti-DDoS authentication page protect yourself from every attack type All Layer 7 Attacks Mitigating Historic Attacks DoS DoS Implications DDoS All Brute Force Attacks Zero day exploits Social Engineering Rainbow Tables Password Cracking Tools Password Lists Dictionary Attacks Time Delay Any Hosting Provider Any CMS or Custom Website Unlimited Attempt Frequency Search Attacks HTTP Basic Authentication HTTP Digest Authentication HTML Form Based Authentication Mask Attacks Rule-Based Search Attacks Combinator Attacks Botnet Attacks Unauthorized IPs IP Whitelisting Bruter THC Hydra John the Ripper Brutus Ophcrack unauthorized logins Injection Broken Authentication and Session Management Sensitive Data Exposure XML External Entities (XXE) Broken Access Control Security Misconfiguration Cross-Site Scripting (XSS) Insecure Deserialization Using Components with Known Vulnerabilities Insufficient Logging & Monitoring Drupal WordPress Joomla Flash Magento PHP Plone WHMCS Atlassian Products malicious traffic Adult video script avs KVS Kernel Video Sharing Clip Bucket Tube sites Content Management Systems Social networks scripts backends proxy proxies PHP Python Porn sites xxx adult gaming networks servers sites forums vbulletin phpbb mybb smf simple machines forum xenforo web hosting video streaming buffering ldap upstream downstream download upload rtmp vod video over dl hls dash hds mss livestream drm mp4 mp3 swf css js html php python sex m3u zip rar archive compressed mitigation code source sourcecode chan 4chan 4chan.org 8chan.net 8ch 8ch.net infinite chan 8kun 8kun.net anonymous anon tor services .onion torproject.org nginx.org nginx.com openresty.org darknet dark net deepweb deep web darkweb dark web mirror vpn reddit reddit.com adobe flash hackthissite.org dreamhack hack hacked hacking hacker hackers hackerz hackz hacks code coding script scripting scripter source leaks leaked leaking cve vulnerability great firewall china america japan russia .gov government http1 http2 http3 quic q3 litespeedtech litespeed apache torrents torrent torrenting webtorrent bittorrent bitorrent bit-torrent cyberlocker cyberlockers cyber locker cyberbunker warez keygen key generator free irc internet relay chat peer-to-peer p2p cryptocurrency crypto bitcoin miner browser xmr monero coinhive coin hive coin-hive litecoin ethereum cpu cycles popads pop-ads advert advertisement networks banner ads protect ovh blazingfast.io amazon steampowered valve store.steampowered.com steamcommunity thepiratebay lulzsec antisec xhamster pornhub porn.com pornhub.com xhamster.com xvideos xvdideos.com xnxx xnxx.com popads popcash cpm ppc]]
- [[https://github.com/Canop/rhit][Canop/rhit: A nginx log explorer]]
- [[https://github.com/DarrenTsung/nginx-hash-test][DarrenTsung/nginx-hash-test: Playing around and verifying NGINX hash functionality.]]
- [[https://github.com/detectify/vulnerable-nginx][detectify/vulnerable-nginx: An intentionally vulnerable NGINX setup]]
- [[https://github.com/digitalocean/nginxconfig.io][digitalocean/nginxconfig.io: ‚öôÔ∏è NGi–òX config generator on steroids üíâ]]
- [[https://froxlor.org/][Froxlor Server Management Panel]]
- [[https://github.com/alexchamberlain/ngx-mongodb][GitHub - alexchamberlain/ngx-mongodb: MongoDB Module for Nginx]]
- [[https://github.com/hnlq715/nginx-vts-exporter][hnlq715/nginx-vts-exporter: Simple server that scrapes Nginx vts stats and exports them via HTTP for Prometheus consumption]]
- [[https://ubiq.co/tech-blog/disable-nginx-cache/][How To Disable NGINX Cache - Ubiq BI]]
- [[https://github.com/jkroepke/access-log-exporter][jkroepke/access-log-exporter: A Prometheus exporter that receives access logs from nginx through the syslog protocol and converts them into metrics.]]
- [[https://openresty-reference.readthedocs.io/en/latest/Lua_Nginx_API/][Lua Ngx API - OpenResty Reference]]
- [[https://github.com/lucasdillmann/nginx-ignition][lucasdillmann/nginx-ignition: An user interface for the nginx web server, aimed at developers and enthusiasts that don't want to manage configuration files manually]]
- [[https://github.com/NBPub/BeatLog][NBPub/BeatLog: BeatLog parses NGINX reverse proxy and fail2ban logs into readable tables and reports. Use BeatLog to assess server traffic and tailor fail2ban filters.]]
- [[https://devdocs.io/nginx/][nginx documentation ‚Äî DevDocs]]
- [[https://github.com/nginxinc/crossplane][nginxinc/crossplane: Quick and reliable way to convert NGINX configurations into JSON and back.]]
- [[https://github.com/nginxinc/nginx-prometheus-exporter][nginxinc/nginx-prometheus-exporter: NGINX Prometheus Exporter for NGINX and NGINX Plus]]
- [[http://freenginx.org/][nginx news: The freenginx.org project. The goal of the project is to keep nginx development free from arbitrary corporate actions.]]
- [[https://github.com/openresty/nginx-tutorials][openresty/nginx-tutorials: Nginx Tutorials]]
- [[https://github.com/openresty/programming-openresty][openresty/programming-openresty: Programming OpenResty Book]]
- [[https://github.com/pretzelhands/jinx][pretzelhands/jinx: ‚ú®jinx - a magical nginx wrapper]]
- [[https://github.com/Safe3/openresty-manager][Safe3/openresty-manager: Modern, secure, and elegant server control panel, alternative to OpenResty Edge and Nginx Proxy Manager.]]
- [[https://github.com/schenkd/nginx-ui][schenkd/nginx-ui: Nginx UI allows you to access and modify the nginx configurations files without cli.]]
- [[https://github.com/sous-chefs/nginx][sous-chefs/nginx: Development repository for the nginx cookbook]]
- [[https://github.com/SpiderLabs/ModSecurity][SpiderLabs/ModSecurity: ModSecurity is an open source, cross platform web application firewall (WAF) engine for Apache, IIS and Nginx that is developed by Trustwave's SpiderLabs. It has a robust event-based programming language which provides protection from a range of attacks against web applications and allows for HTTP traffic monitoring, logging and real-time analysis. With over 10,000 deployments world-wide, ModSecurity is the most widely deployed WAF in existence.]]
- [[https://github.com/vozlt/nginx-module-vts][vozlt/nginx-module-vts: Nginx virtual host traffic status module]]
- [[https://github.com/x-way/ngx_http_remote_passwd][x-way/ngx_http_remote_passwd: Nginx module which makes the Basic Auth password available as variable $remote_passwd]]
- [[https://github.com/yandex/gixy][yandex/gixy: Nginx configuration static analyzer]]
- [[https://github.com/yandex/gixy][yandex/gixy: Nginx configuration static analyzer]]
- [[https://habr.com/ru/post/548110/][–î–æ–º–∞—à–Ω–∏–π DPI, –∏–ª–∏ –∫–∞–∫ –±–æ—Ä–æ—Ç—å—Å—è —Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º –µ–≥–æ –∂–µ –º–µ—Ç–æ–¥–∞–º–∏ / –•–∞–±—Ä]]
- [[https://habr.com/ru/company/nixys/blog/665126/][–ö–∞–∫ –∑–∞—â–∏—Ç–∏—Ç—å—Å—è –æ—Ç dos/ddos, –∏–ª–∏ –ö–∞–∫ —è –Ω–∞—á–∞–ª –≤–Ω–æ–≤—å –≤—ã—Å—ã–ø–∞—Ç—å—Å—è –ø–æ –Ω–æ—á–∞–º / –•–∞–±—Ä]]
- [[https://habr.com/ru/company/nixys/blog/661233/][–ö–∞–∫ –∏–∑–±–µ–∂–∞—Ç—å 10 —á–∞—Å—Ç—ã—Ö –æ—à–∏–±–æ–∫ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ NGINX / –•–∞–±—Ä]]
- [[https://habr.com/ru/post/583562/][–ü–µ—Å–æ—á–Ω–∏—Ü–∞ –¥–ª—è Nginx / –•–∞–±—Ä]]
  - [[https://nginx-playground.wizardzines.com/][nginx playground]]
- [[https://serveradmin.ru/ustanovka-i-nastrojka-nginx/][–ü–æ–¥—Ä–æ–±–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ Nginx —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ ‚Äî Server Admin]]
- [[https://serveradmin.ru/nginx-redirect/][–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π redirect 301 –¥–ª—è SEO –≤ Nginx ‚Äî Server Admin]]
- [[https://habr.com/ru/post/652479/][–†–µ—Ü–µ–ø—Ç—ã Nginx: –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü –æ–± –æ—à–∏–±–∫–∞—Ö / –•–∞–±—Ä]]

* Modules

- [[https://nginx-clojure.github.io/][Home@Nginx-Clojure]]
- [[https://github.com/ajax16384/ngx_http_untar_module][ajax16384/ngx_http_untar_module: Nginx HTTP Untar Module]]
- [[https://github.com/Taymindis/nginx-link-function][Taymindis/nginx-link-function: It is a NGINX module that provides dynamic linking to your application in server context and call the function of your application in location directive]]

* Configuration
** Minimal
  Save as /tmp/nginx/nginx.conf
  #+begin_src nginx
    pid /tmp/nginx/pid;
    error_log /dev/stdout;
    events { }
    http {
        client_body_temp_path /tmp/nginx/client_body_temp;
        proxy_temp_path /tmp/nginx/proxy_temp;
        fastcgi_temp_path /tmp/nginx/fastcgi_temp;
        uwsgi_temp_path /tmp/nginx/uwsgi_temp;
        scgi_temp_path /tmp/nginx/scgi_temp;
        access_log /dev/stdout;
        include /gnu/store/hi9vi5061sjkysyrx9qw6dc03l6iwjbj-nginx-1.19.3/share/nginx/conf/mime.types;
        server {
            listen 8080;
            server_name localhost;
            root /tmp/nginx;
            index index.html ;
            server_tokens off;
        }
    }
  #+end_src
  and run as =/gnu/store/...-nginx-1.19.3/sbin/nginx -c /tmp/nginx.conf -p /tmp/nginx -g 'daemon off;'=
** Lua
   #+begin_src nginx
     location /hello {
         default_type 'text/plain';
         content_by_lua '
         ngx.say("Hello world!")
         ';
     }
   #+end_src

- [[https://blog.openresty.com/en/lua-cpu-flame-graph/?src=org][Introduction to Lua-Land CPU Flame Graphs - OpenResty Official Blog]]

* Forks
- [[https://github.com/webserver-llc/angie][webserver-llc/angie: Angie - drop-in replacement for Nginx]]

* Learning
- [[https://github.com/Tinywan/lua-nginx-redis][Tinywan/lua-nginx-redis: Redis„ÄÅLua„ÄÅNginx„ÄÅOpenResty Á¨îËÆ∞ÂíåËµÑÊñô]]
- [[https://www.nginx.com/blog/improving-nginx-performance-with-kernel-tls/][Improving NGINX Performance with Kernel TLS and SSL_sendfile( ) - NGINX]]
- [[https://github.com/tldr-devops/nginx-common-configuration][tldr-devops/nginx-common-configuration: Nginx common useful configuration]]

** [[https://alex.dzyoba.com/blog/nginx-mirror/][NGINX mirroring tips and tricks]]

**** January 14, 2019

Lately, I‚Äôve been playing with nginx and its relatively new [[http://nginx.org/en/docs/http/ngx_http_mirror_module.html][*mirror*  module]] which appeared in 1.13.4. The mirror module allows you to copy requests to another backend while ignoring answers from it. The example use cases for this are:

- Pre-production testing by observing how your new system handle real production traffic
- Logging of requests for security analysis. This is [[https://docs.wallarm.com/en/admin-en/mirror-traffic-en.htm][what Wallarm tool do]]
- Copying requests for data science research
- etc.

I‚Äôve used it for pre-production testing of the new rewritten system to see how well (if at all ;-) it can handle the production workload. There are some non-obvious problems and tips that I didn‚Äôt find when I started this journey and now I wanted to share it.

*** Basic setup

Let‚Äôs begin with a simple setup. Say, we have some backend that handles production workload and we put a proxy in front of it:

[[https://alex.dzyoba.com/img/nginx-mirror-basic-setup.png]]

Here is the nginx config:

#+begin_src markdown
upstream backend {
    server backend.local:10000;
}

server {
    server_name proxy.local;
    listen 8000;

    location / {
        proxy_pass http://backend;
    }
}

#+end_src

There are 2 parts ‚Äì backend and proxy. The proxy (nginx) is listening on port 8000 and just passing requests to the backend on port 10000. Nothing fancy, but let‚Äôs do a quick load test to see how it performs. I‚Äôm using [[https://github.com/rakyll/hey][=hey=  tool]] because it‚Äôs simple and allows generating constant load instead of bombarding as hard as possible like many other tools do (wrk, apache benchmark, siege).

#+begin_src markdown
$ hey -z 10s -q 1000 -n 100000 -c 1 -t 1 http://proxy.local:8000

Summary:
  Total:	10.0016 secs
  Slowest:	0.0225 secs
  Fastest:	0.0003 secs
  Average:	0.0005 secs
  Requests/sec:	995.8393

  Total data:	6095520 bytes
  Size/request:	612 bytes

Response time histogram:
  0.000 [1]	|
  0.003 [9954]	|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†
  0.005 [4]	|
  0.007 [0]	|
  0.009 [0]	|
  0.011 [0]	|
  0.014 [0]	|
  0.016 [0]	|
  0.018 [0]	|
  0.020 [0]	|
  0.022 [1]	|


Latency distribution:
  10% in 0.0003 secs
  25% in 0.0004 secs
  50% in 0.0005 secs
  75% in 0.0006 secs
  90% in 0.0007 secs
  95% in 0.0007 secs
  99% in 0.0009 secs

Details (average, fastest, slowest):
  DNS+dialup:	0.0000 secs, 0.0003 secs, 0.0225 secs
  DNS-lookup:	0.0000 secs, 0.0000 secs, 0.0008 secs
  req write:	0.0000 secs, 0.0000 secs, 0.0003 secs
  resp wait:	0.0004 secs, 0.0002 secs, 0.0198 secs
  resp read:	0.0001 secs, 0.0000 secs, 0.0012 secs

Status code distribution:
  [200]	9960 responses

#+end_src

Good, most of the requests are handled in less than a millisecond and there are no errors ‚Äì that‚Äôs our baseline.

*** Basic mirroring

Now, let‚Äôs put another test backend and mirror traffic to it

[[https://alex.dzyoba.com/img/nginx-mirror-mirror-setup.png]]

The basic mirroring is configured like this:

#+begin_src markdown
upstream backend {
    server backend.local:10000;
}

upstream test_backend {
    server test.local:20000;
}

server {
    server_name proxy.local;
    listen 8000;

    location / {
        mirror /mirror;
        proxy_pass http://backend;
    }

    location = /mirror {
        internal;
        proxy_pass http://test_backend$request_uri;
    }

}

#+end_src

We add =mirror=  directive to mirror requests to the internal location and define that internal location. In that internal location we can do whatever nginx allows us to do but for now we just simply proxy pass all requests.

Let‚Äôs load test it again to check how mirroring affects the performance:

#+begin_src markdown
$ hey -z 10s -q 1000 -n 100000 -c 1 -t 1 http://proxy.local:8000

Summary:
  Total:	10.0010 secs
  Slowest:	0.0042 secs
  Fastest:	0.0003 secs
  Average:	0.0005 secs
  Requests/sec:	997.3967

  Total data:	6104700 bytes
  Size/request:	612 bytes

Response time histogram:
  0.000 [1]	|
  0.001 [9132]	|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†
  0.001 [792]	|‚ñ†‚ñ†‚ñ†
  0.001 [43]	|
  0.002 [3]	|
  0.002 [0]	|
  0.003 [2]	|
  0.003 [0]	|
  0.003 [0]	|
  0.004 [1]	|
  0.004 [1]	|


Latency distribution:
  10% in 0.0003 secs
  25% in 0.0004 secs
  50% in 0.0005 secs
  75% in 0.0006 secs
  90% in 0.0007 secs
  95% in 0.0008 secs
  99% in 0.0010 secs

Details (average, fastest, slowest):
  DNS+dialup:	0.0000 secs, 0.0003 secs, 0.0042 secs
  DNS-lookup:	0.0000 secs, 0.0000 secs, 0.0009 secs
  req write:	0.0000 secs, 0.0000 secs, 0.0002 secs
  resp wait:	0.0004 secs, 0.0002 secs, 0.0041 secs
  resp read:	0.0001 secs, 0.0000 secs, 0.0021 secs

Status code distribution:
  [200]	9975 responses

#+end_src

It‚Äôs pretty much the same ‚Äì millisecond latency and no errors. And that‚Äôs good because it proves that mirroring itself doesn‚Äôt affect original requests.

*** Mirroring to buggy backend

That‚Äôs all nice and dandy but what if mirror backend has some bugs and sometimes replies with errors? What would happen to the original requests?

To test this I‚Äôve made a [[https://github.com/dzeban/mirror-backend][trivial Go service]] that can inject errors randomly. Let‚Äôs launch it

#+begin_src markdown
$ mirror-backend -errors
2019/01/13 14:43:12 Listening on port 20000, delay is 0, error injecting is true

#+end_src

and see what load testing will show:

#+begin_src markdown
$ hey -z 10s -q 1000 -n 100000 -c 1 -t 1 http://proxy.local:8000

Summary:
  Total:	10.0008 secs
  Slowest:	0.0027 secs
  Fastest:	0.0003 secs
  Average:	0.0005 secs
  Requests/sec:	998.7205

  Total data:	6112656 bytes
  Size/request:	612 bytes

Response time histogram:
  0.000 [1]	|
  0.001 [7388]	|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†
  0.001 [2232]	|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†
  0.001 [324]	|‚ñ†‚ñ†
  0.001 [27]	|
  0.002 [6]	|
  0.002 [2]	|
  0.002 [3]	|
  0.002 [2]	|
  0.002 [0]	|
  0.003 [3]	|


Latency distribution:
  10% in 0.0003 secs
  25% in 0.0003 secs
  50% in 0.0004 secs
  75% in 0.0006 secs
  90% in 0.0007 secs
  95% in 0.0008 secs
  99% in 0.0009 secs

Details (average, fastest, slowest):
  DNS+dialup:	0.0000 secs, 0.0003 secs, 0.0027 secs
  DNS-lookup:	0.0000 secs, 0.0000 secs, 0.0008 secs
  req write:	0.0000 secs, 0.0000 secs, 0.0001 secs
  resp wait:	0.0004 secs, 0.0002 secs, 0.0026 secs
  resp read:	0.0001 secs, 0.0000 secs, 0.0006 secs

Status code distribution:
  [200]	9988 responses

#+end_src

Nothing changed at all! And that‚Äôs great because errors in the mirror backend don‚Äôt affect the main backend. nginx mirror module ignores responses to the mirror subrequests so this behavior is nice and intended.

*** Mirroring to a slow backend

But what if our mirror backend is not returning errors but just plain slow? How original requests will work? Let‚Äôs find out!

My mirror backend has an option to delay every request by configured amount of seconds. Here I‚Äôm launching it with a 1 second delay:

#+begin_src markdown
$ mirror-backend -delay 1
2019/01/13 14:50:39 Listening on port 20000, delay is 1, error injecting is false

#+end_src

So let‚Äôs see what load test show:

#+begin_src markdown
$ hey -z 10s -q 1000 -n 100000 -c 1 -t 1 http://proxy.local:8000

Summary:
  Total:	10.0290 secs
  Slowest:	0.0023 secs
  Fastest:	0.0018 secs
  Average:	0.0021 secs
  Requests/sec:	1.9942

  Total data:	6120 bytes
  Size/request:	612 bytes

Response time histogram:
  0.002 [1]	|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†
  0.002 [0]	|
  0.002 [1]	|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†
  0.002 [0]	|
  0.002 [0]	|
  0.002 [0]	|
  0.002 [1]	|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†
  0.002 [1]	|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†
  0.002 [0]	|
  0.002 [4]	|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†
  0.002 [2]	|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†


Latency distribution:
  10% in 0.0018 secs
  25% in 0.0021 secs
  50% in 0.0022 secs
  75% in 0.0023 secs
  90% in 0.0023 secs
  0% in 0.0000 secs
  0% in 0.0000 secs

Details (average, fastest, slowest):
  DNS+dialup:	0.0007 secs, 0.0018 secs, 0.0023 secs
  DNS-lookup:	0.0003 secs, 0.0002 secs, 0.0006 secs
  req write:	0.0001 secs, 0.0001 secs, 0.0002 secs
  resp wait:	0.0011 secs, 0.0007 secs, 0.0013 secs
  resp read:	0.0002 secs, 0.0001 secs, 0.0002 secs

Status code distribution:
  [200]	10 responses

Error distribution:
  [10]	Get http://proxy.local:8000: net/http: request canceled (Client.Timeout exceeded while awaiting headers)

#+end_src

What? 1.9 rps? Where is my 1000 rps? We‚Äôve got errors? What‚Äôs happening?

Let me explain how mirroring in nginx works.

**** How mirroring in nginx works

When the request is coming to nginx and if mirroring is enabled, nginx will create a mirror subrequest and do what mirror location specifies ‚Äì in our case, it will send it to the mirror backend.

But the thing is that subrequest is linked to the original request, so /as far as I understand/  unless that mirror subrequest is not finished the original requests will throttle.

That‚Äôs why we get ~2 rps in the previous test ‚Äì =hey=  sent 10 requests, got responses, sent next 10 requests but they stalled because previous mirror subrequests were delayed and then timeout kicked in and errored the last 10 requests.

If we increase the timeout in hey to, say, 10 seconds we will receive no errors and 1 rps:

#+begin_src markdown
$ hey -z 10s -q 1000 -n 100000 -c 1 -t 10 http://proxy.local:8000

Summary:
  Total:	10.0197 secs
  Slowest:	1.0018 secs
  Fastest:	0.0020 secs
  Average:	0.9105 secs
  Requests/sec:	1.0978

  Total data:	6732 bytes
  Size/request:	612 bytes

Response time histogram:
  0.002 [1]	|‚ñ†‚ñ†‚ñ†‚ñ†
  0.102 [0]	|
  0.202 [0]	|
  0.302 [0]	|
  0.402 [0]	|
  0.502 [0]	|
  0.602 [0]	|
  0.702 [0]	|
  0.802 [0]	|
  0.902 [0]	|
  1.002 [10]	|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†


Latency distribution:
  10% in 1.0011 secs
  25% in 1.0012 secs
  50% in 1.0016 secs
  75% in 1.0016 secs
  90% in 1.0018 secs
  0% in 0.0000 secs
  0% in 0.0000 secs

Details (average, fastest, slowest):
  DNS+dialup:	0.0001 secs, 0.0020 secs, 1.0018 secs
  DNS-lookup:	0.0000 secs, 0.0000 secs, 0.0005 secs
  req write:	0.0001 secs, 0.0000 secs, 0.0002 secs
  resp wait:	0.9101 secs, 0.0008 secs, 1.0015 secs
  resp read:	0.0002 secs, 0.0001 secs, 0.0003 secs

Status code distribution:
  [200]	11 responses

#+end_src

So the point here is that *if mirrored subrequests are slow then the original requests will be throttled* . I don‚Äôt know how to fix this but I know the workaround ‚Äì mirror only some part of the traffic. Let me show you how.

*** Mirroring part of the traffic

If you‚Äôre not sure that mirror backend can handle the original load you can mirror only some part of the traffic ‚Äì for example, 10%.

=mirror=  directive is not configurable and replicates all requests to the mirror location so it‚Äôs not obvious how to do this. The key point in achieving this is the internal mirror location. If you remember I‚Äôve said that you can anything to mirrored requests in its location. So here is how I did this:

#+begin_src markdown
 1	upstream backend {
 2	    server backend.local:10000;
 3	}
 4	
 5	upstream test_backend {
 6	    server test.local:20000;
 7	}
 8	
 9	split_clients $remote_addr $mirror_backend {
10	    50% test_backend;
11	    *   "";
12	}
13	
14	server {
15	    server_name proxy.local;
16	    listen 8000;
17	
18	    access_log /var/log/nginx/proxy.log;
19	    error_log /var/log/nginx/proxy.error.log info;
20	
21	    location / {
22	        mirror /mirror;
23	        proxy_pass http://backend;
24	    }
25	
26	    location = /mirror {
27	        internal;
28	        if ($mirror_backend = "") {
29	            return 400;
30	        }
31	
32	        proxy_pass http://$mirror_backend$request_uri;
33	    }
34	
35	}
36	

#+end_src

First of all, in mirror location we proxy pass to the upstream that is taken from variable =$mirror_backend= (line 32). This variable is set in =split_client= block (lines 9-12) based on client remote address. What =split_client=  does is it sets right variable value based on left variable distribution. In our case, we look at requests remote address ( =$remote_addr= variable) and for 50% of remote addresses we set =$mirror_backend= to the =test_backend=, for other requests it‚Äôs set to empty string. Finally, the partial part is performed in mirror location ‚Äì if =$mirror_backend= variable is empty we reject that mirror subrequest, otherwise we =proxy_pass=  it. Remember that failure in mirror subrequests doesn‚Äôt affect original requests so it‚Äôs safe to drop request with error status.

The beauty of this solution is that you can split traffic for mirroring based on any variable or combination. If you want to really differentiate your users then remote address may not be the best split key ‚Äì user may use many IPs or change them. In that case, you‚Äôre better off using some user-sticky key like API key. For mirroring 50% of traffic based on =apikey= query parameter we just change key in =split_client= :

#+begin_src markdown
split_clients $arg_apikey $mirror_backend {
    50% test_backend;
    *   "";
}

#+end_src

When we‚Äôll query apikeys from 1 to 20 only half of it (11) will be mirrored. Here is the curl:

#+begin_src markdown
$ for i in {1..20};do curl -i "proxy.local:8000/?apikey=${i}" ;done

#+end_src

and here is the log of mirror backend:

#+begin_src markdown
...
2019/01/13 22:34:34 addr=127.0.0.1:47224 host=test_backend uri="/?apikey=1"
2019/01/13 22:34:34 addr=127.0.0.1:47230 host=test_backend uri="/?apikey=2"
2019/01/13 22:34:34 addr=127.0.0.1:47240 host=test_backend uri="/?apikey=4"
2019/01/13 22:34:34 addr=127.0.0.1:47246 host=test_backend uri="/?apikey=5"
2019/01/13 22:34:34 addr=127.0.0.1:47252 host=test_backend uri="/?apikey=6"
2019/01/13 22:34:34 addr=127.0.0.1:47262 host=test_backend uri="/?apikey=8"
2019/01/13 22:34:34 addr=127.0.0.1:47272 host=test_backend uri="/?apikey=10"
2019/01/13 22:34:34 addr=127.0.0.1:47278 host=test_backend uri="/?apikey=11"
2019/01/13 22:34:34 addr=127.0.0.1:47288 host=test_backend uri="/?apikey=13"
2019/01/13 22:34:34 addr=127.0.0.1:47298 host=test_backend uri="/?apikey=15"
2019/01/13 22:34:34 addr=127.0.0.1:47308 host=test_backend uri="/?apikey=17"
...

#+end_src

And the most awesome thing is that partitioning in =split_client= is consistent ‚Äì requests with ~apikey=1~  will always be mirrored.

*** Conclusion

So this was my experience with nginx mirror module so far. I‚Äôve shown you how to simply mirror all of the traffic, how to mirror part of the traffic with the help of =split_client=  module. I‚Äôve also covered error handling and non-obvious problem when normal requests are throttled in case of slow mirror backend.
