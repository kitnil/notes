:PROPERTIES:
:ID:       169228af-10b2-4a7e-b9fa-0619733b14ed
:END:

* Alternatives
- [[https://github.com/aurae-runtime/aurae][aurae-runtime/aurae: Distributed systems runtime daemon written in Rust.]]

* Awesome
- [[https://kubernetes-csi.github.io/docs/drivers.html][Drivers - Kubernetes CSI Developer Documentation]]
- [[https://github.com/k8s-at-home/awesome-home-kubernetes][k8s-at-home/awesome-home-kubernetes: Awesome projects involving running Kubernetes at home]]
- [[https://github.com/Michaelpalacce/HomeLab][Michaelpalacce/HomeLab: My HomeLab environment]]
- [[https://github.com/ramitsurana/awesome-kubernetes][ramitsurana/awesome-kubernetes: A curated list for awesome kubernetes sources]]
- [[https://github.com/tomhuang12/awesome-k8s-resources][tomhuang12/awesome-k8s-resources: A curated list of awesome Kubernetes tools and resources.]]

* CDI
** EFS
#+begin_quote
EFS is dead slow and not performant at all. Stay away from EFS except for the
lightest of data storage needs.
#+end_quote

* cert-manager
- [[https://cert-manager.io/docs/troubleshooting/webhook/][The Definitive Debugging Guide for the cert-manager Webhook Pod - cert-manager Documentation]]

- [[https://github.com/cert-manager/cert-manager/issues/4831][Unable to apply Cert Manager v1.7.0 CRDs · Issue #4831 · cert-manager/cert-manager]]
- [[https://cert-manager.io/docs/release-notes/release-notes-1.7/#upgrading-with-server-side-apply][Release 1.7 - cert-manager Documentation]]
#+begin_src bash
  crds=("certificaterequests.cert-manager.io" "certificates.cert-manager.io" "challenges.acme.cert-manager.io" "clusterissuers.cert-manager.io" "issuers.cert-manager.io" "orders.acme.cert-manager.io")

  for crd in "${crds[@]}"; do
    manager_index="$(kubectl get crd "${crd}" --show-managed-fields --output json | jq -r '.metadata.managedFields | map(.manager == "cainjector") | index(true)')"
    kubectl patch crd "${crd}" --type=json -p="[{\"op\": \"remove\", \"path\": \"/metadata/managedFields/${manager_index}\"}]"
  done
#+end_src

* Cheat sheet

- [[https://github.com/eon01/kubectl-SheetCheat][eon01/kubectl-SheetCheat: The Definitive Kubectl Sheetcheat. ⭐ Give it a star if you like it. Work (always) in progress !]]

- [[https://gist.github.com/egeneralov/4309aec2994af7e025330509905e824e][chroot to host namespace]]
  #+begin_src bash
    #!/bin/sh -xe

    for NAME in $(kubectl get nodes -o name | awk -F \/ '{print $2}'); do
      cat << EOF | kubectl apply -f -
        apiVersion: v1
        kind: Pod
        metadata:
         name: kube-support-$NAME
         namespace: kube-system
        spec:
          terminationGracePeriodSeconds: 1
          containers:
            - name: debian
              image: debian:buster
              command: ["/bin/sleep"]
              args: ["infinity"]
              volumeMounts:
                - name: host
                  mountPath: /host
              securityContext:
                privileged: true
          volumes:
            - name: host
              hostPath:
                path: /
                type: Directory
          nodeSelector:
            kubernetes.io/hostname: $NAME
          tolerations:
            - operator: "Exists"
      EOF
    done
  #+end_src

32- [[https://learnk8s.io/a/a-visual-guide-on-troubleshooting-kubernetes-deployments/troubleshooting-kubernetes.en_en.v3.pdf][Visual guide on troubleshooting Kubernetes deployments]]

#+begin_example
  [root@vm30:~]# lsns | grep nginx
  4026532800 mnt         2  80051 root             nginx: master process nginx -g daemon off;
  4026532801 pid         2  80051 root             nginx: master process nginx -g daemon off;
  4026532802 cgroup      2  80051 root             nginx: master process nginx -g daemon off;
  4026532865 mnt        20  90081 101              /usr/bin/dumb-init -- /nginx-ingress-controller --publish-service=ingress-nginx/ingress-nginx-controller --election-id=ingress-controller-leader --controller-class=k8s.io/ingress-nginx --ingress-class=nginx --configmap=ingress-nginx/ingress-nginx-controller --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key
  4026532866 pid        20  90081 101              /usr/bin/dumb-init -- /nginx-ingress-controller --publish-service=ingress-nginx/ingress-nginx-controller --election-id=ingress-controller-leader --controller-class=k8s.io/ingress-nginx --ingress-class=nginx --configmap=ingress-nginx/ingress-nginx-controller --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key
  4026532867 cgroup     20  90081 101              /usr/bin/dumb-init -- /nginx-ingress-controller --publish-service=ingress-nginx/ingress-nginx-controller --election-id=ingress-controller-leader --controller-class=k8s.io/ingress-nginx --ingress-class=nginx --configmap=ingress-nginx/ingress-nginx-controller --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key

  [root@vm30:~]# nsenter -t 80051 -n curl localhost 
  <!DOCTYPE html>
  <html>
  ...
#+end_example

- clean state
  : rm -rf /var/lib/{kubernetes,cfssl,cni,containerd,etcd,kubelet}

- print everything
  : kubectl get all -A
  : kubectl get all --all-namespaces
  : kubectl get all,cm,secret,ing -A
  : kubectl api-resources --verbs=list
  : kubectl get events -A

- API
  : kubectl api-resources
  : kubectl api-versions

- cdi-uploadproxy
: kubectl get service cdi-uploadproxy -n cdi

- show pods labels
  : kubectl get pods --show-labels

- get pod's nodes matching specific label
  : kubectl -l app.kubernetes.io/name=opensearch --output=custom-columns='NAME:spec.nodeName' --no-headers=true --namespace=opensearch get pods

- delete pods by labels
  : kubectl delete -n kube-system -l k8s-app=kube-dns pods

- delete failed pods
  : kubectl delete pods --field-selector status.phase=Failed --all-namespaces

- delete terminating ns
#+begin_example
  oleg@guixsd ~$ kubectl proxy
  Starting to serve on 127.0.0.1:8001

  NAMESPACE=cdi
  kubectl get namespace $NAMESPACE -o json |jq '.spec = {"finalizers":[]}' >temp.json
  curl -k -H "Content-Type: application/json" -X PUT --data-binary @temp.json 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE/finalize
#+end_example

- pvc
#+begin_example
  oleg@guixsd ~$ virtctl image-upload --pvc-name=guix-vm-pv --pvc-size=2Gi --image-path=/tmp/guix.qcow2 --uploadproxy-url=https://10.43.181.37 --insecure
  PVC default/guix-vm-pv not found
  PersistentVolumeClaim default/guix-vm-pv created
  Waiting for PVC guix-vm-pv upload pod to be ready...
  Pod now ready
  Uploading data to https://10.43.181.37
#+end_example

- get storage info
  : kubectl get --raw "/api/v1/nodes/kube7/proxy/stats/summary

- get specific node
  : kubectl get all --all-namespaces -o wide --field-selector spec.nodeName=kube7

- show node labels
  : kubectl get nodes --show-labels

- get context
  #+begin_example
    oleg@guixsd ~$ kubectl config get-contexts
    CURRENT   NAME           CLUSTER        AUTHINFO             NAMESPACE
    ,*         k8s-cluster0   k8s-cluster0   k8s-cluster0-admin   
  #+end_example

- change context
  #+begin_example
    kubectl config use-context k8s-cluster0
  #+end_example

- change namespace
  #+begin_example
    kubectl config set-context --current --namespace=<insert-namespace-name-here>
    # Validate it
    kubectl config view --minify | grep namespace:
  #+end_example

- run linter
  : docker run --network=host --rm -it -v $HOME/.kube:/root/.kube -e POPEYE_REPORT_DIR=/tmp/popeye -v /tmp/popeye:/tmp/popeye derailed/popeye --context k8s-cluster0-lb -n default --save --out html --output-file report.html

- mark master nodes as unschedulable
  : kubectl taint node kube3 unschedulable=true:NoSchedule

- list nodes NAME ARCH KERNEL KUBLET CPU RAM
  : kubectl get nodes -o custom-columns=NAME:.metadata.name,ARCH:.status.nodeInfo.architecture,KERNEL:.status.nodeInfo.kernelVersion,KUBLET:.status.nodeInfo.kubeletVersion,CPU:.status.capacity.cpu,RAM:.status.capacity.memory

- list nodes with taints
  : kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints --no-headers
  : kubectl get nodes -o=jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.taints}{"\n"}{end}'
  : kubectl get nodes -o go-template='{{range $item := .items}}{{with $nodename := $item.metadata.name}}{{range $taint := $item.spec.taints}}{{if and (eq $taint.key "node-role.kubernetes.io/master") (eq $taint.effect "NoSchedule")}}{{printf "%s\n" $nodename}}{{end}}{{end}}{{end}}{{end}}'
  : kubectl get nodes -o go-template='{{range $item := .items}}{{with $nodename := $item.metadata.name}}{{range $taint := $item.spec.taints}}{{if and (eq $taint.effect "NoSchedule")}}{{printf "%s\n" $nodename}}{{end}}{{end}}{{end}}{{end}}'

- delete taint
  : kubectl taint node kube8 unschedulable-

- drain node
  : kubectl drain kube6 --ignore-daemonsets --delete-emptydir-data

- reboot node
  : ansible kubeX.intr -m reboot -a 'search_paths=/run/current-system/sw/bin'

- undrain node
  : kubectl uncordon kube6

- get documentation
  : kubectl explain --api-version=networking.k8s.io/v1 ingress.spec.rules.http.paths.backend

- dump all information
  : kubectl cluster-info dump

- list all objects in a namespace
  : parallel --color -j 4 kubectl get -o wide --show-kind --ignore-not-found -n NAMESPACE ::: $(kubectl api-resources --verbs=list --namespaced -o name)

- list pending pods
  : kubectl get pods --field-selector status.phase=Pending --all-namespaces -o wide

- list pods by creation time
  : kubectl get pods -A -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,NODE:.spec.nodeName,HOSTIP:.status.hostIP,PHASE:.status.phase,START_TIME:.metadata.creationTimestamp --sort-by=.metadata.creationTimestamp

- run pod with networking utilities for debuging
  : kubectl run --rm -it netshoot --image=nicolaka/netshoot /bin/bash

- run pod on specific node with =kubectl run=
  : kubectl run mypod --image alpine --overrides='{"apiVersion": "v1", "spec": {"nodeSelector": { "kubernetes.io/hostname": "kube1" }}}' --command -- sleep 100000000000000
  : kubectl run --rm -it netshoot --image=nicolaka/netshoot --overrides='{"apiVersion": "v1", "spec": {"nodeSelector": { "kubernetes.io/hostname": "kube1" }}}' /bin/bash

- mariadb client
  : kubectl run --rm -it mysql-client --image=nixery.dev/shell/less/mycli -- /bin/bash

- run with specific multus network
  : kubectl run --rm -it netshoot --image=nicolaka/netshoot --overrides='{"apiVersion": "v1", "metadata": {"annotations": {"k8s.v1.cni.cncf.io/networks": "default/macvlan-br253"}}, "spec": {"nodeSelector": { "kubernetes.io/hostname": "kube6" }}}' /bin/bash

- iperf
  : kubectl run iperf --image nixery.dev/shell/tmux/iperf --overrides='{"apiVersion": "v1", "spec": {"nodeSelector": { "kubernetes.io/hostname": "kube6" }}}' --command -- sleep 100000000000000

- expose
  : kubectl expose --port=5201 --type=LoadBalancer pod iperf

- run nginx on specific node
  : kubectl run --overrides='{"apiVersion": "v1", "spec": {"nodeSelector": { "kubernetes.io/hostname": "kube6" }}}' --image nginx nginx

- run wrk
  : kubectl run --restart=Never -n example-hpa --rm -it wrk --image=nixery.dev/shell/wrk -- /bin/bash

- expose to specific address
  : kubectl expose --overrides='{"spec": {"loadBalancerIP": "78.108.82.13"}}' --port=80 --type=LoadBalancer pod nginx-76d6c9b8c-6hlfw

- restart coredns deployment
  : kubectl -n kube-system rollout restart deployment coredns

- show authentication privileges
  : kubectl auth can-i get pods

- compare nodes labels
  : diff -u <(kubectl get nodes kube7 -o json | jq --raw-output '.metadata.labels | keys[] | .' | sort -u) <(kubectl get nodes kube8 -o json | jq --raw-output '.metadata.labels | keys[] | .' | sort -u)

- Sort nodes by:
  - age
    : kubectl get nodes --sort-by=".metadata.creationTimestamp"
  - last transition time
    : kubectl get nodes --sort-by=".status.conditions[?(@.reason == 'KubeletReady' )].lastTransitionTime"
  - IP range
    : kubectl get nodes --sort-by=".status.addresses[?(@.type == 'InternalIP' )].address"

- continuous print logs all pods matching label
  : kubectl logs --max-log-requests=9 --namespace example-egress-gateway-kube11 -l app.kubernetes.io/name=curl --tail 10 -f

- [[https://www.ibm.com/docs/en/cloud-paks/cp-management/2.0.0?topic=kubectl-using-service-account-tokens-connect-api-server][Using service account tokens to connect with the API server - IBM Documentation]]
  : curl --cacert /run/secrets/kubernetes.io/serviceaccount/ca.crt -H "Authorization: Bearer $(cat /run/secrets/kubernetes.io/serviceaccount/token)" https://kubernetes.intr:6443/api/v1/pod/namespaces/jenkins

** authentication

[[https://stackoverflow.com/questions/51612976/how-to-view-members-of-subject-with-group-kind][kubernetes - How to view members of subject with Group kind - Stack Overflow]]

#+begin_example
  $ kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.subjects[0].kind=="Group") | select(.subjects[0].name=="system:masters")'
  {
    "apiVersion": "rbac.authorization.k8s.io/v1",
    "kind": "ClusterRoleBinding",
    "metadata": {
      "annotations": {
        "rbac.authorization.kubernetes.io/autoupdate": "true"
      },
      "creationTimestamp": "2022-05-14T12:14:48Z",
      "labels": {
        "kubernetes.io/bootstrapping": "rbac-defaults"
      },
      "name": "cluster-admin",
      "resourceVersion": "201",
      "uid": "df5858b7-b5e4-4796-a167-70cab0fe26a0"
    },
    "roleRef": {
      "apiGroup": "rbac.authorization.k8s.io",
      "kind": "ClusterRole",
      "name": "cluster-admin"
    },
    "subjects": [
      {
        "apiGroup": "rbac.authorization.k8s.io",
        "kind": "Group",
        "name": "system:masters"
      }
    ]
  }
#+end_example

#+begin_example
  $ kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.subjects[0].kind=="Group") | select(.subjects[0].name=="system:masters") | .metadata.name'
  cluster-admin
#+end_example

#+begin_example
  $ kubectl get clusterrolebindings -o go-template='{{range .items}}{{range .subjects}}{{.kind}}-{{.name}} {{end}} {{" - "}} {{.metadata.name}} {{"\n"}}{{end}}' | grep "^Group-system:masters"
  Group-system:masters   -  cluster-admin
#+end_example

Have a read through the Kubernetes 'Authenticating' docs. Kubernetes does not
have an in-built mechanism for defining and controlling users (as distinct
from ServiceAccounts which are used to provide a cluster identity for Pods,
and therefore services running on them).

This means that Kubernetes does not therefore have any internal DB to
reference, to determine and display group membership.

In smaller clusters, x509 certificates are typically used to authenticate
users. The API server is configured to trust a CA for the purpose, and then
users are issued certificates signed by that CA. As you had noticed, if the
subject contains an 'Organisation' field, that is mapped to a Kubernetes
group. If you want a user to be a member of more than one group, then you
specify multiple 'O' fields. (As an aside, to my mind it would have made more
sense to use the 'OU' field, but that is not the case)

In answer to your question, it appears that in the case of a cluster where
users are authenticated by certificates, your only route is to have access to
the issued certs, and to check for the presence of the 'O' field in the
subject. I guess in more advanced cases, Kubernetes would be integrated with a
centralised tool such as AD, which could be queried natively for group
membership.

*** Anonymous access

[[https://kubernetes.io/docs/reference/access-authn-authz/rbac/][Using RBAC Authorization | Kubernetes]]

*** User roles
- [[https://kubernetes.io/docs/reference/access-authn-authz/rbac/#auto-reconciliation][Using RBAC Authorization | Kubernetes]]

*** Cheat sheet

: kubectl get rolebindings,clusterrolebindings --all-namespaces  -o custom-columns='KIND:kind,NAMESPACE:metadata.namespace,NAME:metadata.name,SERVICE_ACCOUNTS:subjects[?(@.kind=="ServiceAccount")].name'

#+begin_example
  $ kubectl get clusterroles system:discovery -o yaml
#+end_example

*** kubectl config generation

#+begin_example
  kubectl config --kubeconfig config set-cluster k8s --server=https://kubernetes.intr:6443 --certificate-authority=ssl/ca.pem --embed-certs
  kubectl config --kubeconfig config set-credentials view --client-key ssl/view-key.pem --client-certificate ssl/view.pem --embed-certs
  kubectl config --kubeconfig config set-context default --cluster=k8s --user=view --namespace default
  kubectl config --kubeconfig config use-context default
#+end_example

** Balance
#+begin_src markdown
  [amazon web services - Spread pods over cluster - DevOps Stack Exchange](https://devops.stackexchange.com/questions/12246/spread-pods-over-cluster)

  > Take a look at the [Descheduler](https://github.com/kubernetes-sigs/descheduler). This project runs as a Kubernetes Job that aims at killing pods when it thinks the cluster is unbalanced.
  > 
  > The [`LowNodeUtilization`](https://github.com/kubernetes-sigs/descheduler#lownodeutilization) strategy seems to fit your case:
  > 
  > > This strategy finds nodes that are under utilized and evicts pods, if possible, from other nodes in the hope that recreation of evicted pods will be scheduled on these underutilized nodes.
  > 
  > * * *
  > 
  > Another option is to apply a little of chaos engineering manually, forcing a Rolling Update on your deployment, and hopefully, the scheduler will fix the balance problem when pods are recreated.
  > 
  > You can use the `kubectl rollout restart my-deployment`. It's way better than simply deleting the pods with `kubectl delete pod`, as the rollout will ensure availability during the "rebalancing" (although deleting the pods altogether increases your chances for a better rebalance).
#+end_src

** CronJob on every Kubernetes node
#+begin_src yaml
  apiVersion: batch/v1beta1
  kind: CronJob
  metadata:
    name: mycronjob
    labels:
      jobgroup: parallel
  spec:
    schedule: "*/5 * * * *"
    successfulJobsHistoryLimit: 0
    failedJobsHistoryLimit: 0
    jobTemplate:
      spec:
        template:
          metadata:
            name: kubejob
            labels:
              jobgroup: parallel
          spec:
            topologySpreadConstraints:
              - maxSkew: 2
                topologyKey: kubernetes.io/hostname
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchLabels:
                    jobgroup: parallel
            containers:
            - name: mycron-container
              image: alpine
              imagePullPolicy: IfNotPresent
              command: ['sh', '-c', 'echo Job Pod is Running ; sleep 10']
            restartPolicy: OnFailure
            terminationGracePeriodSeconds: 0
        parallelism: 5
    concurrencyPolicy: Allow
#+end_src

** Prometheus
- [[https://prometheus.kubernetes.corp1.majordomo.ru/graph?g0.expr=topk(10%2C%20sum%20by%20(container)%20(kube_pod_container_resource_limits%7Bresource%3D%22cpu%22%7D))&g0.tab=1&g0.stacked=0&g0.show_exemplars=0&g0.range_input=1h&g1.expr=topk(15%2C%20node_namespace_pod_container%3Acontainer_cpu_usage_seconds_total%3Asum_irate%7B%7D)&g1.tab=1&g1.stacked=0&g1.show_exemplars=0&g1.range_input=2w&g2.expr=topk(15%2C%20sum(container_memory_working_set_bytes%7Bjob%3D%22kubelet%22%2C%20metrics_path%3D%22%2Fmetrics%2Fcadvisor%22%2C%20container!%3D%22%22%2C%20image!%3D%22%22%7D)%20by%20(container))&g2.tab=1&g2.stacked=0&g2.show_exemplars=0&g2.range_input=1h][Prometheus Top pods]]
* CNI
- [[https://github.com/platform9/luigi][platform9/luigi: The plumber you'll hire to install all your Kubernetes network plumbing]]

* Community
- [[https://kubernetes.slack.com/][kubernetes.slack.com]]
  - [[https://slack.kubernetes.io/][slack.kubernetes.io]]
- [[https://discuss.kubernetes.io/]]
- [[https://npwg-team.slack.com/][Multus Slack Invite]]

* Distribution
- [[https://github.com/labring/sealos][labring/sealos: Sealos is a Kubernetes distribution, a Cloud Operating System designed for managing cloud-native applications. Demo: https://cloud.sealos.io]]

* Helm
- [[https://helm.sh/docs/topics/charts/][Helm Documentation | Charts]]
- [[https://github.com/truecharts/charts][truecharts/charts: Community App Catalog for TrueNAS SCALE]]
- [[https://github.com/twuni/docker-registry.helm][twuni/docker-registry.helm: Helm chart for a Docker registry. Successor to stable/docker-registry chart.]]
** Tools
- [[https://github.com/EladLeev/kubeseal-convert][EladLeev/kubeseal-convert: A tool for importing secrets from a pre-existing secrets management systems (e.g. Vault, Secrets Manager) into a SealedSecret]]
- [[https://github.com/etcd-carry/etcd-carry][etcd-carry/etcd-carry: etcd-carry provides the ability to synchronize resources in the K8s cluster that meet custom rules to the standby k8s cluster in real time.]]
- [[https://github.com/kvesta/vesta][kvesta/vesta: A static analysis of vulnerabilities, Docker and Kubernetes cluster configuration detect toolkit]]
- [[https://github.com/jatalocks/kube-reqsizer][jatalocks/kube-reqsizer: A Kubernetes controller for automatically optimizing pod requests based on their continuous usage. VPA alternative that can work with HPA.]]
- [[https://github.com/databus23/schelm][databus23/schelm: Render a helm manifest to a directory]]
** Cheat sheet
: helm create mychart

* How it works

First of all you should have a working etcd.

#+begin_example
  [root@kube1:~]# ss -tulpn
  Netid          State           Recv-Q          Send-Q                   Local Address:Port                     Peer Address:Port          Process
  tcp            LISTEN          0               4096                         127.0.0.1:38421                         0.0.0.0:*              users:(("containerd",=885,fd=14))
  tcp            LISTEN          0               128                       172.16.103.9:22                            0.0.0.0:*              users:(("sshd",pid=911,fd=3))
  tcp            LISTEN          0               4096                      172.16.103.9:443                           0.0.0.0:*              users:(("kube-apiserver",pid=3104049,fd=7))
  tcp            LISTEN          0               4096                      172.16.103.9:2379                          0.0.0.0:*              users:(("etcd",pid=3104054,fd=9))
  tcp            LISTEN          0               4096                      172.16.103.9:2380                          0.0.0.0:*              users:(("etcd",pid=3104054,fd=8))
#+end_example

After that you should start kube-apiserver, which allows to use kubectl
utility, which connects you to the cluster.

#+begin_example
  [root@kube1:~]# kubectl --insecure-skip-tls-verify get -A all
  NAMESPACE   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
  default     service/kubernetes   ClusterIP   10.0.0.1     <none>        443/TCP   10h
#+end_example

=kubelet= will start itself and =containerd= service

starting the following units: systemd-modules-load.service, systemd-sysctl.service
the following new units were started: containerd.service, kubelet.service

* How-to
- [[https://habr.com/ru/post/569124/][Cам себе облако: установка ELK и TICK стеков в Kubernetes / Хабр]]

* Ingress

- install
  : kubectl create namespace ingress-nginx
  : helm install -n ingress-nginx ingress-nginx ingress-nginx/ingress-nginx
* Installation
- [[https://github.com/adieu/nixos-k8s-flake][adieu/nixos-k8s-flake: Kubernetes Flake for NixOS with full control]]
- [[https://github.com/alexellis/k8s-on-raspbian][alexellis/k8s-on-raspbian: Kubernetes on Raspbian (Raspberry Pi)]]
- [[https://github.com/banzaicloud/pke][banzaicloud/pke: PKE is an extremely simple CNCF certified Kubernetes installer and distribution, designed to work on any cloud, VM or bare metal.]]
- [[https://github.com/bbigras/k3s-on-fly.io][bbigras/k3s-on-fly.io]]
- [[https://github.com/c3os-io/c3os][c3os-io/c3os: Linux Immutable OS for Automated (Decentralized) Kubernetes clusters with k3s]]
- [[https://github.com/canonical/microk8s][canonical/microk8s: MicroK8s is a small, fast, single-package Kubernetes for developers, IoT and edge.]]
- [[https://github.com/containerd/containerd/tree/main/contrib/ansible][containerd/contrib/ansible at main · containerd/containerd]]
- [[https://github.com/devtron-labs/devtron][devtron-labs/devtron: Tool integration platform for Kubernetes]]
- [[https://github.com/edgelesssys/constellation][edgelesssys/constellation: Constellation is the first Confidential Kubernetes. Constellation shields entire Kubernetes clusters from the (cloud) infrastructure using confidential computing.]]
- [[https://kubernetes.io/docs/setup/][Getting started | Kubernetes]]
  - [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/][Creating a cluster with kubeadm | Kubernetes]]
  - [[https://github.com/vmware-tanzu/sonobuoy][vmware-tanzu/sonobuoy: Sonobuoy is a diagnostic tool that makes it easier to understand the state of a Kubernetes cluster by running a set of Kubernetes conformance tests and other plugins in an accessible and non-destructive manner.]]
- [[https://github.com/giantswarm/mayu][giantswarm/mayu: Mayu helps you to provision a cluster of bare metal machines with CoreOS Container Linux]]
- [[https://github.com/justinas/nixos-ha-kubernetes][justinas/nixos-ha-kubernetes: Toy highly-available Kubernetes cluster on NixOS]]
- [[https://github.com/k0sproject/k0s][k0sproject/k0s: k0s - The Zero Friction Kubernetes by Team Lens]]
- [[https://github.com/karmada-io/karmada][karmada-io/karmada: Open, Multi-Cloud, Multi-Cluster Kubernetes Orchestration]]
- [[https://github.com/kelseyhightower/standalone-kubelet-tutorial][kelseyhightower/standalone-kubelet-tutorial: Standalone Kubelet Tutorial]]
- [[https://kubernetes.io/docs/reference/setup-tools/kubeadm/][Kubeadm | Kubernetes]]
  - [[https://github.com/jpetazzo/ampernetacle][jpetazzo/ampernetacle deploy a Kubernetes cluster on Oracle Cloud Inafrastructure]]
- [[https://github.com/kubealex/libvirt-k8s-provisioner][kubealex/libvirt-k8s-provisioner: Automate your k8s installation]]
- [[https://collabnix.com/3-node-kubernetes-cluster-on-bare-metal-system-in-5-minutes/][Kubernetes Cluster on Bare Metal System Made Possible using MetalLB – Collabnix]]
- [[https://blog.alexellis.io/kubernetes-in-10-minutes/][Kubernetes on bare-metal in 10 minutes]]
- [[https://jamesdefabia.github.io/docs/getting-started-guides/docker/][Kubernetes - Running Kubernetes Locally via Docker]]
- [[https://github.com/kubesphere/kubesphere][kubesphere/kubesphere: The container platform tailored for Kubernetes multi-cloud, datacenter, and edge management ⎈ 🖥 ☁️]]
- [[https://github.com/kvaps/kubefarm][kvaps/kubefarm: Automated Kubernetes deployment and the PXE-bootable servers farm]]
- [[https://github.com/kvaps/kubernetes-in-kubernetes][kvaps/kubernetes-in-kubernetes: Deploy Kubernetes in Kubernetes using Helm]]
- [[https://github.com/miekg/vks][miekg/vks: Virtual Kubelet Provider for Systemd]]
- [[https://github.com/MusicDin/kubitect][MusicDin/kubitect: Kubitect provides a simple way to set up a highly available Kubernetes cluster across multiple hosts.]]
- [[https://github.com/NetBook-ai/spawner][NetBook-ai/spawner: Mulit-Cloud infrastructure orchestrator for kubernetes first development.]]
- [[https://github.com/nocalhost/nocalhost][nocalhost/nocalhost: Nocalhost is Cloud Native Dev Environment.]]
- [[https://www.cloudbees.com/blog/one-command-to-kubernetes-with-docker-compose][One Command to Kubernetes with Docker Compose | Cloudbees Blog]]
- [[https://github.com/poseidon/typhoon][poseidon/typhoon: Minimal and free Kubernetes distribution with Terraform]]
- [[https://github.com/rancher/k3os][rancher/k3os: Purpose-built OS for Kubernetes, fully managed by Kubernetes.]]
- [[https://github.com/techno-tim/k3s-ansible][techno-tim/k3s-ansible: A fully automated HA k3s etcd install with kube-vip, MetalLB, and more]]
- [[https://balaskas.gr/blog/2022/08/31/creating-a-kubernetes-cluster-with-kubeadm-on-ubuntu-2204-lts/][Ευάγγελος Μπαλάσκας - Creating a kubernetes cluster with kubeadm on Ubuntu 22.04 LTS » Evaggelos Balaskas - System Engineer]]

** Rancher

- [[https://rancher.com/quick-start][Getting Started with Kubernetes | Rancher Quick Start]]

#+begin_example
  01 Prepare a Linux Host
  Prepare a Linux host with supported version of Docker on the host.

  02 Start the server
  To install and run Rancher, execute the following Docker command on your host:

  $ sudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher

  To access the Rancher server UI, open a browser and go to the hostname or
  address where the container was installed. You will be guided through setting
  up your first cluster.
#+end_example

* Issues

** [[https://stackoverflow.com/questions/66252195/kubernetes-pod-wont-start-1-nodes-had-a-volume-affinity-conflict][Question]]

I have a pod that won't start with a volume affinity conflict. This is a
bare-metal cluster so it's unrelated to regions. The pod has 4 persistent
volume claims which are all reporting bound so I'm assuming it's not one of
those. There are 4 nodes, one of them is tainted so that the pod will not
start on it, one of them is tainted specifically so that the pod WILL start on
it. That's the only affinity I have set up to my knowledge. The message looks
like this:

0/4 nodes are available: 1 node(s) had taint {XXXXXXX}, 
that the pod didn't tolerate, 1 node(s) had volume node 
affinity conflict, 2 Insufficient cpu, 2 Insufficient memory.

This is what I would have expected apart from the volume affinity
conflict. There are no other affinities set other than to point it at this
node. I'm really not sure why it's doing this or where to even begin. The
message isn't super helpful. It does NOT say which node or which volume there
is a problem with. The one thing I don't really understand is how binding
works. One of the PVC's is mapped to a PV on another node however it is
reporting as bound so I'm not completely certain if that's the problem. I am
using local-storage as the storage class. I'm wondering if that's the problem
but I'm fairly new to Kubernetes and I'm not sure where to look.

** Answer

You got 4 Nodes but none of them are available for scheduling due to a
different set of conditions. Note that each Node can be affected by multiple
issues and so the numbers can add up to more than what you have on total
nodes. Let's try to address these issues one by one:

    Insufficient memory: Execute kubectl describe node <node-name> to check
    how much free memory is available there. Check the requests and limits of
    your pods. Note that Kubernetes will block the full amount of memory a pod
    requests regardless how much this pod uses.

    Insufficient cpu: Analogical as above.

    node(s) had volume node affinity conflict: Check out if the nodeAffinity
    of your PersistentVolume (kubectl describe pv) matches the node label
    (kubectl get nodes). Check if the nodeSelector in your pod also
    matches. Make sure you set up the Affinity and/or AntiAffinity rules
    correctly. More details on that can be found here.

    node(s) had taint {XXXXXXX}, that the pod didn't tolerate: You can use
    kubectl describe node to check taints and kubectl taint nodes <node-name>
    <taint-name>- in order to remove them. Check the Taints and Tolerations
    for more details.

* Katacoda Kubernetes

Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.

More details can be found at https://github.com/kubernetes/minikube

** Step 1 - Start Minikube
Minikube has been installed and configured in the environment. Check that it is properly installed, by running the minikube version command:

minikube version

Start the cluster, by running the minikube start command:

minikube start --wait=false

Great! You now have a running Kubernetes cluster in your online terminal. Minikube started a virtual machine for you, and a Kubernetes cluster is now running in that VM.

** Step 2 - Cluster Info
The cluster can be interacted with using the kubectl CLI. This is the main approach used for managing Kubernetes and the applications running on top of the cluster.

Details of the cluster and its health status can be discovered via kubectl cluster-info

To view the nodes in the cluster using kubectl get nodes

If the node is marked as NotReady then it is still starting the components.

This command shows all nodes that can be used to host our applications. Now we have only one node, and we can see that it’s status is ready (it is ready to accept applications for deployment).

** Step 3 - Deploy Containers
With a running Kubernetes cluster, containers can now be deployed.

Using kubectl run, it allows containers to be deployed onto the cluster - kubectl create deployment first-deployment --image=katacoda/docker-http-server

The status of the deployment can be discovered via the running Pods - kubectl get pods

Once the container is running it can be exposed via different networking options, depending on requirements. One possible solution is NodePort, that provides a dynamic port to a container.

kubectl expose deployment first-deployment --port=80 --type=NodePort

The command below finds the allocated port and executes a HTTP request.

export PORT=$(kubectl get svc first-deployment -o go-template='{{range.spec.ports}}{{if .nodePort}}{{.nodePort}}{{"\n"}}{{end}}{{end}}')
echo "Accessing host01:$PORT"
curl host01:$PORT

The result is the container that processed the request.

** Step 4 - Dashboard
Enable the dashboard using Minikube with the command minikube addons enable dashboard

Make the Kubernetes Dashboard available by deploying the following YAML definition. This should only be used on Katacoda.

kubectl apply -f /opt/kubernetes-dashboard.yaml

The Kubernetes dashboard allows you to view your applications in a UI. In this deployment, the dashboard has been made available on port 30000 but may take a while to start.

To see the progress of the Dashboard starting, watch the Pods within the kube-system namespace using kubectl get pods -n kubernetes-dashboard -w

Once running, the URL to the dashboard is https://2886795308-30000-simba08.environments.katacoda.com/

* kubespray
- [[https://github.com/pulumi/kubespy][pulumi/kubespy: Tools for observing Kubernetes resources in real time, powered by Pulumi.]]

#+begin_src bash
  kubectl -n backup delete job/kubespray

  cat << EOF | kubectl apply -f -
  apiVersion: batch/v1
  kind: Job
  metadata:
    labels:
      app: kubespray
    name: kubespray
    namespace: backup
  spec:
    backoffLimit: 1
    completions: 1
    parallelism: 1
    template:
      metadata:
        labels:
          app: kubespray
      spec:
        containers:
        - command:
          - /usr/bin/ansible-playbook
          - -i
          - /tmp/project-kubespray/inventory/project/inventory.ini
          - cluster.yml
          image: registry.gitlab.com/egeneralov/aio-bundle:a06acba5
          name: kubespray
          volumeMounts:
          - mountPath: /root/.ssh/id_rsa
            name: kubespray
            subPath: id_rsa
          - mountPath: /root/.ssh/config
            name: kubespray
            subPath: config
          - mountPath: /tmp/project-kubespray
            name: kubespray-files
          workingDir: /tmp/project-kubespray
        restartPolicy: Never
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 384
            items:
            - key: id_rsa
              path: id_rsa
            - key: id_rsa.pub
              path: id_rsa.pub
            - key: config
              path: config
            name: backup
          name: kubespray
        - hostPath:
            path: /tmp/project-kubespray
            type: DirectoryOrCreate
          name: kubespray-files
  EOF
#+end_src
* kubevirt

#+begin_example
  oleg@guixsd ~$ kubectl get pods -n kubevirt
  NAME                               READY   STATUS    RESTARTS        AGE
  virt-api-8986f8d94-m5zr4           1/1     Running   9 (99m ago)     6h56m
  virt-handler-lvzs4                 1/1     Running   28 (101m ago)   2d10h
  virt-controller-5599f57bf4-rjsr9   1/1     Running   9 (101m ago)    6h56m
  virt-controller-5599f57bf4-5qljd   1/1     Running   9 (101m ago)    6h56m
  virt-api-8986f8d94-gww4h           1/1     Running   9 (99m ago)     6h56m
  virt-operator-5df564dc7c-258xk     1/1     Running   9 (101m ago)    6h56m
  virt-operator-5df564dc7c-q7mj2     1/1     Running   9 (101m ago)    6h56m
#+end_example

* kustomize

#+begin_src bash
  for image in $(for commit in $(git branches-by-date-origin | awk '/apps-cluster1-hms-development/ { print $1 }'); do PAGER=cat git show "$commit" | grep -A 1 '^+-' | awk '{ print $NF }' | xargs echo; done | sed 's/\s/:/' | xargs echo); do kustomize edit set image "$image"; done
#+end_src

#+begin_src bash
  for image in $(for commit in $(git branches-by-date-origin | awk '/apps-cluster1-hms-development/ { print $1 }'); do PAGER=cat git show "$commit" | grep -A 1 '^-  newTag' | awk '{ print $NF }' | xargs echo; done | sed 's/\s/:/' | xargs echo); do eval "$(echo "sed -i 's:${image}:'" kustomization.yaml)"; done
#+end_src

* Learning
- [[https://www.youtube.com/watch?v=n4zxKk2an3U][(28) Learn Kubernetes with this Introductory Course in One Hour (All Core Components and Hands On!) - YouTube]]
- [[https://www.youtube.com/playlist?list=PLxNYxgaZ8Rscf-XJ5VfXgbDAk1vL4xaMl][(2) Learn Kubernetes with Google - YouTube]]
- [[https://www.youtube.com/watch?v=lkXLsD6-4jA&list=LL&index=1][(3) Kubernetes kube-proxy Modes: iptables and ipvs, Deep Dive - YouTube]]
- [[https://www.youtube.com/watch?v=xhva6DeKqVU][(7) How load balancing and service discovery works in Kubernetes - YouTube]]
- [[https://www.youtube.com/watch?v=A-tVEoYqQsk][(7) What I Learnt Fixing 50+ Broken Kubernetes Clusters — David Flanagan 2.1.4 - YouTube]]
- [[https://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/][Accessing Kubernetes Pods from Outside of the Cluster - Ales Nosek - The Software Practitioner]]
- [[https://suraj.io/post/add-new-k8s-node-bootstrap-token/][Add new Node to k8s cluster with Bootstrap token - Suraj Deshmukh]]
- [[https://github.com/ahmetb/kubernetes-network-policy-recipes][ahmetb/kubernetes-network-policy-recipes: Example recipes for Kubernetes Network Policies that you can just copy paste]]
- [[https://opensource.com/article/19/6/introduction-kubernetes-secrets-and-configmaps][An Introduction to Kubernetes Secrets and ConfigMaps | Opensource.com]]
- [[https://scribe.bus-hit.me/m/global-identity?redirectUrl=https%3A%2F%2Fitnext.io%2Fbreaking-down-and-fixing-kubernetes-4df2f22f87c3][Breaking down and fixing Kubernetes]]
- [[https://polarsquad.com/blog/check-your-kubernetes-deployments][Check your Kubernetes deployments! — Polar Squad]]
- [[https://kubernetes.io/docs/tasks/configure-pod-container/][Configure Pods and Containers | Kubernetes]]
- [[https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#clean-up][Configure Quality of Service for Pods | Kubernetes]]
- [[https://habr.com/ru/company/flant/blog/329830/][Container Networking Interface (CNI) — сетевой интерфейс и стандарт для Linux-контейнеров / Хабр]]
- [[https://github.com/container-storage-interface/spec][container-storage-interface/spec: Container Storage Interface (CSI) Specification.]]
- [[https://stackoverflow.com/questions/43189487/create-kubernetes-nginx-ingress-without-gcp-load-balancer][Create kubernetes nginx ingress without GCP load-balancer - Stack Overflow]]
- [[https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/][Create static Pods | Kubernetes]]
- [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/][Creating a cluster with kubeadm | Kubernetes]]
- [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/][Creating Highly Available Clusters with kubeadm | Kubernetes]]
- [[https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF][CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF]]
- [[https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/][Debugging Kubernetes nodes with crictl | Kubernetes]]
- [[https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container][Debug Running Pods | Kubernetes]]
- [[https://github.com/defo89/kubernetes-the-hard-way-lab][defo89/kubernetes-the-hard-way-lab: Kubernetes The Hard Way in Lab (on VMware ESXi)]]
- [[https://github.com/dgkanatsios/CKAD-exercises][dgkanatsios/CKAD-exercises: A set of exercises to prepare for Certified Kubernetes Application Developer exam by Cloud Native Computing Foundation]]
- [[https://github.com/diegolnasc/kubernetes-best-practices][diegolnasc/kubernetes-best-practices: A cookbook with the best practices to working with kubernetes.]]
- [[https://engineering.empathy.co/distributed-load-testing-with-k6/][Distributed Load Testing with K6]]
- [[https://github.com/eon01/kubernetes-workshop][eon01/kubernetes-workshop: A Gentle introduction to Kubernetes with more than just the basics. Give it a star if you like it.]]
- [[https://github.com/eranyanay/cni-from-scratch][eranyanay/cni-from-scratch: Kubernetes networking: CNI plugin from scratch, in bash]]
- [[https://github.com/erebe/personal-server][erebe/personal-server: Personal server configuration with k3s]]
- [[https://blog.container-solutions.com/fluxcd-argocd-jenkins-x-gitops-tools][FluxCD, ArgoCD or Jenkins X: Which Is the Right GitOps Tool for You?]]
- [[https://agones.dev/site/docs/faq/#how-is-traffic-routed-from-the-allocated-port-to-the-gameserver-container][Frequently Asked Questions | Agones]]
- [[https://scribe.citizen4.eu/m/global-identity?redirectUrl=https%3A%2F%2Fitnext.io%2Fgitopsify-cloud-infrastructure-with-crossplane-and-flux-d605d3043452][GitOpsify Cloud Infrastructure with Crossplane and Flux]]
- [[https://kubernetes.io/docs/reference/glossary/?fundamental=true][Glossary | Kubernetes]]
- [[https://github.com/golang-design/under-the-hood][golang-design/under-the-hood: 📚 Go: Under The Hood | https://golang.design/under-the-hood]]
- [[https://stackoverflow.com/questions/53545732/how-do-i-access-a-private-docker-registry-with-a-self-signed-certificate-using-k][How do I access a private Docker registry with a self signed certificate using Kubernetes? - Stack Overflow]]
- [[https://jvns.ca/blog/2017/08/05/how-kubernetes-certificates-work/][How Kubernetes certificate authorities work]]
- [[https://opensource.com/article/19/11/how-contribute-kubernetes][How to contribute to Kubernetes if you have a full-time job | Opensource.com]]
- [[https://www.digitalocean.com/community/tutorials/how-to-progressively-deliver-releases-using-flagger-on-digitalocean-kubernetes][How To Progressively Deliver Releases Using Flagger on DigitalOcean Kubernetes | DigitalOcean]]
- [[https://dockerlabs.collabnix.com/kubernetes/beginners/Install-and-configure-a-multi-master-Kubernetes-cluster-with-kubeadm.html][Install and configure a multi-master Kubernetes cluster with kubeadm | dockerlabs]]
- [[https://kubernetes.io/docs/concepts/cluster-administration/addons/][Installing Addons | Kubernetes]]
- [[https://docs.openshift.com/dedicated/3/dev_guide/integrating_external_services.html][Integrating External Services | Developer Guide | OpenShift Dedicated 3]]
- [[https://github.com/kelseyhightower/kubernetes-the-hard-way][kelseyhightower/kubernetes-the-hard-way: Bootstrap Kubernetes the hard way on Google Cloud Platform. No scripts.]]
- [[https://github.com/kodekloudhub/certified-kubernetes-administrator-course][kodekloudhub/certified-kubernetes-administrator-course: Certified Kubernetes Administrator - CKA Course]]
- [[https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#options-for-software-load-balancing][kubeadm/ha-considerations.md at main · kubernetes/kubeadm]]
- [[https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/][Kubernetes 1.24: Volume Expansion Now A Stable Feature | Kubernetes]]
- [[https://kubernetes.io/blog/2023/04/21/node-log-query-alpha/][Kubernetes 1.27: Query Node Logs Using The Kubelet API | Kubernetes]]
- [[https://iximiuz.com/en/posts/kubernetes-api-structure-and-terminology/][Kubernetes API Basics - Resources, Kinds, and Objects]]
- [[https://wiki.archlinux.org/title/Kubernetes][Kubernetes - ArchWiki]]
- [[https://serverfault.com/questions/955293/how-to-increase-disk-size-in-a-stateful-set][kubernetes - How to increase disk size in a stateful set - Server Fault]]
- [[https://fabianlee.org/2022/04/18/kubernetes-kustomize-with-helm-charts/][Kubernetes: kustomize with Helm charts | Fabian Lee : Software Engineer]]
- [[https://www.youtube.com/watch?v=K3jNo4z5Jx8][Kubernetes Namespaces Explained in 15 mins | Kubernetes Tutorial 21]]
  : kubectl api-resources --namespaced=true
- [[https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-volumes-example-nfs-persistent-volume.html][Kubernetes Volumes Guide – Examples for NFS and Persistent Volume - Kubernetes Book]]
- [[https://rtfm.co.ua/kubernetes-znakomstvo-chast-1-arxitektura-i-osnovnye-komponenty-obzor/][Kubernetes: знакомство, часть 1 – архитектура и основные компоненты, обзор]]
- [[https://habr.com/ru/company/southbridge/blog/334846/][Kubernetes на голом железе за 10 минут / Хабр]]
- [[https://www.youtube.com/watch?v=Z9hBIoO4KOs][KubeVirt deep dive - YouTube]]
  - [[https://github.com/davidvossel/kubevirt-demo-manifests][davidvossel/kubevirt-demo-manifests]]
- [[https://scribe.citizen4.eu/kubernetes-tutorials/learn-how-to-assign-pods-to-nodes-in-kubernetes-using-nodeselector-and-affinity-features-e62c437f3cf8][Learn How to Assign Pods to Nodes in Kubernetes Using nodeSelector and Affinity Features]]
- [[https://learnkubernetes.withgoogle.com/#multicluster-services-api][Learn Kubernetes with Google]]
- [[https://learnk8s.io/rbac-kubernetes][Limiting access to Kubernetes resources with RBAC]]
- [[https://github.com/magnologan/awesome-k8s-security][magnologan/awesome-k8s-security: A curated list for Awesome Kubernetes Security resources]]
- [[https://www.starkandwayne.com/blog/managing-your-kubernetes-cluster-with-daemonsets/][Managing Your k8s Cluster via DaemonSets - Stark & Wayne]]
- [[https://kubernetes.io/docs/reference/tools/map-crictl-dockercli/][Mapping from dockercli to crictl | Kubernetes]]
- [[https://www.cncf.io/blog/2023/03/07/my-first-experience-with-kyverno-%f0%9f%a4%af%f0%9f%a4%a9/][My first experience with Kyverno: 🤯🤩 | Cloud Native Computing Foundation]]
- [[https://github.com/neargle/my-re0-k8s-security][neargle/my-re0-k8s-security: [WIP] 整理过去的分享，从零开始的Kubernetes攻防 🧐]]
- [[https://scribe.citizen4.eu/aspecto/opentelemetry-operator-for-kubernetes-practical-guide-part-4-1d8d86eb2bb][OpenTelemetry Operator for Kubernetes: Practical Guide | Part 4]]
- [[https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster][Operating etcd clusters for Kubernetes | Kubernetes]]
- [[http://ceur-ws.org/Vol-2839/paper11.pdf][paper11.pdf]]
- [[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#recovering-from-failure-when-expanding-volumes][Persistent Volumes | Kubernetes]]
- Pin pod to specific node:
  - Affinity
  - Toleration
  - Taint
- [[https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/][Pod Lifecycle | Kubernetes]]
- Port forwarding
  - =kubectl port-forward my-deployment-xxxxxxxxxx-xxxxx 8005:80 &=
- [[https://piotrminkowski.com/2022/12/02/renew-certificates-on-kubernetes-with-cert-manager-and-reloader/][Renew Certificates on Kubernetes with Cert Manager and Reloader]]
- [[https://github.com/rootsongjc/kubernetes-handbook][rootsongjc/kubernetes-handbook: Kubernetes中文指南/云原生应用架构实战手册 - https://jimmysong.io/kubernetes-handbook]]
- [[https://github.com/run-x/awesome-kubernetes][run-x/awesome-kubernetes: A curated list for awesome kubernetes projects, tools and resources.]]
- [[https://kubernetes.io/docs/concepts/scheduling-eviction/_print/][Scheduling, Preemption and Eviction | Kubernetes]]
- [[https://devopstales.github.io/kubernetes/k8s-metallb-bgp-pfsense/][Self-hosted Load Balancer for bare metal Kubernetes - devopstales]]
- [[https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/][Set Kubelet parameters via a config file | Kubernetes]]
- [[https://gabrieltanner.org/blog/ha-kubernetes-cluster-using-k3s/][Setting up a HA Kubernetes cluster using K3S]]
- [[https://medium.com/@sven_50828/setting-up-a-high-availability-kubernetes-cluster-with-multiple-masters-31eec45701a2][Setting up a high-availability Kubernetes cluster with multiple masters | by Sven Hakvoort | Medium]]
- Show documentation
  - =kubectl explain deployment=
  - =kubectl explain deployment.spec=
- [[https://www.cncf.io/blog/2021/04/12/simplifying-multi-clusters-in-kubernetes/][Simplifying multi-clusters in Kubernetes | Cloud Native Computing Foundation]]
- [[https://github.com/stefanprodan/podinfo][stefanprodan/podinfo: Go microservice template for Kubernetes]]
- [[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/][Taints and Tolerations | Kubernetes]]
- [[https://github.com/techiescamp/kubernetes-learning-path][techiescamp/kubernetes-learning-path: A roadmap to learn Kubernetes from scratch (Beginner to Advanced level)]]
- [[https://habr.com/ru/company/nixys/blog/649663/][Terraform модуль для создания RBAC-правил / Хабр]]
- [[https://www.freecodecamp.org/news/the-kubernetes-handbook/][The Kubernetes Handbook – Learn Kubernetes for Beginners]]
- [[https://kubernetes.io/docs/tasks/debug/debug-cluster/][Troubleshooting Clusters | Kubernetes]]
- [[https://www.alibabacloud.com/help/en/container-service-for-kubernetes/latest/use-the-host-network][Use the host network]]
  #+begin_src yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      hostNetwork: true
      containers:
      - name: nginx
        image: nginx
  #+end_src
- [[https://piotrminkowski.com/2023/03/20/vault-with-secrets-store-csi-driver-on-kubernetes/][Vault with Secrets Store CSI Driver on Kubernetes]]
- [[https://github.com/Vinum-Security/kubernetes-security-checklist][Vinum-Security/kubernetes-security-checklist: Kubernetes Security Checklist and Requirements - All in One (authentication, authorization, logging, secrets, configuration, network, workloads, dockerfile)]]
- [[https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist][walidshaari/Certified-Kubernetes-Security-Specialist: Curated resources help you prepare for the CNCF/Linux Foundation CKS 2021 "Kubernetes Certified Security Specialist" Certification exam. Please provide feedback or requests by raising issues, or making a pull request. All feedback for improvements are welcome. thank you.]]
- [[https://github.com/walidshaari/Kubernetes-Certified-Administrator][walidshaari/Kubernetes-Certified-Administrator: Online resources that will help you prepare for taking the CNCF CKA 2020 "Kubernetes Certified Administrator" Certification exam. with time, This is not likely the comprehensive up to date list - please make a pull request if there something that should be added here.]]
- [[https://www.techtarget.com/searchitoperations/tip/When-to-use-Kubernetes-operators-vs-Helm-charts][When to use Kubernetes operators vs. Helm charts]]
- [[https://scribe.citizen4.eu/@senthilrch/woot-kubernetes-adds-support-for-swap-memory-92541aad01a0][Woot…Kubernetes Adds Support for Swap Memory]]
- [[https://github.com/wrmilling/k3s-gitops][wrmilling/k3s-gitops: GitOps principles to define kubernetes cluster state via code]]
- [[https://developers.redhat.com/blog/2020/11/10/you-probably-need-liveness-and-readiness-probes#example_1__a_static_file_server__nginx_][You (probably) need liveness and readiness probes | Red Hat Developer]]
- [[https://habr.com/ru/company/qiwi/blog/585392/][Безопасность Kubernetes — это просто / Хабр]]
- [[https://habr.com/ru/company/nixys/blog/658985/][Введение в k3s: познакомимся с младшим братом k8s / Хабр]]
- [[https://habr.com/ru/companies/flant/articles/541642/][Вертикальное автомасштабирование pod'ов в Kubernetes: полное руководство / Хабр]]
- [[https://habr.com/ru/company/flant/blog/420813/][За кулисами сети в Kubernetes / Хабр]]
- [[https://habr.com/ru/post/657641/][Как Kubernetes создает и запускает контейнеры: Иллюстрированное руководство / Хабр]]
- [[https://habr.com/ru/company/flant/blog/521406/][Как pod в Kubernetes получает IP-адрес / Хабр]]
- [[https://serveradmin.ru/kubernetes-ingress/][Настройка Ingress controller в Kubernetes — Server Admin]]
- [[https://habr.com/ru/company/nixys/blog/652925/][О границах масштабируемости Kubernetes / Хабр]]
- [[https://habr.com/ru/company/nixys/blog/664660/][Ошибочные шаблоны при построении образов контейнеров / Хабр]]
- [[https://habr.com/ru/company/netologyru/blog/580902/][Планирование подов для узлов Kubernetes: развёрнутое руководство / Хабр]]
- [[https://dotsandbrackets.com/kubernetes-example-ru/][Разбираем Kubernetes пример - Dots and Brackets]]
- [[http://itisgood.ru/2019/11/18/raspredelennoe-hranenie-dannyh-v-kubernetes/][☸️ Распределенное хранение данных в Kubernetes — IT is good]]
- [[https://habr.com/ru/post/673730/][Сертификаты K8S или как распутать вермишель Часть 1 / Хабр]]
- [[https://habr.com/ru/company/flant/blog/676678/][Снапшоты в Kubernetes: что это и как ими пользоваться / Хабр]]
- [[https://habr.com/ru/company/otus/blog/653973/][Тестирование сетевых политик с помощью Minikube / Хабр]]
- [[https://habr.com/ru/company/nixys/blog/657359/][Универсальный Helm-чарт для ваших приложений / Хабр]]
- [[https://serveradmin.ru/kubernetes-ustanovka/][Установка кластера Kubernetes | serveradmin.ru]]

** [[https://discuss.kubernetes.io/t/high-availability-host-numbers/13143][High Availability Host Numbers - General Discussions - Discuss Kubernetes]]
K8s uses the RAFT consensus algorithm for quorum. In order to maintain quorum, you will need floor(n/2)+1 healthy master nodes.

Practicaly this means:

    1 master node: you will require 1 healthy master node for quorum, the loss of the master node will render the cluster headless.
    2 master nodes: you will require 2 healthy master nodes for quorum, the loss of either master node will render the cluster headless.
    3 master nodes: you will require 2 healty master nodes for quorum, the loss of one of the master nodes can be compensated.
    4 master nodes: you will require 3 healty master nodes for quorum, the loss of one on the master nodes can be compensated. A setup with 4 master nodes has no advantage over a 3 master nodes setup.
    5 master nodes: you will require 3 healthy master nodes for quorum, the loss of up to two master nodes can be compsensated.
    6 master nodes: you will require 4 healty master nodes for quorum, the loss of up to two master nodes can be compensated. No advantage compared to 5 master nodes.
    7 master nodes: you will require 4 healthy master nodes for quorum, the loss of up to three master nodes can be compsensated.

This is the reason why it is recommended to use an odd number of master nodes for the control plane. More then 7 master nodes will result in a overhead for determining cluster membership and quorum, it is not recommended. Depending on your needs, you typically end up with 3 or 5 master nodes.

* Misc
- [[https://github.com/aquasecurity/kube-bench][aquasecurity/kube-bench: Checks whether Kubernetes is deployed according to security best practices as defined in the CIS Kubernetes Benchmark]]
- [[https://github.com/kubevirt/kubevirt][kubevirt/kubevirt: Kubernetes Virtualization API and runtime in order to define and manage virtual machines.]]
- [[https://github.com/kubernetes-sigs/kind][kubernetes-sigs/kind: Kubernetes IN Docker - local clusters for testing Kubernetes]]
- [[https://github.com/criticalstack/quake-kube][quake-kube Quake 3 on Kubernetes]]
- [[https://github.com/oneinfra/oneinfra][oneinfra/oneinfra: Kubernetes as a Service]]
- [[https://github.com/hjacobs/kubernetes-failure-stories][hjacobs/kubernetes-failure-stories: Compilation of public failure/horror stories related to Kubernetes]]
- [[https://github.com/kubernetes/kops][kubernetes/kops: Kubernetes Operations (kops) - Production Grade K8s Installation, Upgrades, and Management]]
- [[https://github.com/argoproj/argo-cd/][argoproj/argo-cd: Declarative continuous deployment for Kubernetes.]]
- [[https://fluxcd.io/][Flux]]
- [[https://github.com/criticalstack/quake-kube][criticalstack/quake-kube: Quake 3 on Kubernetes]]
- [[https://github.com/flyteorg/flyte][flyteorg/flyte: Kubernetes-native workflow automation platform for complex, mission-critical data and ML processes at scale. It has been battle-tested at Lyft, Spotify, Freenome, and others and is truly open-source.]]
- [[https://github.com/storax/kubedoom][storax/kubedoom: Kill Kubernetes pods by playing Id's DOOM!]]
- [[https://opensource.com/article/21/6/kube-doom][Play Doom on Kubernetes | Opensource.com]]
- [[https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner][kubernetes-sigs/sig-storage-lib-external-provisioner]]

* Monitoring
- [[https://habr.com/ru/post/586206/][Kubernetes monitoring от простого к сложному (Николай Храмчихин) / Хабр]]

* Networking

: $ kubectl describe node | grep '^Name\|PodCIDR'
#+begin_example
  Name:               kube1
  PodCIDR:                      10.1.0.0/24
  PodCIDRs:                     10.1.0.0/24
  ...
  Name:               kube7
  PodCIDR:                      10.1.6.0/24
  PodCIDRs:                     10.1.6.0/24
#+end_example

* Operators
- [[https://github.com/flant/shell-operator][flant/shell-operator: Shell-operator is a tool for running event-driven scripts in a Kubernetes cluster]]
- [[https://operatorhub.io/][OperatorHub.io | The registry for Kubernetes Operators]]
- [[https://github.com/strimzi/strimzi-kafka-operator][strimzi/strimzi-kafka-operator: Apache Kafka® running on Kubernetes]]
- [[https://habr.com/ru/company/southbridge/blog/556860/][Пишем оператор Kubernetes с нуля / Хабр]]

** Prometheus Operator
- [[https://cloud.tencent.com/developer/article/2208456][Prometheus Operator中探针的使用 - 腾讯云开发者社区-腾讯云]]

* Problems

#+begin_example
  E1111 19:31:15.276111       1 reflector.go:127] pkg/mod/k8s.io/client-go@v0.19.1/tools/cache/reflector.go:156: Failed to watch *v1.Endpoints: failed to list *v1.Endpoints: endpoints is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "endpoints" in API group "" at the cluster scope
#+end_example

from [[https://giters.com/ivanfioravanti/kubernetes-the-hard-way-on-azure/issues/54][Recent update seems to be missing ClusterRole for CoreDNS - Giters]]

#+begin_src bash
  cat <<EOF | kubectl apply -f -
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: coredns
  rules:
  - apiGroups:
    - ""
    resources:
    - endpoints
    - services
    - pods
    - namespaces
    verbs:
    - list
    - watch
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: coredns
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: coredns
  subjects:
  - kind: ServiceAccount
    name: coredns
    namespace: kube-system
  EOF
#+end_src

then redeploy with:
: kubectl -n kube-system rollout restart deployment coredns

** kubevirt

*** unable to schedule after reboot
#+begin_example
  kubectl describe node kube8
  ...
  Allocatable:
    cpu:                            8
    devices.kubevirt.io/kvm:        0
    devices.kubevirt.io/sev:        0
    devices.kubevirt.io/tun:        0
    devices.kubevirt.io/vhost-net:  0
    ephemeral-storage:              441863943051
    hugepages-1Gi:                  0
    hugepages-2Mi:                  0
    memory:                         65826164Ki
    pods:                           110
#+end_example
* Prometheus
- pod resources
  : https://prometheus.kubernetes.corp1.majordomo.ru/graph?g0.expr=topk(10%2C%20sum%20by%20(container)%20(kube_pod_container_resource_limits%7Bresource%3D%22cpu%22%7D))&g0.tab=1&g0.stacked=0&g0.show_exemplars=0&g0.range_input=1h&g1.expr=topk(15%2C%20node_namespace_pod_container%3Acontainer_cpu_usage_seconds_total%3Asum_irate%7B%7D)&g1.tab=1&g1.stacked=0&g1.show_exemplars=0&g1.range_input=2w&g2.expr=topk(15%2C%20sum(container_memory_working_set_bytes%7Bjob%3D%22kubelet%22%2C%20metrics_path%3D%22%2Fmetrics%2Fcadvisor%22%2C%20container!%3D%22%22%2C%20image!%3D%22%22%7D)%20by%20(container))&g2.tab=1&g2.stacked=0&g2.show_exemplars=0&g2.range_input=1h
- persistentce volume size
  : https://prometheus.kubernetes.corp1.majordomo.ru/graph?g0.expr=(sum%20without(instance%2C%20node)%20(kubelet_volume_stats_capacity_bytes%7Bcluster%3D%22%22%2C%20job%3D%22kubelet%22%2C%20metrics_path%3D%22%2Fmetrics%22%7D)%20-%20sum%20without(instance%2C%20node)%20(kubelet_volume_stats_available_bytes%7Bcluster%3D%22%22%2C%20job%3D%22kubelet%22%2C%20metrics_path%3D%22%2Fmetrics%22%7D))&g0.tab=0&g0.stacked=0&g0.show_exemplars=0&g0.range_input=1d
* pv
** dynamic
- [[https://github.com/rancher/local-path-provisioner][rancher/local-path-provisioner: Dynamically provisioning persistent local storage with Kubernetes]]
#+begin_example
  $ kubectl create -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.22/deploy/local-path-storage.yaml
  namespace/local-path-storage created
  serviceaccount/local-path-provisioner-service-account created
  clusterrole.rbac.authorization.k8s.io/local-path-provisioner-role created
  clusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind created
  deployment.apps/local-path-provisioner created
  storageclass.storage.k8s.io/local-path created
  configmap/local-path-config created
#+end_example

#+begin_example
  $ kubectl delete -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.22/deploy/local-path-storage.yaml
  namespace "local-path-storage" deleted
  serviceaccount "local-path-provisioner-service-account" deleted
  clusterrole.rbac.authorization.k8s.io "local-path-provisioner-role" deleted
  clusterrolebinding.rbac.authorization.k8s.io "local-path-provisioner-bind" deleted
  deployment.apps "local-path-provisioner" deleted
  storageclass.storage.k8s.io "local-path" deleted
  configmap "local-path-config" deleted
#+end_example

* Storage
- [[https://vitobotta.com/2020/01/04/linstor-storage-the-kubernetes-way/][Linstor storage for Kubernetes, the Kubernetes way]]
- [[https://github.com/piraeusdatastore/piraeus][piraeusdatastore/piraeus: High Available Datastore for Kubernetes]]
- [[https://temofeev.ru/info/articles/sostoyanie-i-proizvoditelnost-resheniy-dlya-postoyannogo-khraneniya-dannykh-v-kubernetes/][Состояние и производительность решений для постоянного хранения данных в Kubernetes]]

* Tools
- [[https://habr.com/ru/company/southbridge/blog/523358/][11 инструментов, делающих Kubernetes лучше / Блог компании Southbridge / Хабр]]
- [[https://habr.com/ru/company/southbridge/blog/523790/][12 инструментов, делающих Kubernetes легче / Блог компании Southbridge / Хабр]]
- [[https://www.youtube.com/watch?v=CB79eTFbR0w][(2) 10 Must-Have Kubernetes Tools - YouTube]]
- [[https://www.youtube.com/c/%D0%90%D1%80%D1%82%D1%83%D1%80%D0%9A%D1%80%D1%8E%D0%BA%D0%BE%D0%B2/videos][(63) Артур Крюков - YouTube]]
- [[https://github.com/abahmed/kwatch][abahmed/kwatch: monitor & detect crashes in your Kubernetes(K8s) cluster instantly]]
- [[https://github.com/ahmetb/kubectl-tree][ahmetb/kubectl-tree: kubectl plugin to browse Kubernetes object hierarchies as a tree]]
- [[https://github.com/airwallex/k8s-pod-restart-info-collector][airwallex/k8s-pod-restart-info-collector: Automated troubleshooting of Kubernetes Pods issues. Collect K8s pod restart reasons, logs, and events automatically.]]
- [[https://github.com/airwallex/k8s-pod-restart-info-collector][airwallex/k8s-pod-restart-info-collector: Automated troubleshooting of Kubernetes Pods issues. Collect K8s pod restart reasons, logs, and events automatically.]]
- [[https://github.com/alexellis/arkade][alexellis/arkade: Your one-stop CLI for Kubernetes]]
- [[https://github.com/alexellis/k3sup][alexellis/k3sup: bootstrap Kubernetes with k3s over SSH < 1 min]]
- [[https://github.com/alexellis/registry-creds][alexellis/registry-creds: Replicate Kubernetes ImagePullSecrets to all namespaces]]
- [[https://github.com/alexellis/run-job][alexellis/run-job: Run a Kubernetes Job and get the logs when it's done 🏃‍♂️]]
- [[https://github.com/alibaba/open-local][alibaba/open-local: cloud-native local storage management system]]
- [[https://blog.flant.com/announcing-shell-operator-to-simplify-creating-of-kubernetes-operators/][Announcing shell-operator to simplify creating of Kubernetes operators – Flant blog]]
  - [[https://github.com/flant/shell-operator][flant/shell-operator: Shell-operator is a tool for running event-driven scripts in a Kubernetes cluster]]
- [[https://github.com/appvia/krane][appvia/krane: Kubernetes RBAC static analysis & visualisation tool]]
- [[https://github.com/aquasecurity/starboard][aquasecurity/starboard: Kubernetes-native security toolkit]]
- [[https://github.com/ArctarusLimited/Fractal][ArctarusLimited/Fractal: Zero-compromise Kubernetes resource generation with Nix, Jsonnet and Kustomize. Planned Helm support.]]
- [[https://github.com/armosec/kubescape][armosec/kubescape: kubescape is the first tool for testing if Kubernetes is deployed securely as defined in Kubernetes Hardening Guidance by to NSA and CISA (https://www.nsa.gov/News-Features/Feature-Stories/Article-View/Article/2716980/nsa-cisa-release-kubernetes-hardening-guidance/)]]
- [[https://github.com/arnehilmann/k8s-auto-updater][arnehilmann/k8s-auto-updater: auto-update your cluster: sync your docker images and restart pods running on outdated images]]
- [[https://github.com/arttor/helmify][arttor/helmify: Creates Helm chart from Kubernetes yaml]]
- [[https://github.com/aws/karpenter][aws/karpenter: Kubernetes Node Autoscaling: built for flexibility, performance, and simplicity.]]
- [[https://github.com/backube/snapscheduler][backube/snapscheduler: Scheduled snapshots for Kubernetes persistent volumes]]
- [[https://github.com/backube/volsync][backube/volsync: Asynchronous data replication for Kubernetes volumes]]
- [[https://github.com/banzaicloud/jwt-to-rbac][banzaicloud/jwt-to-rbac: JWT-to-RBAC lets you automatically generate RBAC resources based on JWT tokens]]
- [[https://github.com/bitnami-labs/charts-syncer][bitnami-labs/charts-syncer: Tool for synchronizing Helm Chart repositories.]]
- [[https://github.com/bitnami-labs/healthcheck-tools][bitnami-labs/healthcheck-tools: Set of Go tools to check different elements of your stack (SSL, SMTP, Permissions...)]]
- [[https://github.com/bitnami-labs/sealed-secrets][bitnami-labs/sealed-secrets: A Kubernetes controller and tool for one-way encrypted Secrets]]
- [[https://github.com/box/kube-iptables-tailer][box/kube-iptables-tailer: A service for better network visibility for your Kubernetes clusters.]]
- [[https://github.com/brigadecore/brigade][brigadecore/brigade: Event-driven scripting for Kubernetes]]
- [[https://www.heroku.com/careers/builds-pipeline-services-software-engineer-186][Builds & Pipeline Services Software Engineer | Heroku]]
  - [[https://github.com/opencontainers/image-spec][opencontainers/image-spec: OCI Image Format]]
  - [[https://github.com/knative][Knative - Kubernetes-based platform to build, deploy, and manage modern serverless workloads]]
    - [[https://github.com/cppforlife/knctl][cppforlife/knctl: Knative CLI]]
    - [[https://habr.com/ru/company/southbridge/blog/475630/][Knative — платформа как услуга на основе k8s с поддержкой serverless / Хабр]]
    - [[https://www.youtube.com/watch?v=AybvgX3jA5U][KNative scale to 0 - YouTube]]
    - [[https://opensource.com/article/20/11/knative][Create your first Knative app | Opensource.com]]
    - [[https://opensource.com/article/21/2/knative-eventing][A step-by-step guide to Knative eventing | Opensource.com]]
  - [[https://github.com/tektoncd/pipeline][tektoncd/pipeline: A cloud-native Pipeline resource.]]
  - [[https://buildpacks.io/][Cloud Native Buildpacks · Cloud Native Buildpacks]]
- [[https://habr.com/ru/company/flant/blog/485716/][Calico для сети в Kubernetes: знакомство и немного из опыта / Хабр]]
- [[https://github.com/carina-io/carina][carina-io/carina: Carina: an high performance and ops-free local storage for kubernetes]]
- [[https://github.com/c-bata/kube-prompt][c-bata/kube-prompt: An interactive kubernetes client featuring auto-complete.]]
- [[https://github.com/cdk8s-team/cdk8s][cdk8s-team/cdk8s: Define Kubernetes native apps and abstractions using object-oriented programming]]
- [[https://github.com/cdk-team/CDK][cdk-team/CDK: 📦 Make security testing of K8s, Docker, and Containerd easier.]]
- [[https://cello-proj.github.io/cello/][Cello - The engine for cloud deployments]]
- [[https://github.com/chenjiandongx/kubectl-images][chenjiandongx/kubectl-images: Show container images used in the cluster.]]
- [[https://github.com/cilium/hubble][cilium/hubble: Hubble - Network, Service & Security Observability for Kubernetes using eBPF]]
- [[https://github.com/cloudnativelabs/kube-router][cloudnativelabs/kube-router: Kube-router, a turnkey solution for Kubernetes networking.]]
- [[https://github.com/cloudnativelabs/kube-shell][cloudnativelabs/kube-shell: Kubernetes shell: An integrated shell for working with the Kubernetes]]
- [[https://landscape.cncf.io/][Cloud Native Landscape]]
- [[https://github.com/cloud-native-skunkworks/ubuntu-operator][cloud-native-skunkworks/ubuntu-operator: Control Ubuntu from Kubernetes.]]
- [[https://github.com/clusternet/clusternet][clusternet/clusternet: Managing your Kubernetes clusters (including public, private, edge, etc) as easily as visiting the Internet ⎈]]
- [[https://github.com/containerd/stargz-snapshotter][containerd/stargz-snapshotter: Fast container image distribution plugin with lazy pulling]]
- [[https://github.com/ContainerSolutions/kubernetes-examples][ContainerSolutions/kubernetes-examples: Minimal self-contained examples of standard Kubernetes features and patterns in YAML]]
- [[https://github.com/containers/podman-desktop][containers/podman-desktop: Podman Desktop - A graphical tool for developing on containers and Kubernetes]]
- [[https://github.com/corneliusweig/ketall][corneliusweig/ketall: Like `kubectl get all`, but get really all resources]]
- [[https://github.com/corneliusweig/rakkess][corneliusweig/rakkess: Review Access - kubectl plugin to show an access matrix for k8s server resources]]
- [[https://github.com/coroot/coroot][coroot/coroot: A monitoring and troubleshooting tool for microservice architectures.]]
- [[https://github.com/darkbitio/k8s-mirror][darkbitio/k8s-mirror: Creates a local mirror of a Kubernetes cluster in a docker container to support offline reviewing]]
- [[https://github.com/datreeio/datree][datreeio/datree: Prevent Kubernetes misconfigurations from reaching production (again 😤 )! The CLI integration provides a policy enforcement solution to run automatic checks for rule violations. See our docs: https://hub.datree.io]]
- [[https://github.com/deislabs/krustlet][deislabs/krustlet: Kubernetes Rust Kubelet]]
- [[https://github.com/derailed/k9s][derailed/k9s: 🐶 Kubernetes CLI To Manage Your Clusters In Style!]]
- [[https://github.com/derailed/popeye][derailed/popeye: 👀 A Kubernetes cluster resource sanitizer]]
- [[https://github.com/devspace-cloud/devspace][devspace-cloud/devspace: DevSpace - The Fastest Developer Tool for Kubernetes ⚡ Automate your deployment workflow with DevSpace and develop software directly inside Kubernetes.]]
- [[https://github.com/dty1er/kubecolor][dty1er/kubecolor: colorizes kubectl output]]
- [[https://github.com/eigengrau/krew2nix][eigengrau/krew2nix: Makes kubectl plug-ins from the Krew repository accessible to Nix]]
- [[https://github.com/eldadru/ksniff][eldadru/ksniff: Kubectl plugin to ease sniffing on kubernetes pods using tcpdump and wireshark]]
- [[https://github.com/emissary-ingress/emissary][emissary-ingress/emissary: open source Kubernetes-native API gateway for microservices built on the Envoy Proxy]]
- [[https://github.com/external-secrets/external-secrets][external-secrets/external-secrets: External Secrets Operator reads information from a third-party service like AWS Secrets Manager and automatically injects the values as Kubernetes Secrets.]]
- [[https://github.com/f0xtek/k8sresourceless][f0xtek/k8sresourceless: A utility to find all Kubernetes Pods that have no CPU or memory resource requests defined for their containers.]]
- [[https://github.com/FairwindsOps/polaris][FairwindsOps/polaris: Validation of best practices in your Kubernetes clusters]]
- [[https://github.com/fluxcd/flagger][fluxcd/flagger: Progressive delivery Kubernetes operator (Canary, A/B Testing and Blue/Green deployments)]]
- [[https://github.com/fluxcd/flux2][fluxcd/flux2: Open and extensible continuous delivery solution for Kubernetes. Powered by GitOps Toolkit.]]
- [[https://github.com/furiko-io/furiko][furiko-io/furiko: Cloud-native, enterprise-level cron job platform for Kubernetes]]
- [[https://github.com/gatblau/onix][gatblau/onix: A reactive configuration manager designed to support Infrastructure as a Code provisioning, and bi-directional configuration management providing a single source of truth across multi-cloud environments.]]
- [[https://github.com/giantswarm/kvm-operator][giantswarm/kvm-operator: Handles Kubernetes clusters running on a Kubernetes cluster with workers and masters in KVMs on bare metal]]
- [[https://werf.io/][Giterministic CLI tool | werf]]
- [[https://github.com/GoogleContainerTools/kaniko][GoogleContainerTools/kaniko: Build Container Images In Kubernetes]]
- [[https://github.com/GoogleContainerTools/kpt][GoogleContainerTools/kpt: A package-centric toolchain that enables a WYSIWYG configuration authoring, automation, and delivery experience, which simplifies managing Kubernetes platforms and KRM-driven infrastructure at scale by manipulating declarative Configuration as Data, separated from the code that transforms it.]]
- [[https://github.com/GoogleContainerTools/skaffold][GoogleContainerTools/skaffold: Easy and Repeatable Kubernetes Development]]
- [[https://github.com/GoogleContainerTools/skaffold][GoogleContainerTools/skaffold: Easy and Repeatable Kubernetes Development]]
- [[https://github.com/google/ko][google/ko: Build and deploy Go applications on Kubernetes]]
- [[https://github.com/groundcover-com/caretta][groundcover-com/caretta: Instant K8s service dependency map, right to your Grafana.]]
- [[https://github.com/groundcover-com/murre][groundcover-com/murre: Murre is an on-demand, scaleable source of container resource metrics for K8s.]]
- [[https://github.com/hall/kubenix][hall/kubenix: Kubernetes management with Nix]]
- [[https://goharbor.io/][Harbor cloud native repository for Kubernetes]]
- [[https://habr.com/ru/post/575646/][Helmwave v0.12.8 / Хабр]]
  - [[https://github.com/helmwave/helmwave][helmwave/helmwave: 🌊 Helmwave is like docker-compose for @helm]]
- [[https://github.com/imuxin/kubectl-watch][imuxin/kubectl-watch: A kubectl plugin to provide a pretty delta change view of being watched kubernetes resources]]
- [[https://github.com/inercia/k3x][inercia/k3x: A UI for local Kubernetes clusters created with k3d]]
- [[https://github.com/infracloudio/botkube][infracloudio/botkube: An app that helps you monitor your Kubernetes cluster, debug critical deployments & gives recommendations for standard practices]]
- [[https://github.com/inlets/inlets-operator][inlets/inlets-operator: Get public LoadBalancers on your local Kubernetes clusters]]
- [[https://github.com/instrumenta/kubeval/][instrumenta/kubeval: Validate your Kubernetes configuration files, supports multiple Kubernetes versions]]
- [[https://www.youtube.com/watch?v=KIIkVD7gujY][Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant & Doug Hellmann - YouTube]]
- [[https://www.suse.com/c/introduction-k3d-run-k3s-docker-src/][Introduction to k3d: Run K3s in Docker | SUSE Communities]]
- [[https://github.com/istio/istio][istio/istio: Connect, secure, control, and observe services.]]
- [[https://github.com/itzg/kube-metrics-exporter][itzg/kube-metrics-exporter: Simple application that accesses the Kubernetes metrics API and exports the pod metrics for Prometheus scraping]]
- [[https://github.com/jaegertracing/jaeger][jaegertracing/jaeger: CNCF Jaeger, a Distributed Tracing Platform]]
- [[https://github.com/jcmoraisjr/haproxy-ingress][jcmoraisjr/haproxy-ingress: HAProxy Ingress]]
- [[https://github.com/jdockerty/kubectl-oomd][jdockerty/kubectl-oomd: Show pods/containers which have recently been 'OOMKilled', as a kubectl plugin.]]
- [[https://github.com/jenkinsci/kubernetes-operator][jenkinsci/kubernetes-operator: Kubernetes native Jenkins Operator]]
- [[https://github.com/jeremykross/konstellate][jeremykross/konstellate: Free and Open Source GUI to Visualize Kubernetes Applications.]]
- [[https://github.com/johanhaleby/kubetail][johanhaleby/kubetail: Bash script to tail Kubernetes logs from multiple pods at the same time]]
- [[https://github.com/k0sproject/k0s][k0sproject/k0s: k0s - Zero Friction Kubernetes]]
- [[https://github.com/k3d-io/k3d][k3d-io/k3d: Little helper to run CNCF's k3s in Docker]]
- [[https://github.com/k3s-io/kine][k3s-io/kine: Run Kubernetes on MySQL, Postgres, sqlite, dqlite, not etcd.]]
- [[https://k3s.io/][K3s: Lightweight Kubernetes]]
- [[https://github.com/k8s-at-home/charts][k8s-at-home/charts: Helm charts for applications you run at home]]
- [[https://github.com/k8sgpt-ai/k8sgpt][k8sgpt-ai/k8sgpt: Giving Kubernetes SRE superpowers to everyone]]
- [[https://github.com/k8snetworkplumbingwg/multus-cni][k8snetworkplumbingwg/multus-cni: A CNI meta-plugin for multi-homed pods in Kubernetes]]
- [[https://github.com/kadalu/kadalu][kadalu/kadalu: A lightweight Persistent storage solution for Kubernetes / OpenShift / Nomad using GlusterFS in background.]]
- [[https://github.com/kakao/detek][kakao/detek: Extensible problem-detecting CLI tool for reliable Kubernetes cluster operations and rapid problem detections.]]
- [[https://github.com/kdash-rs/kdash][kdash-rs/kdash: A simple and fast dashboard for Kubernetes]]
- [[https://github.com/kedacore/keda][kedacore/keda: KEDA is a Kubernetes-based Event Driven Autoscaling component. It provides event driven scale for any container running in Kubernetes]]
- [[https://keda.sh/][KEDA | Kubernetes Event-driven Autoscaling]]
  - [[https://kubevirt.io/user-guide/virtual_machines/replicaset/][VirtualMachineInstanceReplicaSet - KubeVirt user guide]]
  - [[https://dev.to/anirudhgarg_99/scale-up-and-down-a-http-triggered-function-app-in-kubernetes-using-keda-4m42][Scale a HTTP Triggered app up and down in Kubernetes using KEDA and Prometheus - DEV Community]]
  - [[https://www.youtube.com/watch?v=nzSyOcBRKhQ][Serverless Track: KEDA - Kubernetes-based Event-Driven Autoscaling - Marek Grabarz - YouTube]]
  - [[https://www.youtube.com/watch?v=DqJTzjtQpOM][Why You Shouldn't Use K8s Autoscaling?!! - YouTube]]
  - [[https://github.com/dailymotion-oss/osiris][dailymotion-oss/osiris: A general purpose, scale-to-zero component for Kubernetes]]
  - [[https://github.com/kedacore/http-add-on][kedacore/http-add-on: Add-on for KEDA to scale HTTP workloads]]
    - [[https://github.com/kedacore/http-add-on/issues/438][Support for scaling custom resources and statefulset · Issue #438 · kedacore/http-add-on]]
- [[https://github.com/keel-hq/keel][keel-hq/keel: Kubernetes Operator to automate Helm, DaemonSet, StatefulSet & Deployment updates]]
- [[https://github.com/kei6u/kubectl-explore][kei6u/kubectl-explore: A better kubectl explain with the fuzzy finder]]
- [[https://github.com/komodorio/helm-dashboard][komodorio/helm-dashboard: The missing UI for Helm - visualize your releases]]
- [[https://github.com/konveyor/move2kube][konveyor/move2kube: Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes/Openshift.]]
- [[https://github.com/kragniz/tor-ingress-controller][kragniz/tor-ingress-controller: An ingress controller to expose kubernetes services as onion services on the tor network]]
- [[https://github.com/kris-nova/naml][kris-nova/naml: Not another markup language. Framework for replacing Kubernetes YAML with Go.]]
- [[https://github.com/ksync/ksync][ksync/ksync: Sync files between your local system and a kubernetes cluster.]]
- [[https://github.com/kubecfg/kubecfg][kubecfg/kubecfg: A tool for managing complex enterprise Kubernetes environments as code.]]
- [[https://github.com/kubecolor/kubecolor][kubecolor/kubecolor: Colorize your kubectl output]]
- [[https://github.com/kubefarm/dnsmasq-controller][kubefarm/dnsmasq-controller: Run DNS- and DHCP-server inside Kubernetes cluster]]
- [[https://github.com/kubenav/kubenav][kubenav/kubenav: kubenav is the navigator for your Kubernetes clusters right in your pocket.]]
- [[https://github.com/kubeovn/kube-ovn][kubeovn/kube-ovn: A Kubernetes Network Fabric for Enterprises that is Rich in Functions and Easy in Operations (Project under CNCF)]]
- [[https://github.com/kuberhealthy/kuberhealthy][kuberhealthy/kuberhealthy: A Kubernetes operator for running synthetic checks as pods. Works great with Prometheus!]]
- [[https://kompose.io/][Kubernetes + Compose = Kompose]]
- [[https://github.com/kubernetes-csi/csi-driver-nfs][kubernetes-csi/csi-driver-nfs: This driver allows Kubernetes to access NFS server on Linux node.]]
- [[https://github.com/kubernetes/kube-state-metrics][kubernetes/kube-state-metrics: Add-on agent to generate and expose cluster-level metrics.]]
- [[https://www.synacktiv.com/en/publications/kubernetes-namespaces-isolation-what-it-is-what-it-isnt-life-universe-and-everything.html][Kubernetes namespaces isolation - what it is, what it isn't, life,]]
- [[https://github.com/kubernetes/node-problem-detector][kubernetes/node-problem-detector: This is a place for various problem detectors running on the Kubernetes nodes.]]
- [[https://github.com/kubernetes-sigs/cluster-api][kubernetes-sigs/cluster-api: Home for Cluster API, a subproject of sig-cluster-lifecycle]]
- [[https://github.com/kubernetes-sigs/cluster-api][kubernetes-sigs/cluster-api: Home for Cluster API, a subproject of sig-cluster-lifecycle]]
- [[https://github.com/kubernetes-sigs/cluster-api-provider-kubevirt][kubernetes-sigs/cluster-api-provider-kubevirt: Cluster API Provider for KubeVirt]]
- [[https://github.com/kubernetes-sigs/community-images][kubernetes-sigs/community-images: kubectl plugin that displays images running in a Kubernetes cluster that were pulled from community owned repositories and warn the user to switch repositories if needed]]
- [[https://github.com/kubernetes-sigs/descheduler][kubernetes-sigs/descheduler: Descheduler for Kubernetes]]
- [[https://github.com/kubernetes-sigs/ingress2gateway][kubernetes-sigs/ingress2gateway: Convert Ingress resources to Gateway API resources]]
- [[https://github.com/kubernetes-sigs/kubefed][kubernetes-sigs/kubefed: Kubernetes Cluster Federation]]
- [[https://github.com/kubernetes-sigs/kube-scheduler-simulator][kubernetes-sigs/kube-scheduler-simulator: A web-based simulator for the Kubernetes scheduler]]
- [[https://github.com/kubernetes-sigs/kui][kubernetes-sigs/kui: A hybrid command-line/UI development experience for cloud-native development]]
- [[https://github.com/kubernetes-sigs/kustomize][kubernetes-sigs/kustomize: Customization of kubernetes YAML configurations]]
- [[https://github.com/kubernetes-sigs/kwok][kubernetes-sigs/kwok: Simulate thousands of fake kubelets, on a laptop with minimum resource footprint.]]
- [[https://github.com/kubernetes-sigs/metrics-server][kubernetes-sigs/metrics-server: Scalable and efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines.]]
- [[https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner][kubernetes-sigs/nfs-subdir-external-provisioner: Dynamic sub-dir volume provisioner on a remote NFS server.]]
- [[https://github.com/kubernetes-sigs/node-feature-discovery][kubernetes-sigs/node-feature-discovery: Node feature discovery for Kubernetes]]
- [[https://github.com/kubernetes-sigs/prometheus-adapter][kubernetes-sigs/prometheus-adapter: An implementation of the custom.metrics.k8s.io API using Prometheus]]
- [[https://github.com/kubesphere/kube-events][kubesphere/kube-events: K8s Event Exporting, Filtering and Alerting in Multi-Tenant Environment]]
- [[https://github.com/kubevious/kubevious][kubevious/kubevious: Kubevious - application centric Kubernetes UI and continuous assurance provider]]
- [[https://github.com/kube-vip/kube-vip][kube-vip/kube-vip: Kubernetes Control Plane Virtual IP and Load-Balancer]]
- [[https://kubevirt.io/][KubeVirt.io Building a virtualization API for Kubernetes]]
- [[https://www.cncf.io/blog/2022/07/11/kubevirt-kube-ovn-networking-for-cloud-native-virtualization/][KubeVirt + Kube-OVN: Networking for cloud native virtualization | Cloud Native Computing Foundation]]
- [[https://github.com/kubevoy/kubevoy][kubevoy/kubevoy: Slack bot for monitoring resources for kubernetes, It's customizable across multiple channels]]
- [[https://github.com/kubewharf/kelemetry][kubewharf/kelemetry: Global control plane tracing for Kubernetes]]
- [[https://github.com/kubewharf/kubegateway][kubewharf/kubegateway: Specific Layer7 Gateway for kube-apiserver]]
- [[https://github.com/kumahq/kuma][kumahq/kuma: 🐻 The multi-zone service mesh for containers, Kubernetes and VMs. Built with Envoy. CNCF Sandbox Project.]]
- [[https://github.com/kvaps/kubectl-build][kvaps/kubectl-build: Build dockerfiles directly in your Kubernetes cluster.]]
- [[https://github.com/kvaps/kubectl-node-shell][kvaps/kubectl-node-shell: Exec into node via kubectl]]
- [[https://github.com/kyverno/kyverno][kyverno/kyverno: Kubernetes Native Policy Management]]
- [[https://github.com/Ladicle/kubectl-rolesum][Ladicle/kubectl-rolesum: Summarize Kubernetes RBAC roles for the specified subjects.]]
- [[https://github.com/lastbackend/lastbackend][lastbackend/lastbackend: System for containerized apps management. From build to scaling.]]
- [[https://github.com/lensapp/lens][lensapp/lens: Lens - The Kubernetes IDE]]
- [[https://github.com/lightrun-platform/koolkits][lightrun-platform/koolkits: 🧰 Opinionated, language-specific, batteries-included debug container images for Kubernetes.]]
- [[https://github.com/LINBIT/linstor-server][LINBIT/linstor-server: High Performance Software-Defined Block Storage for container, cloud and virtualisation. Fully integrated with Docker, Kubernetes, Openstack, Proxmox etc.]]
- local development
  - Telepresence
  - garden.io
  - tilt.dev
  - okteto.com
  - devspace
  - skaffold
- [[https://github.com/loft-sh/loft][loft-sh/loft: Namespace & Virtual Cluster Manager for Kubernetes - Lightweight Virtual Clusters, Self-Service Provisioning for Engineers and 70% Cost Savings with Sleep Mode]]
- [[https://github.com/loft-sh/vcluster][loft-sh/vcluster: vcluster - Create fully functional virtual Kubernetes clusters - Each vcluster runs inside a namespace of the underlying k8s cluster. It's cheaper than creating separate full-blown clusters and it offers better multi-tenancy and isolation than regular namespaces.]]
- [[https://github.com/loft-sh/vcluster][loft-sh/vcluster: vcluster - Create fully functional virtual Kubernetes clusters - Each vcluster runs inside a namespace of the underlying k8s cluster. It's cheaper than creating separate full-blown clusters and it offers better multi-tenancy and isolation than regular namespaces.]]
- [[https://longhorn.io/][Longhorn - Cloud native distributed block storage for Kubernetes]]
- [[https://github.com/longhorn/longhorn][longhorn/longhorn: Cloud-Native distributed storage built on and for Kubernetes]]
- [[https://github.com/lucky-sideburn/kubeinvaders][lucky-sideburn/kubeinvaders: Gamified Chaos Engineering Tool for Kubernetes]]
- [[https://github.com/madhuakula/kubernetes-goat][madhuakula/kubernetes-goat: Kubernetes Goat is a "Vulnerable by Design" cluster environment to learn and practice Kubernetes security using an interactive hands-on playground 🚀]]
- [[https://github.com/metalbear-co/mirrord][metalbear-co/mirrord: Connect your local process and your cloud environment, and run local code in cloud conditions.]]
- [[https://github.com/metallb/metallb][metallb/metallb: A network load-balancer implementation for Kubernetes using standard routing protocols]]
- [[https://opensource.com/article/22/5/migrating-databases-kubernetes-using-konveyor][Migrate databases to Kubernetes using Konveyor | Opensource.com]]
  - [[https://github.com/konveyor/tackle-diva][konveyor/tackle-diva: Tackle Data-intensive Validity Analyzer]]
- [[https://opensource.com/article/21/6/migrate-vms-kubernetes-forklift][Migrate virtual machines to Kubernetes with this new tool | Opensource.com]]
- [[https://github.com/NCCloud/mayfly][NCCloud/mayfly: An operator to manage ephemeral Kubernetes resources.  Mayfly is a Kubernetes operator that enables you to create temporary resources on the cluster that will expire after a certain period of time.]]
- [[https://github.com/NCCloud/watchtower][NCCloud/watchtower: Effortlessly monitor your Kubernetes resource activity]]
- [[https://github.com/nmstate/kubernetes-nmstate][nmstate/kubernetes-nmstate: Declarative node network configuration driven through Kubernetes API.]]
- [[https://github.com/octarinesec/kube-scan][octarinesec/kube-scan: kube-scan: Octarine k8s cluster risk assessment tool]]
- [[https://github.com/okteto/okteto][okteto/okteto: Develop your applications directly in your Kubernetes Cluster]]
- [[https://github.com/omrikiei/ktunnel][omrikiei/ktunnel: A cli that exposes your local resources to kubernetes]]
- [[https://github.com/opencost/opencost][opencost/opencost: Cross-cloud cost allocation models for Kubernetes workloads]]
- [[https://github.com/openebs/openebs][openebs/openebs: Leading Open Source Container Attached Storage, built using Cloud Native Architecture, simplifies running Stateful Applications on Kubernetes.]]
- [[https://github.com/openkruise/kruise][openkruise/kruise: Automate application management on Kubernetes (project under CNCF)]]
- [[https://github.com/open-policy-agent/gatekeeper][open-policy-agent/gatekeeper: Gatekeeper - Policy Controller for Kubernetes]]
- [[https://github.com/openshift/cluster-kube-scheduler-operator][openshift/cluster-kube-scheduler-operator: Installs and maintains the kube-scheduler on a cluster.]]
- [[https://github.com/patrickdappollonio/kubectl-slice][patrickdappollonio/kubectl-slice: Split multiple Kubernetes files into smaller files with ease. Split multi-YAML files into individual files.]]
- [[https://github.com/piraeusdatastore/linstor-csi][piraeusdatastore/linstor-csi: CSI plugin for LINSTOR]]
- [[https://github.com/pixie-io/pixie][pixie-io/pixie: Instant Kubernetes-Native Application Observability]]
- [[https://github.com/porter-dev/porter][porter-dev/porter: Kubernetes powered PaaS that runs in your own cloud.]]
- [[https://habr.com/ru/company/nixys/blog/682612/][Pre-Commit хуки, о которых DevOps-инженер должен знать, чтобы управлять Kubernetes / Хабр]]
- [[https://github.com/projectcalico/calico][projectcalico/calico: Cloud native networking and network security]]
- [[https://github.com/project-copacetic/copacetic][project-copacetic/copacetic: 📦 CLI tool for directly patching container images using reports from vulnerability scanners]]
- [[https://github.com/prometheus-operator/prometheus-operator][prometheus-operator/prometheus-operator: Prometheus Operator creates/configures/manages Prometheus clusters atop Kubernetes]]
- [[https://github.com/Qovery/pleco][Qovery/pleco: Automatically removes Cloud managed services and Kubernetes resources based on tags with TTL]]
- [[https://github.com/rancher/k3s][rancher/k3s: Lightweight Kubernetes]]
- [[https://github.com/rancher/kim][rancher/kim: In ur kubernetes, buildin ur imagez]]
- [[https://github.com/rancher-sandbox/rancher-desktop][rancher-sandbox/rancher-desktop: Container Management and Kubernetes on the Desktop]]
- [[https://github.com/reactive-tech/kubegres][reactive-tech/kubegres: Kubegres is a Kubernetes operator allowing to create a cluster of PostgreSql instances and manage databases replication, failover and backup.]]
- [[https://github.com/robert-nemet/klock][robert-nemet/klock: Lock for K8s]]
- [[https://github.com/robscott/kube-capacity][robscott/kube-capacity: A simple CLI that provides an overview of the resource requests, limits, and utilization in a Kubernetes cluster]]
- [[https://github.com/robusta-dev/kubernetes-chatgpt-bot][robusta-dev/kubernetes-chatgpt-bot: A ChatGPT bot for Kubernetes issues.]]
- [[https://github.com/rook/rook][rook/rook: Storage Orchestration for Kubernetes]]
- [[https://github.com/rook/rook][rook/rook: Storage Orchestration for Kubernetes]]
- [[https://charlottemach.com/2020/11/03/windows-kubevirt-k3s.html][Running a Windows VM on KubeVirt on K3s · cookies and containers]]
- [[https://github.com/saschagrunert/kubernix][saschagrunert/kubernix: Single dependency Kubernetes clusters for local testing, experimenting and development]]
- [[https://github.com/saschagrunert/kubernix][saschagrunert/kubernix: Single dependency Kubernetes clusters for local testing, experimenting and development]]
- [[https://github.com/sbstp/kubie][sbstp/kubie: A more powerful alternative to kubectx and kubens]]
- [[https://schemahero.io/][SchemaHero - A modern approach to database schema migrations]]
- [[https://github.com/sealerio/sealer][sealerio/sealer: A tool to seal application's all dependencies and Kubernetes into CloudImage, distribute this application anywhere via CloudImage, and run it within any cluster in one command.]]
- [[https://github.com/senthilrch/kube-fledged][senthilrch/kube-fledged: A kubernetes operator for creating and managing a cache of container images directly on the cluster worker nodes, so application pods start almost instantly]]
- [[https://github.com/slok/simple-ingress-external-auth][slok/simple-ingress-external-auth: A very simple external authentication service for Kubernetes ingresses (ingress-nginx, traefik...)]]
- [[https://github.com/soluble-ai/kubetap][soluble-ai/kubetap: Kubectl plugin to interactively proxy Kubernetes Services with ease]]
- [[https://github.com/Soluto/kamus][Soluto/kamus: An open source, git-ops, zero-trust secret encryption and decryption solution for Kubernetes applications]]
- [[https://github.com/sozercan/kubectl-ai][sozercan/kubectl-ai: ✨ Kubectl plugin for OpenAI GPT]]
- [[https://github.com/spacecloud-io/space-cloud][spacecloud-io/space-cloud: Open source Firebase + Heroku to develop, scale and secure serverless apps on Kubernetes]]
- [[https://github.com/spacecloud-io/space-cloud][spacecloud-io/space-cloud: Open source Firebase + Heroku to develop, scale and secure serverless apps on Kubernetes]]
- [[https://github.com/stackrox/kube-linter][stackrox/kube-linter: KubeLinter is a static analysis tool that checks Kubernetes YAML files and Helm charts to ensure the applications represented in them adhere to best practices.]]
- [[https://github.com/stackrox/stackrox][stackrox/stackrox: The StackRox Kubernetes Security Platform performs a risk analysis of the container environment, delivers visibility and runtime alerts, and provides recommendations to proactively improve security by hardening the environment.]]
- [[https://github.com/stakater/Reloader][stakater/Reloader: A Kubernetes controller to watch changes in ConfigMap and Secrets and do rolling upgrades on Pods with their associated Deployment, StatefulSet, DaemonSet and DeploymentConfig – [✩Star] if you're using it!]]
- [[https://github.com/stashed/stash][stashed/stash: 🛅 Backup your Kubernetes Stateful Applications]]
- [[https://github.com/stern/stern][stern/stern: ⎈ Multi pod and container log tailing for Kubernetes -- Friendly fork of https://github.com/wercker/stern]]
- [[https://github.com/stevelacy/kuberhaus][stevelacy/kuberhaus: Kubernetes resource dashboard with node/pod layout and resource requests]]
- [[https://github.com/sustainable-computing-io/kepler][sustainable-computing-io/kepler: Kepler (Kubernetes-based Efficient Power Level Exporter) uses eBPF to probe energy related system stats and exports as Prometheus metrics]]
- [[https://github.com/tdihp/dspcap][tdihp/dspcap: A humble bash script set that uses daemonset to capture tcpdump from all k8s nodes, then collect the captures.]]
- [[https://github.com/teamcode-inc/kubeorbit][teamcode-inc/kubeorbit: Test your application on Kubernetes in a brand new simple way⚡]]
- [[https://github.com/team-soteria/rback][team-soteria/rback: RBAC in Kubernetes visualizer]]
- [[https://habr.com/ru/company/nixys/blog/481992/][Tekton Pipeline — Kubernetes-нативные pipelines / Хабр]]
- [[https://opensource.com/article/21/6/chaos-mesh-kubernetes][Test your Kubernetes experiments with an open source web interface | Opensource.com]]
- [[https://tinkerbell.org/][Tinkerbell - Bare Metal Automation]]
- [[https://github.com/tkestack/tke][tkestack/tke: Native Kubernetes container management platform supporting multi-tenant and multi-cluster]]
- [[https://github.com/tohjustin/kube-lineage][tohjustin/kube-lineage: A CLI tool to display all dependencies or dependents of an object in a Kubernetes cluster.]]
- [[https://github.com/tommy351/pullup][tommy351/pullup: Update Kubernetes resources by webhooks.]]
- [[https://github.com/topolvm/pvc-autoresizer][topolvm/pvc-autoresizer: Auto-resize PersistentVolumeClaim objects based on Prometheus metrics]]
- [[https://github.com/traefik/mesh][traefik/mesh: Traefik Mesh - Simpler Service Mesh]]
- [[https://github.com/tricorder-observability/starship][tricorder-observability/starship: Starship: next-generation Observability platform built with eBPF+WASM]]
- [[https://github.com/Trois-Six/k8s-diagrams][Trois-Six/k8s-diagrams: Create diagrams from the Kubernetes API with go-diagrams.]]
- [[https://github.com/troytop/cfk8sterm][troytop/cfk8sterm: A ttyd web terminal image on Alpine with cf, kubectl, and other tools]]
- [[https://github.com/txn2/kubefwd][txn2/kubefwd: Bulk port forwarding Kubernetes services for local development.]]
- [[https://github.com/up9inc/mizu][up9inc/mizu: API traffic viewer for Kubernetes enabling you to view all API communication between microservices. Think TCPDump and Wireshark re-invented for Kubernetes]]
- [[https://developers.redhat.com/blog/2020/11/18/using-multus-and-datavolume-in-kubevirt#using_multus_in_kubevirt][Using Multus and DataVolume in KubeVirt | Red Hat Developer]]
- [[https://github.com/utkuozdemir/pv-migrate][utkuozdemir/pv-migrate: CLI tool to easily migrate Kubernetes persistent volumes]]
- [[https://github.com/vfx1b/kubectl-really-get-all][vfx1b/kubectl-really-get-all: A kubectl get plugin that allows you to list every resource in your cluster]]
- [[https://github.com/vmware-tanzu/octant][vmware-tanzu/octant: Highly extensible platform for developers to better understand the complexity of Kubernetes clusters.]]
- [[https://github.com/vmware-tanzu/velero][vmware-tanzu/velero: Backup and migrate Kubernetes applications and their persistent volumes]]
- [[https://github.com/weaveworks/kured][weaveworks/kured: Kubernetes Reboot Daemon]]
- [[https://github.com/weaveworks/tf-controller][weaveworks/tf-controller: A GitOps Terraform controller for Kubernetes]]
- [[https://github.com/we-dcode/kubetunnel][we-dcode/kubetunnel: Develop microservices locally while being connected to your Kubernetes environment]]
- [[https://scribe.bus-hit.me/flant-com/kubernetes-operator-in-python-451f2d2e33f3][Writing a Kubernetes Operator in Python without frameworks and SDK]]
- [[https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml][YAML and Kubernetes syntax support - Visual Studio Marketplace]]
- [[https://github.com/yashbhutwala/kubectl-df-pv][yashbhutwala/kubectl-df-pv: kubectl plugin - giving admins df (disk free) like utility for persistent volumes]]
- [[https://github.com/yokawasa/kubectl-plugin-ssh-jump][yokawasa/kubectl-plugin-ssh-jump: A kubectl plugin to access nodes or remote services using a SSH jump Pod]]
- [[https://github.com/yunionio/cloudpods][yunionio/cloudpods: A cloud-native open-source unified multi-cloud and hybrid-cloud platform. 开源、云原生的多云管理及混合云融合平台]]
- [[https://github.com/zegl/kube-score][zegl/kube-score: Kubernetes object analysis with recommendations for improved reliability and security]]
- [[https://habr.com/ru/post/540220/][Настройка отказоустойчивого кластера Kubernetes на серверах с публичной и приватной сетью с помощью Kubeadm / Хабр]]
- [[https://habr.com/ru/company/flant/blog/524196/#][Обзор k9s — продвинутого терминального интерфейса для Kubernetes]]
- [[https://habr.com/ru/company/flant/blog/575972/][Обзор Kalm — веб-интерфейса для деплоя приложений и управления ими в Kubernetes / Хабр]]
- [[https://habr.com/ru/post/542834/][Первый взгляд на Tekton Pipelines / Хабр]]
- [[https://habr.com/ru/company/flant/blog/676678/][Снапшоты в Kubernetes: что это и как ими пользоваться / Хабр]]

** [[https://github.com/WoozyMasta/kube-dump][WoozyMasta/kube-dump: Backup a Kubernetes cluster as a yaml manifest]]
#+begin_src shell
  docker run \
         --network host \
         --tty \
         --interactive \
         --rm \
         --volume "${HOME}/.kube:/.kube" \
         --volume "${HOME}/ansible-out/files/kubernetes:/dump" \
         woozymasta/kube-dump:latest \
         dump-namespaces -d /dump --kube-config /.kube/config
#+end_src

** minikube
- [[https://habr.com/ru/company/vk/blog/648117/][Как работать с Minikube: рекомендации и полезные советы / Хабр]]
- [[https://github.com/kubevirt/demo][kubevirt/demo: Easy to use KubeVirt demo based on minikube.]]

** crossplane
- [[https://github.com/crossplane-contrib/provider-ansible][crossplane-contrib/provider-ansible: Crossplane Ansible provider]]
- [[https://github.com/crossplane/provider-template][crossplane/provider-template: A template for building @crossplane providers.]]
- [[https://github.com/crossplane/terrajet][crossplane/terrajet: Generate Crossplane Providers from any Terraform Provider]]

* [[https://blog.cloudowski.com/articles/how-to-modify-containers-wihtout-rebuilding/][How to modify containers without rebuilding their image]]


How to modify containers without rebuilding their image

6 minute read

Containers are a beautiful piece of technology that ease the development of modern applications and also the maintenance of modern environments. One thing that draws many people to them is how they reduce the time required to set up a service, or a whole environment, with everything included. It is possible mainly because there are so many container images available and ready to use. You will probably need to build your own container images with your applications, but many containers in your environment will use prebuilt images prepared by someone else. It’s especially worth considering for software that is provided by the software vendor or a trusted group of developers like it has been done in the case of “official” images published on Docker Hub. In both cases, it makes your life easier by letting someone else take care of updates, packaging new versions, and making sure it works.
But what if you want to change something in those images? Maybe it’s a minor change or something bigger that is specific for your particular usage of the service. The first instinct may tell you to rebuild that image. This, however, brings some overhead - these images will have to be published, rebuilt when new upstream versions are published, and you lose most of the benefits that come with those prebuilt versions.
There is an alternative to that - actually, I found four of them which I will describe below. These solutions will allow you to keep all the benefits and adjust the behavior of running containers in a seamless way.
Method 1 - init-containers

Init-containers were created to provide additional functionality to the main container (or containers) defined in a Pod. They are executed before the main container and can use a different container image. In case of any failure, they will prevent the main container from starting. All logs can be easily retrieved and troubleshooting is fairly simple - they are fetched just like any other container defined in a Pod by providing its name. This methods is quiote popular among services such as databases to initialize and configure them based on configuration parameters.
Example

The following example uses a dedicated empty volume for storing data initialized by an init-container. In this specific case, it’s just a simple “echo” command, but in a real-world scenario, this can be a script that does something more complex.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx-init
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      initContainers:
        - name: prepare-webpage
          image: busybox:1.28
          command: ["sh", "-c"]
          args: [
              "set -x;
              echo '<h2>Page prepared by an init container</h2>' > /web/index.html;
              echo 'Init finished successfully'
              ",
            ]
          volumeMounts:
            - mountPath: /web
              name: web
      containers:
        - image: nginx:1.19
          name: nginx
          volumeMounts:
            - mountPath: /usr/share/nginx/html/
              name: web
          ports:
            - containerPort: 80
              name: http
      volumes:
        - name: web
          emptyDir: {}

Method 2 - post-start hook

A Post-start hook can be used to execute some action just after the main container starts. It can be either a script executed in the same context as the container or an HTTP request that is executed against a defined endpoint. In most cases, it would probably be a shell script. Pod stays in the ContainerCreating state until this script ends. It can be tricky to debug since there are no logs available. There are more caveats and this should be used only for simple, non-invasive actions. The best feature of this method is that the script is executed when the service in the main container starts and can be used to interact with the service (e.g. by executing some API requests). With a proper readinessProbe configuration, this can give a nice way of initializing the application before any requests are allowed.
Example

In the following example a post-start hook executes the echo command, but again - this can be anything that uses the same set of files available on the container filesystem in order to perform some sort of initialization.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx-hook
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - image: nginx:1.19
          name: nginx
          ports:
            - containerPort: 80
              name: http
          lifecycle:
            postStart:
              exec:
                command:
                  [
                    "sh",
                    "-c",
                    "sleep 5;set -x; echo '<h2>Page prepared by a PostStart hook</h2>' > /usr/share/nginx/html/index.html",
                  ]

Method 3 - sidecar container

This method leverages the concept of the Pod where multiple containers run at the same time sharing IPC and network kernel namespaces. It’s been widely used in the Kubernetes ecosystem by projects such as Istio, Consul Connect, and many others. The assumption here is that all containers run simultaneously which makes it a little bit tricky to use a sidecar container to modify the behaviour of the main container. But it’s doable and it can be used to interact with the running application or a service. I’ve been using this feature with the Jenkins helm chart where there’s a sidecar container responsible for reading ConfigMap objects with Configuration-as-Code config entries.
Example

Nothing new here, just the “echo” command with a little caveat - since sidecar containers must obey restartPolicy setting, they must run after they finish their actions and thus it uses a simple while infinite loop. In more advanced cases this would be rather some small daemon (or a loop that checks some state) that runs like a service.


apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx-sidecar
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - image: nginx:1.19
          name: nginx
          volumeMounts:
            - mountPath: /usr/share/nginx/html/
              name: web
          ports:
            - containerPort: 80
              name: http
        - name: prepare-webpage
          image: busybox:1.28
          command: ["sh", "-c"]
          args: [
              "set -x;
              echo '<h2>Page prepared by a sidecar container</h2>' > /web/index.html;
              while :;do sleep 9999;done
              ",
            ]
          volumeMounts:
            - mountPath: /web
              name: web
      volumes:
        - name: web
          emptyDir: {}

Method 4 - entrypoint

The last method uses the same container image and is similar to the Post-start hook except it runs before the main app or service. As you probably know in every container image there is an ENTRYPOINT command defined (explicitly or implicitly) and we can leverage it to execute some arbitrary scripts. It is often used by many official images and in this method we will just prepend our own script to modify the behavior of the main container. In more advanced scenarios you could actually provide a modified version of the original entrypoint file.
Example

This method is a little bit more complex and involves creating a ConfigMap with a script content that is executed before the main entrypoint. Our script for modifying nginx entrypoint is embedded in the following ConfigMap

apiVersion: v1
kind: ConfigMap
metadata:
  name: scripts
data:
  prestart-script.sh: |-
    #!/usr/bin/env bash

    echo '<h2>Page prepared by a script executed before entrypoint container</h2>' > /usr/share/nginx/html/index.html

    exec /docker-entrypoint.sh nginx -g "daemon off;" # it's "ENTRYPOINT CMD" extracted from the main container image definition

One thing that is very important is the last line with exec. It executes the original entrypoint script and must match it exactly as it is defined in the Dockerfile. In this case it requires additional arguments that are defined in the CMD.

Now let’s define the Deployment object

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx-script
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - image: nginx:1.19
          name: nginx
          command: ["bash", "-c", "/scripts/prestart-script.sh"]
          ports:
            - containerPort: 80
              name: http
          volumeMounts:
            - mountPath: /scripts
              name: scripts
      volumes:
        - name: scripts
          configMap:
            name: scripts
            defaultMode: 0755 # <- this is important

That is pretty straightforward - we override the entrypoint with command and we also must make sure our script is mounted with proper permissions (thus defaultMode needs to be defined).
Comparison table

Containers are about reusability and often it’s much easier to make small adjustments without rebuilding the whole container image and take over the responsibility of publishing and maintaining it. It’s just an implementation of the KISS principle.

* Windows
- [[https://tensorworks.com.au/blog/bringing-full-gpu-support-to-windows-containers-in-kubernetes/][Bringing full GPU support to Windows containers in Kubernetes | TensorWorks]]
- [[https://github.com/MicrosoftDocs/SupportArticles-docs/blob/main/support/windows-server/containers/support-for-windows-containers-docker-on-premises-scenarios.md][SupportArticles-docs/support-for-windows-containers-docker-on-premises-scenarios.md at main · MicrosoftDocs/SupportArticles-docs]]
- [[https://thomasvanlaere.com/posts/2021/06/exploring-windows-containers/][Thomas Van Laere | Exploring Windows Containers]]
- [[https://samcogan.com/using-host-process-containers-to-interact-with-kubernetes-windows-nodes/][Using Host Process Containers to Interact with Kubernetes Windows Nodes - samcogan.com]]
** [[https://www.jamessturtevant.com/posts/Windows-Containers-on-Windows-10-without-Docker-using-Containerd/][Windows Containers on Windows 10 without Docker (using Containerd) · James Sturtevant]]
*** [[https://github.com/microsoft/Windows-Containers/issues/210][How to use containerd with crictl on Windows Server 2022 · Issue #210 · microsoft/Windows-Containers]]
All features:
: Add-WindowsFeature Containers,Hyper\-V,Hyper\-V\-Tools,Hyper\-V\-PowerShell \-Restart \-IncludeManagementTools

Minimum is:
: Add-WindowsFeature Containers -IncludeManagementTools

** Issues
- [[https://github.com/rancher/rke2/issues/3915][kubelet fails to pull docker.io/rancher/pause:3.6 on Windows 11 Pro node · Issue #3915 · rancher/rke2]]
  - [[https://github.com/containerd/containerd/pull/8137][Allow containerd on Windows 11 to use Windows Server 2022 images by hach-que · Pull Request #8137 · containerd/containerd]]

* ChatGPT

** How to tell kubernetes deployment not to run pod on specific node?

There are a few ways to tell Kubernetes not to run a pod on a specific node:

    Node Selector: You can use a node selector to specify which nodes the pod can run on. You can label the node you want to exclude, and then add a node selector to the pod specification that excludes that label. For example, to exclude a node with a label node-type=exclude, you can add the following to the pod spec:

python

nodeSelector:
  node-type: "not-exclude"

    Node Affinity: Node affinity allows you to specify rules for the scheduling of pods based on the labels of the nodes. You can use node affinity to exclude a node by setting a negative match expression. For example, to exclude a node with a label node-type=exclude, you can add the following to the pod spec:

yaml

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: node-type
              operator: NotIn
              values:
                - exclude

    Taints and tolerations: Taints are used to mark a node as unsuitable for certain pods. Tolerations are used to allow pods to be scheduled on nodes with specific taints. You can use taints and tolerations to exclude a node by tainting the node and adding a corresponding toleration to the pod. For example, to exclude a node with a taint of exclude=true:NoSchedule, you can add the following to the node:

java

kubectl taint nodes <node-name> exclude=true:NoSchedule

And then add the following to the pod spec:

vbnet

tolerations:
  - key: exclude
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"

These are just a few examples of how you can exclude a node from running a pod in Kubernetes. The approach you choose will depend on your specific use case and requirements.

** How much does Helm give overhead in Kubernetes?

Helm is a package manager for Kubernetes that helps you manage and deploy applications to your Kubernetes cluster. While Helm does add some overhead to your Kubernetes deployment, the benefits it provides can far outweigh the costs.

The amount of overhead that Helm adds to your Kubernetes deployment will depend on a variety of factors, including the complexity of your application, the size of your cluster, and the resources available to you.

Here are some potential areas where Helm might add overhead to your Kubernetes deployment:

    Resource usage: Helm uses additional resources in your cluster to manage the deployment of your applications. This includes additional Kubernetes objects such as releases, charts, and config maps. Depending on the size and complexity of your deployment, this could impact the performance of your cluster.

    Learning curve: If you're new to Helm, there is a learning curve to understanding how to use it effectively. This can require additional time and resources upfront to get up to speed.

    Maintenance: Helm requires ongoing maintenance to ensure that your deployments are up to date and secure. This includes updating Helm charts and releases, as well as monitoring and troubleshooting any issues that arise.

Despite these potential overheads, there are many benefits to using Helm in your Kubernetes deployment. Here are some potential benefits:

    Reusability: Helm charts can be easily shared and reused across multiple teams and projects, reducing the amount of duplication and increasing efficiency.

    Standardization: Helm provides a standard way of packaging and deploying applications to Kubernetes, making it easier to manage and maintain your deployments.

    Versioning: Helm allows you to version your deployments, making it easier to roll back to a previous version if needed.

    Security: Helm includes security features such as RBAC and TLS encryption to help secure your deployments.

Ultimately, whether or not Helm is worth the overhead for your Kubernetes deployment will depend on your specific use case and requirements. However, for many organizations, the benefits of using Helm outweigh the costs.

** How much objects Kubernetes cluster can handle?

The number of objects that a Kubernetes cluster can handle depends on several factors, including the size and configuration of the cluster, the types of objects being managed, and the resources available to the cluster.

In general, Kubernetes can handle a large number of objects. The Kubernetes API server, which is responsible for managing Kubernetes objects, is designed to scale horizontally, meaning that additional API servers can be added to handle additional load.

However, as the number of objects in a Kubernetes cluster grows, the performance of the cluster can be affected. This is because the Kubernetes API server must maintain an index of all the objects in the cluster, and this index can become slower to search and update as the number of objects grows.

To mitigate this performance impact, there are several best practices that can be followed:

    Limit the number of objects: To reduce the load on the Kubernetes API server, it is recommended to limit the number of objects in the cluster. This can be achieved by deleting unnecessary objects, consolidating objects where possible, and using efficient object designs.

    Optimize object design: When designing objects, it is important to consider the performance impact of each object. For example, using labels and selectors effectively can reduce the number of objects that need to be indexed.

    Scale the API server: If the number of objects in the cluster is expected to grow significantly, it may be necessary to add additional API server instances to handle the load.

    Use caching and indexing: To improve the performance of the Kubernetes API server, caching and indexing can be used. For example, a caching layer like memcached can be used to cache frequently accessed objects, and indexing can be used to speed up searches.

Ultimately, the maximum number of objects that a Kubernetes cluster can handle will depend on the specific configuration and resources available to the cluster, as well as the design and usage of the objects in the cluster.

** How to tell Kubernetes do not run pods on node unless specified.

To prevent Kubernetes from scheduling pods on specific nodes by default, you can add a taint to those nodes and then add a corresponding toleration to your pod specifications.

Here are the general steps to follow:

    Add a taint to the nodes that you want to restrict. A taint is a label that tells Kubernetes not to schedule pods on a node unless the pod specifies a matching toleration. For example, you can add a taint with the following command:

    php

kubectl taint nodes <node-name> key=value:taint-effect

Replace <node-name> with the name of the node that you want to restrict, key and value with a key-value pair to identify the taint, and taint-effect with NoSchedule, NoExecute, or PreferNoSchedule depending on the effect you want the taint to have. For example:

kubectl taint nodes node1 special=restricted:NoSchedule

This command adds a taint to the node1 node with the special=restricted key-value pair and the NoSchedule effect.

Add a toleration to your pod specifications. A toleration is a specification that tells Kubernetes to allow a pod to be scheduled on a node with a matching taint. To add a toleration to your pod specifications, add the following tolerations section to your pod specification:

vbnet

tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"

Replace key and value with the key-value pair that matches the taint you added to the node, and effect with the effect that you specified when adding the taint (NoSchedule, NoExecute, or PreferNoSchedule). For example:

vbnet

    tolerations:
    - key: "special"
      operator: "Equal"
      value: "restricted"
      effect: "NoSchedule"

    This tolerations section specifies that the pod can be scheduled on a node with a taint that has the special=restricted key-value pair and the NoSchedule effect.

By adding a taint and a corresponding toleration, you can prevent Kubernetes from scheduling pods on nodes unless the pods explicitly specify that they can tolerate the taint. This can be useful for reserving nodes for specific workloads or for testing purposes.

** How to configure Kubernetes deployment not to run same pod on the same host?

To configure a Kubernetes deployment to avoid running the same pod on the same host, you can use a pod anti-affinity rule. A pod anti-affinity rule specifies that pods should not be scheduled on the same node if they meet certain criteria, such as having the same label or belonging to the same replica set.

Here are the general steps to follow:

    Add a label to the pods that you want to avoid scheduling on the same node. For example, you can add a label with the following command:

    css

kubectl label pods <pod-name> key=value

Replace <pod-name> with the name of the pod that you want to label, and key and value with a key-value pair to identify the label. For example:

perl

kubectl label pods my-app-1234-5678 app=my-app

This command adds a label with the app=my-app key-value pair to the my-app-1234-5678 pod.

Add a pod anti-affinity rule to your deployment specifications. A pod anti-affinity rule specifies that pods should not be scheduled on the same node if they have a certain label. To add a pod anti-affinity rule to your deployment specifications, add the following podAntiAffinity section to your deployment specification:

yaml

    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - my-app
        topologyKey: "kubernetes.io/hostname"

    This podAntiAffinity section specifies that pods with the app=my-app label should not be scheduled on the same node. The topologyKey field specifies the level at which the rule applies; in this case, it applies at the node level (kubernetes.io/hostname).

By adding a pod anti-affinity rule, you can ensure that Kubernetes does not schedule multiple pods with the same label on the same node. This can be useful for ensuring high availability and avoiding resource contention.

** But it will block creating new pods with Pending status. How to avoid this?

If you want to avoid blocking the creation of new pods with a Pending status while also ensuring that no two pods with the same label are scheduled on the same node, you can use a combination of pod anti-affinity and pod disruption budgets.

Here are the general steps to follow:

    Add a label to the pods that you want to avoid scheduling on the same node, as described in the previous answer.

    Add a pod anti-affinity rule to your deployment specifications, as described in the previous answer.

    Add a pod disruption budget to your deployment specifications. A pod disruption budget specifies the maximum number of pods that can be unavailable at any given time. To add a pod disruption budget to your deployment specifications, add the following podDisruptionBudget section to your deployment specification:

    yaml

    podDisruptionBudget:
      selector:
        matchLabels:
          app: my-app
      maxUnavailable: 1

    This podDisruptionBudget section specifies that no more than one pod with the app=my-app label can be unavailable at any given time.

With this combination of pod anti-affinity and pod disruption budgets, Kubernetes will avoid scheduling multiple pods with the same label on the same node while also allowing new pods to be created even if other pods are in a Pending state. If a node becomes unavailable, Kubernetes will ensure that at least one pod with the app=my-app label is still available by either moving the pod to a different node or creating a new pod to replace it.

** Can Kustomize change object kind and how if it can?

No, Kustomize cannot change the kind of a Kubernetes object. The kind field is a fundamental part of the object's identity and is defined by the Kubernetes API, so it cannot be modified by Kustomize.

Kustomize is designed to modify the contents of existing Kubernetes object manifests, rather than changing their fundamental structure or identity. The tool provides a number of mechanisms for making targeted modifications to the fields, labels, annotations, and other attributes of objects in a declarative, repeatable way.

However, it is possible to achieve similar functionality to changing the kind of an object by using the transformers feature of Kustomize. Transformers can be used to modify the YAML structure of a Kubernetes object manifest, which could include modifying the kind field.

Here is an example of using a transformer to change the kind field of a Deployment object to a StatefulSet:

yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-container
          image: my-image
          ports:
            - containerPort: 80
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- deployment.yaml
transformers:
- deployment-to-statefulset.yaml

In this example, the deployment.yaml file defines a Deployment object with the name my-deployment. The kustomization.yaml file includes a transformers section that specifies a transformer file deployment-to-statefulset.yaml.

The deployment-to-statefulset.yaml file includes the following transformer:

yaml

apiVersion: builtin
kind: Filter
metadata:
  name: deployment-to-statefulset
spec:
  filters:
  - path: kind
    kind: EditTransformer
    version: v1
    options:
      value: StatefulSet

This transformer modifies the kind field of any object that matches its filter to StatefulSet. When you apply the Kustomization using the kubectl apply command, Kustomize will modify the kind field of the Deployment object to StatefulSet. Note that this is a non-standard and potentially risky operation, so it should be used with caution and only when absolutely necessary.

** I tried it, but got output `error: unable to load builtin Filter.builtin.[noGrp]`.

The error message unable to load builtin Filter.builtin.[noGrp] typically indicates that the version of Kustomize you are using is not compatible with the syntax of the Filter transformer in your deployment-to-statefulset.yaml file.

The Filter transformer was introduced in Kustomize version 3.2.0, so if you are using an earlier version of Kustomize, you may encounter this error. To fix the error, you can either upgrade your version of Kustomize to at least version 3.2.0, or you can modify the syntax of the transformer to be compatible with the version of Kustomize you are using.

If you are already using Kustomize 3.2.0 or later, it's possible that the Filter transformer is not installed or enabled in your installation of Kustomize. In this case, you can try reinstalling or updating Kustomize to ensure that the Filter transformer is available.

Alternatively, you can try using a different transformer that is compatible with the version of Kustomize you are using, such as the patch transformer or the kustomize transformer. These transformers provide similar functionality to the Filter transformer and should work with any version of Kustomize.
wigust@yahoo.com
Could you describe how to use `patch` suggested in the previous message?

Sure! The patch transformer in Kustomize can be used to modify specific fields of Kubernetes objects by applying JSON or YAML patches to their manifest files. Here's an example of how to use the patch transformer to change the kind field of a Deployment object to a StatefulSet:

    Create a Deployment manifest file, for example deployment.yaml, with the following contents:

    yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-container
          image: my-image
          ports:
            - containerPort: 80

Create a patch file, for example deployment-patch.yaml, that specifies the changes you want to make to the kind field:

yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  template:
    metadata:
      annotations:
        kustomize.k8s.io/patch: |
          - op: replace
            path: /kind
            value: StatefulSet

This patch file modifies the kind field of the my-deployment object to StatefulSet. Note that the kustomize.k8s.io/patch annotation is used to apply the patch to the object.

Create a kustomization.yaml file that specifies the resources and transformers to apply:

makefile

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- deployment.yaml
transformers:
- deployment-patch.yaml

This kustomization.yaml file specifies the deployment.yaml file as the input resource, and the deployment-patch.yaml file as the transformer to apply.

Run kubectl apply to apply the changes:

php

    kubectl apply -k <directory containing kustomization.yaml>

    This command applies the changes to the Kubernetes cluster. The deployment.yaml file is modified by the deployment-patch.yaml transformer, and the resulting manifest is deployed to the cluster.

That's it! The patch transformer can be used to modify any field of a Kubernetes object, so it's a powerful tool for customizing Kubernetes deployments. Note that the syntax of the patch file is based on JSON patches, so you'll need to be familiar with this format to use the patch transformer effectively.

** I apologize for the confusion. You are correct, the previous solution was not correct for converting a Deployment to a DaemonSet. Here is an updated solution:

    Create a kustomization.yaml file:

typescript

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
- deployment.yaml

patches:
- path: deployment.yaml
  kind: Deployment
  version: apps/v1
  patch: |-
    - op: remove
      path: /spec/replicas
    - op: add
      path: /spec/template/spec/nodeSelector
      value:
        node-type: worker
    - op: add
      path: /spec/template/spec/tolerations/-
      value:
        key: node-type
        operator: Equal
        value: worker
        effect: NoSchedule
    - op: add
      path: /spec/template/spec/hostNetwork
      value: true
    - op: add
      path: /spec/template/spec/dnsPolicy
      value: ClusterFirstWithHostNet
    - op: replace
      path: /spec/strategy/type
      value: Recreate
    - op: replace
      path: /kind
      value: DaemonSet

This Kustomization defines a Deployment resource and a patch that replaces the Deployment with a DaemonSet. The DaemonSet is scheduled to run one pod per node that has the label node-type=worker. The patch also removes the replicas field, as it is not needed for DaemonSet.

    Create a deployment.yaml file:

yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-container
        image: nginx
        ports:
        - containerPort: 80

    Run kubectl apply -k . to apply the Kustomization to the cluster.

This should create a DaemonSet with one pod per node labeled with node-type=worker. Each pod runs a single container with the nginx image.

** What is init container?

An init container is a special type of container that runs before the main container of a pod starts. Its primary purpose is to perform any initialization or pre-processing tasks that are required before the main container starts running. The init container can run a command or a script, and can also perform various other tasks such as installing software, downloading data, or configuring the environment.

Init containers can be used to handle a wide range of scenarios, such as:

    Waiting for a database or other service to become available before starting the main container
    Setting up a shared volume or configuration file that is required by multiple containers in the same pod
    Running security checks or configuration validation before starting the main container
    Initializing a database or other data store before it is accessed by the main container

Init containers are defined in the same YAML file as the main container, but in a separate container specification. They are run sequentially, with the next init container starting only after the previous one has completed successfully. If an init container fails, the pod is restarted until the init container succeeds.

** How to set /etc/hostname inside a pod?

You can set the hostname inside a pod by using an init container to modify the /etc/hostname file. Here's an example of how to do it:

    Create a ConfigMap that contains the hostname value you want to set:

yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: hostname-configmap
data:
  hostname: my-hostname

    Create a Pod with an init container that sets the hostname:

yaml

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  initContainers:
  - name: set-hostname
    image: busybox
    command: ["/bin/sh", "-c", "echo $(hostname) > /etc/hostname"]
    volumeMounts:
    - name: hostname-volume
      mountPath: /etc/hostname
  containers:
  - name: my-container
    image: my-image
    volumeMounts:
    - name: hostname-volume
      mountPath: /etc/hostname
    env:
    - name: MY_HOSTNAME
      valueFrom:
        configMapKeyRef:
          name: hostname-configmap
          key: hostname
  volumes:
  - name: hostname-volume
    configMap:
      name: hostname-configmap

In this example, the set-hostname init container runs the echo $(hostname) > /etc/hostname command to set the hostname to the value of the hostname of the container, which is passed to the MY_HOSTNAME environment variable of the main container using a ConfigMap. The volumeMounts and volumes sections ensure that the /etc/hostname file is mounted in both containers.

** What ports required for Kubelet?

Kubelet, the Kubernetes node agent, requires several ports to be open for communication with the Kubernetes control plane and other nodes in the cluster. Here are the default ports required by Kubelet:

    TCP port 10250: The Kubelet API server port. It is used for communication between the Kubelet and other Kubernetes components such as the kube-apiserver, kubelet's http endpoint, and kubelet's read-only port.

    TCP port 10255: The read-only port for the Kubelet's HTTP endpoint. It provides access to the health check, metrics, and log files for the Kubelet.

    TCP port 10256: The Kubelet's cadvisor port. It provides access to the container metrics for the Kubelet.

It's worth noting that these ports can be customized by passing command-line flags to the Kubelet or by setting configuration options in the Kubelet's configuration file. Also, some Kubernetes distributions may use different ports by default, so it's always a good idea to consult the documentation or vendor-specific guides for the specific distribution being used.

** How to deny specific query type, e.g. POST?

To deny a specific query type, such as POST, for an ingress resource in Kubernetes, you can use an Nginx configuration snippet that specifies the limit_except directive. Here is an example:

yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      limit_except GET HEAD {
        deny all;
      }
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /specific-path
            pathType: Prefix
            backend:
              service:
                name: example-service
                port:
                  name: http

In this example, the configuration-snippet annotation specifies a custom Nginx configuration snippet that uses the limit_except directive to deny all requests except for GET and HEAD for the specified location (in this case, /specific-path).

You can modify the example to deny other query types, such as PUT or DELETE, by adding them to the list inside the limit_except block:

yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      limit_except GET HEAD PUT DELETE {
        deny all;
      }
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /specific-path
            pathType: Prefix
            backend:
              service:
                name: example-service
                port:
                  name: http

In this modified example, the limit_except block specifies that requests for the /specific-path location must be either GET, HEAD, PUT, or DELETE. Any other type of request will be denied.

** What is CPU Requests Commitment?

CPU requests commitment is a concept in Kubernetes, which is a popular container orchestration platform used for deploying and managing containerized applications. In Kubernetes, each container that runs on a node can make resource requests, which specify the minimum amount of CPU and memory that the container requires to run properly.

CPU requests commitment refers to the amount of CPU resources that a Kubernetes node guarantees to a particular container over a certain period of time. When a container is created in Kubernetes, it is scheduled to run on a node that has sufficient available resources to meet the container's resource requests. Once the container is running, the node guarantees that the requested CPU resources will be available to the container for the duration of its runtime.

If a container exceeds its CPU requests commitment, Kubernetes may throttle the container's CPU usage or evict the container from the node altogether. This helps ensure that other containers on the node are not starved of resources and can operate properly.

Overall, CPU requests commitment is an important concept in Kubernetes for ensuring that containerized applications have the necessary resources to run smoothly, while also preventing resource contention and ensuring that other applications on the same node are not negatively affected.

** What is Memory Usage (WSS) in Kubernetes?

Memory usage (Working Set Size or WSS) in Kubernetes refers to the amount of memory currently being used by a container in a pod. It includes all the memory that a process has accessed recently and is therefore likely to need again in the near future.

In Kubernetes, the memory usage of a container is tracked by the kubelet, which periodically collects memory usage statistics from the container runtime and reports them to the Kubernetes API server. The Kubernetes API server can then use this information to make scheduling decisions and to enforce resource limits and requests.

Memory usage is an important metric for Kubernetes because it can affect the performance and stability of the entire cluster. If a container consumes too much memory, it can cause the node it's running on to become unstable, potentially leading to other pods on the same node being evicted or even causing the node to crash.

Therefore, monitoring and managing memory usage is an important part of optimizing the performance and stability of Kubernetes clusters. Kubernetes provides various tools for monitoring and managing resource usage, including resource quotas, resource limits, and container metrics exposed by kubelet.

** What does mean `Lost` status in PVC?

In Kubernetes, Lost status of a Persistent Volume Claim (PVC) indicates that Kubernetes has lost the connection with the underlying storage.

This can happen when the underlying storage is deleted or when the PVC's storage class is no longer available. When a PVC is in the Lost status, Kubernetes is no longer able to track the state of the underlying storage and the PVC cannot be used by any pods.

To fix this issue, you will need to determine the cause of the underlying storage becoming unavailable and address it accordingly. This may involve creating a new PVC, fixing the storage class or restoring the storage to its previous state. Once the underlying storage is available again, Kubernetes should be able to restore the PVC to its previous state.

** I need a serviceaccount which could modify nodes labels.

To allow a service account to modify node labels, you need to grant it the appropriate permissions using a ClusterRole and a ClusterRoleBinding. Here is an example of how to create a service account with the necessary permissions:

    First, create a file named node-labeler-clusterrole.yaml with the following contents:

yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: node-labeler
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch", "update"]

This ClusterRole grants permission to list and watch nodes, as well as get and update their labels.

    Next, create a file named node-labeler-clusterrolebinding.yaml with the following contents:

yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: node-labeler
subjects:
- kind: ServiceAccount
  name: node-labeler
  namespace: default
roleRef:
  kind: ClusterRole
  name: node-labeler
  apiGroup: rbac.authorization.k8s.io

This ClusterRoleBinding grants the node-labeler service account in the default namespace access to the node-labeler ClusterRole.

    Create a service account named node-labeler:

bash

kubectl create serviceaccount node-labeler

    Finally, apply the ClusterRole and ClusterRoleBinding to the cluster:

bash

kubectl apply -f node-labeler-clusterrole.yaml
kubectl apply -f node-labeler-clusterrolebinding.yaml

After following these steps, you can use the node-labeler service account to modify the labels of nodes in the cluster.

** How to expose statefulset as a service?

To expose a StatefulSet as a Service in Kubernetes, you can create a Headless Service. A Headless Service is a service that does not provide a stable IP address for accessing the pods. Instead, it returns the IP addresses of the individual pods directly.

Here's an example YAML definition of a Headless Service that exposes a StatefulSet:

yaml

apiVersion: v1
kind: Service
metadata:
  name: my-statefulset-service
  labels:
    app: my-statefulset
spec:
  clusterIP: None
  selector:
    app: my-statefulset
  ports:
    - name: http
      port: 80
      protocol: TCP

In the above example:

    The clusterIP field is set to None to create a Headless Service.
    The selector field selects the pods of the StatefulSet to expose.
    The ports field specifies the port to expose.

After applying this Service definition, you can access the individual pods of the StatefulSet directly using their DNS names, which will be in the following format:

typescript

$(podname).$(servicename).$(namespace).svc.cluster.local

Where:

    $(podname) is the name of the pod
    $(servicename) is the name of the Headless Service
    $(namespace) is the namespace of the StatefulSet and the Headless Service

For example, if your Headless Service is named my-statefulset-service in the default namespace, and the StatefulSet has two pods named my-statefulset-0 and my-statefulset-1, you can access them at the following DNS names:

arduino

my-statefulset-0.my-statefulset-service.default.svc.cluster.local
my-statefulset-1.my-statefulset-service.default.svc.cluster.local

You can use these DNS names to connect to the individual pods, for example, in your application code or in your other Kubernetes resources.

** How Kubernetes master decides that node is not available?

In Kubernetes, the master node determines whether a node is available or not by regularly sending heartbeats to nodes in the cluster. These heartbeats are known as "node status updates".

If a node fails to respond to multiple consecutive status update requests, the master node marks the node as "NotReady". The master then begins rescheduling any workloads that were running on the failed node onto other healthy nodes in the cluster.

Additionally, Kubernetes provides a number of mechanisms for detecting when a node becomes unavailable due to factors such as network failure or power outage. These mechanisms include node liveness probes, which periodically check whether an application running on a node is still responsive, and node readiness probes, which determine whether a node is ready to accept incoming traffic.

** Does a worker node check master is available in the same way?

Yes, worker nodes in a Kubernetes cluster also check whether the master node is available in a similar way. Worker nodes periodically send "NodeReady" status updates to the master node to indicate that they are still operational.

If the master node fails to respond to multiple consecutive NodeReady status update requests from a particular worker node, the worker node will mark the master node as "NotReady". However, this typically does not trigger any automatic actions since the master node is considered a critical component of the cluster infrastructure.

It's worth noting that while worker nodes can detect when the master node becomes unavailable, they do not have the ability to initiate any failover or recovery actions on their own. All decisions related to rescheduling workloads, scaling the cluster, and handling other types of failures are made by the master node.

** Is it safe to have Kubernetes nodes of a single cluster be in a different geographical places?

While it is technically possible to have Kubernetes nodes of a single cluster located in different geographical locations, it may not always be the best approach from a performance and reliability standpoint.

Having nodes located in different geographical locations can introduce additional network latency and increase the risk of communication failures. This can result in slower application response times and reduced availability of the cluster as a whole.

To mitigate these risks, you can implement measures such as using a highly available control plane with multiple replicas distributed across different data centers, configuring pod anti-affinity rules to ensure that related pods are not scheduled on nodes in different locations, and leveraging network topology aware scheduling to place pods closer to the users or services they interact with.

Ultimately, the decision to deploy Kubernetes nodes in different geographic locations should be based on factors such as the specific requirements of the application being deployed, the available infrastructure, and the overall goals and constraints of the organization.

** Does Kubernetes use mTLS for communication between the components?

Yes, Kubernetes uses mTLS (Mutual Transport Layer Security) to secure communication between its components. With mTLS, each component in the system has a unique certificate and private key that is used to encrypt and authenticate the traffic between them. This helps ensure that only authorized parties can access the data and services within the cluster. Additionally, Kubernetes also provides mechanisms for managing these certificates, such as rotating them periodically and revoking them if they are compromised.

** What mechanisms?

Kubernetes provides a few mechanisms for managing certificates used for mTLS:

    Certificate Authority (CA) - Kubernetes uses a root CA to sign and issue certificates for its components. This CA is typically managed by the cluster administrator, and is used to establish trust between the various components in the cluster.

    API Server Certificates - The API server is the central component of the Kubernetes control plane, and it uses SSL/TLS certificates to secure communication with the Kubernetes API. These certificates are also signed by the cluster's root CA.

    Node Certificates - Each node in the Kubernetes cluster has its own certificate and private key that is used for mTLS communication with the API server and other components. These certificates are also typically signed by the cluster's root CA.

    Service Account Tokens - Kubernetes uses service account tokens to authenticate requests between components within the cluster. These tokens are issued by the API server, and are signed by a keypair that is rotated periodically.

Overall, these mechanisms provide a secure foundation for communication within Kubernetes clusters, and help ensure that only authorized parties can access the data and services within the cluster.

** Does both mTLS participants should use same CA?

In general, both mTLS (mutual Transport Layer Security) participants do not necessarily need to use the same Certificate Authority (CA). Each participant can obtain their own certificate from a different CA, as long as they trust each other's CA. However, it is generally recommended for both participants to use certificates issued by the same trusted CA in order to ensure smooth and interoperable operation of the mTLS connection.

Using certificates from the same trusted CA helps to avoid potential issues that can arise when different CAs issue certificates with different policies or validation procedures. It can also simplify the management of the certificates, as there will be a single trusted root CA that needs to be configured on both sides.

Ultimately, the decision of whether to use the same CA or different CAs depends on the specific requirements of the mTLS deployment and the security policies in place.

** kube-controller-manager

Kube-controller-manager is one of the core components of Kubernetes, which runs on the control plane nodes and manages various controllers that regulate the state of the cluster. It is responsible for ensuring that the desired state of the cluster matches the actual state by monitoring and reconciling the state of various resources such as pods, services, endpoints, replicasets, and deployments.

The kube-controller-manager includes several controllers, such as the Replication Controller, Endpoints Controller, Namespace Controller, Service Account and Token Controllers, and many more. Each controller watches a specific set of resources and takes appropriate actions to maintain the desired state.

In addition to the built-in controllers, it is also possible to write custom controllers using the Kubernetes API that can manage custom resources or perform custom logic based on events in the cluster.

The kube-controller-manager is designed to be highly available, so multiple instances of the controller manager can be run in a cluster, each running on a different node. This ensures that if one node fails, another controller manager can take over its responsibilities and maintain the desired state of the cluster.

** ClusterFirstWithHostNet

ClusterFirstWithHostNet is a Kubernetes DNS policy that specifies how DNS queries should be resolved within a cluster. With this policy, DNS queries originating from containers in a pod are first sent to the cluster's DNS resolver, which then forwards the query to the node's DNS resolver if it cannot resolve the query itself. This allows containers to resolve DNS queries using both the cluster's DNS and the host's DNS.

The ClusterFirstWithHostNet policy is used when the pod's hostNetwork field is set to true. When this field is set, the pod shares the host's network namespace, which means it can access the host's DNS resolver.

It's important to note that using the ClusterFirstWithHostNet policy can pose security risks, as it allows pods to access the host's DNS resolver. Therefore, it should only be used when necessary and with caution
