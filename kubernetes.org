:PROPERTIES:
:ID:       169228af-10b2-4a7e-b9fa-0619733b14ed
:END:

* Alternatives
- [[https://github.com/aurae-runtime/aurae][aurae-runtime/aurae: Distributed systems runtime daemon written in Rust.]]

* Awesome
- [[https://kubernetes-csi.github.io/docs/drivers.html][Drivers - Kubernetes CSI Developer Documentation]]
- [[https://github.com/k8s-at-home/awesome-home-kubernetes][k8s-at-home/awesome-home-kubernetes: Awesome projects involving running Kubernetes at home]]
- [[https://github.com/Michaelpalacce/HomeLab][Michaelpalacce/HomeLab: My HomeLab environment]]
- [[https://github.com/ramitsurana/awesome-kubernetes][ramitsurana/awesome-kubernetes: A curated list for awesome kubernetes sources]]
- [[https://github.com/tomhuang12/awesome-k8s-resources][tomhuang12/awesome-k8s-resources: A curated list of awesome Kubernetes tools and resources.]]

* CDI
** EFS
#+begin_quote
EFS is dead slow and not performant at all. Stay away from EFS except for the
lightest of data storage needs.
#+end_quote

* cert-manager
- [[https://cert-manager.io/docs/troubleshooting/webhook/][The Definitive Debugging Guide for the cert-manager Webhook Pod - cert-manager Documentation]]

- [[https://github.com/cert-manager/cert-manager/issues/4831][Unable to apply Cert Manager v1.7.0 CRDs · Issue #4831 · cert-manager/cert-manager]]
- [[https://cert-manager.io/docs/release-notes/release-notes-1.7/#upgrading-with-server-side-apply][Release 1.7 - cert-manager Documentation]]
#+begin_src bash
  crds=("certificaterequests.cert-manager.io" "certificates.cert-manager.io" "challenges.acme.cert-manager.io" "clusterissuers.cert-manager.io" "issuers.cert-manager.io" "orders.acme.cert-manager.io")

  for crd in "${crds[@]}"; do
    manager_index="$(kubectl get crd "${crd}" --show-managed-fields --output json | jq -r '.metadata.managedFields | map(.manager == "cainjector") | index(true)')"
    kubectl patch crd "${crd}" --type=json -p="[{\"op\": \"remove\", \"path\": \"/metadata/managedFields/${manager_index}\"}]"
  done
#+end_src

* Cheat sheet

- [[https://github.com/eon01/kubectl-SheetCheat][eon01/kubectl-SheetCheat: The Definitive Kubectl Sheetcheat. ⭐ Give it a star if you like it. Work (always) in progress !]]

- [[https://gist.github.com/egeneralov/4309aec2994af7e025330509905e824e][chroot to host namespace]]
  #+begin_src bash
    #!/bin/sh -xe

    for NAME in $(kubectl get nodes -o name | awk -F \/ '{print $2}'); do
      cat << EOF | kubectl apply -f -
        apiVersion: v1
        kind: Pod
        metadata:
         name: kube-support-$NAME
         namespace: kube-system
        spec:
          terminationGracePeriodSeconds: 1
          containers:
            - name: debian
              image: debian:buster
              command: ["/bin/sleep"]
              args: ["infinity"]
              volumeMounts:
                - name: host
                  mountPath: /host
              securityContext:
                privileged: true
          volumes:
            - name: host
              hostPath:
                path: /
                type: Directory
          nodeSelector:
            kubernetes.io/hostname: $NAME
          tolerations:
            - operator: "Exists"
      EOF
    done
  #+end_src

32- [[https://learnk8s.io/a/a-visual-guide-on-troubleshooting-kubernetes-deployments/troubleshooting-kubernetes.en_en.v3.pdf][Visual guide on troubleshooting Kubernetes deployments]]

#+begin_example
  [root@vm30:~]# lsns | grep nginx
  4026532800 mnt         2  80051 root             nginx: master process nginx -g daemon off;
  4026532801 pid         2  80051 root             nginx: master process nginx -g daemon off;
  4026532802 cgroup      2  80051 root             nginx: master process nginx -g daemon off;
  4026532865 mnt        20  90081 101              /usr/bin/dumb-init -- /nginx-ingress-controller --publish-service=ingress-nginx/ingress-nginx-controller --election-id=ingress-controller-leader --controller-class=k8s.io/ingress-nginx --ingress-class=nginx --configmap=ingress-nginx/ingress-nginx-controller --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key
  4026532866 pid        20  90081 101              /usr/bin/dumb-init -- /nginx-ingress-controller --publish-service=ingress-nginx/ingress-nginx-controller --election-id=ingress-controller-leader --controller-class=k8s.io/ingress-nginx --ingress-class=nginx --configmap=ingress-nginx/ingress-nginx-controller --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key
  4026532867 cgroup     20  90081 101              /usr/bin/dumb-init -- /nginx-ingress-controller --publish-service=ingress-nginx/ingress-nginx-controller --election-id=ingress-controller-leader --controller-class=k8s.io/ingress-nginx --ingress-class=nginx --configmap=ingress-nginx/ingress-nginx-controller --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key

  [root@vm30:~]# nsenter -t 80051 -n curl localhost 
  <!DOCTYPE html>
  <html>
  ...
#+end_example

- clean state
  : rm -rf /var/lib/{kubernetes,cfssl,cni,containerd,etcd,kubelet}

- print everything
  : kubectl get all -A
  : kubectl get all --all-namespaces
  : kubectl get all,cm,secret,ing -A
  : kubectl api-resources --verbs=list
  : kubectl get events -A

- API
  : kubectl api-resources
  : kubectl api-versions

- Query specific API server
  : kubectl --server https://kube1.intr:6443 get pod

- cdi-uploadproxy
  : kubectl get service cdi-uploadproxy -n cdi

- show pods labels
  : kubectl get pods --show-labels

- get pod's nodes matching specific label
  : kubectl -l app.kubernetes.io/name=opensearch --output=custom-columns='NAME:spec.nodeName' --no-headers=true --namespace=opensearch get pods

- delete pods by labels
  : kubectl delete -n kube-system -l k8s-app=kube-dns pods

- delete failed pods
  : kubectl delete pods --field-selector status.phase=Failed --all-namespaces

- delete terminating ns
#+begin_example
  oleg@guixsd ~$ kubectl proxy
  Starting to serve on 127.0.0.1:8001

  NAMESPACE=cdi
  kubectl get namespace $NAMESPACE -o json |jq '.spec = {"finalizers":[]}' >temp.json
  curl -k -H "Content-Type: application/json" -X PUT --data-binary @temp.json 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE/finalize
#+end_example

- pvc
#+begin_example
  oleg@guixsd ~$ virtctl image-upload --pvc-name=guix-vm-pv --pvc-size=2Gi --image-path=/tmp/guix.qcow2 --uploadproxy-url=https://10.43.181.37 --insecure
  PVC default/guix-vm-pv not found
  PersistentVolumeClaim default/guix-vm-pv created
  Waiting for PVC guix-vm-pv upload pod to be ready...
  Pod now ready
  Uploading data to https://10.43.181.37
#+end_example

- get storage info
  : kubectl get --raw "/api/v1/nodes/kube7/proxy/stats/summary

- get specific node
  : kubectl get all --all-namespaces -o wide --field-selector spec.nodeName=kube7

- show node labels
  : kubectl get nodes --show-labels

- get context
  #+begin_example
    oleg@guixsd ~$ kubectl config get-contexts
    CURRENT   NAME           CLUSTER        AUTHINFO             NAMESPACE
    ,*         k8s-cluster0   k8s-cluster0   k8s-cluster0-admin   
  #+end_example

- change context
  #+begin_example
    kubectl config use-context k8s-cluster0
  #+end_example

- change namespace
  #+begin_example
    kubectl config set-context --current --namespace=<insert-namespace-name-here>
    # Validate it
    kubectl config view --minify | grep namespace:
  #+end_example

- run linter
  : docker run --network=host --rm -it -v $HOME/.kube:/root/.kube -e POPEYE_REPORT_DIR=/tmp/popeye -v /tmp/popeye:/tmp/popeye derailed/popeye --context k8s-cluster0-lb -n default --save --out html --output-file report.html

- mark master nodes as unschedulable
  : kubectl taint node kube3 unschedulable=true:NoSchedule

- list nodes NAME ARCH KERNEL KUBLET CPU RAM
  : kubectl get nodes -o custom-columns=NAME:.metadata.name,ARCH:.status.nodeInfo.architecture,KERNEL:.status.nodeInfo.kernelVersion,KUBLET:.status.nodeInfo.kubeletVersion,CPU:.status.capacity.cpu,RAM:.status.capacity.memory

- list nodes with taints
  : kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints --no-headers
  : kubectl get nodes -o=jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.taints}{"\n"}{end}'
  : kubectl get nodes -o go-template='{{range $item := .items}}{{with $nodename := $item.metadata.name}}{{range $taint := $item.spec.taints}}{{if and (eq $taint.key "node-role.kubernetes.io/master") (eq $taint.effect "NoSchedule")}}{{printf "%s\n" $nodename}}{{end}}{{end}}{{end}}{{end}}'
  : kubectl get nodes -o go-template='{{range $item := .items}}{{with $nodename := $item.metadata.name}}{{range $taint := $item.spec.taints}}{{if and (eq $taint.effect "NoSchedule")}}{{printf "%s\n" $nodename}}{{end}}{{end}}{{end}}{{end}}'

- list nodes which does not match selector
  : kubectl get node --selector='cilium.cluster.local/schedulable!=false'

- delete taint
  : kubectl taint node kube8 unschedulable-

- drain node
  : kubectl drain kube6 --ignore-daemonsets --delete-emptydir-data

- reboot node
  : ansible kubeX.intr -m reboot -a 'search_paths=/run/current-system/sw/bin'

- undrain node
  : kubectl uncordon kube6

- get documentation
  : kubectl explain --api-version=networking.k8s.io/v1 ingress.spec.rules.http.paths.backend

- dump all information
  : kubectl cluster-info dump

- list all objects in a namespace
  : parallel --color -j 4 kubectl get -o wide --show-kind --ignore-not-found -n NAMESPACE ::: $(kubectl api-resources --verbs=list --namespaced -o name)

- list pending pods
  : kubectl get pods --field-selector status.phase=Pending --all-namespaces -o wide

- list pods by creation time
  : kubectl get pods -A -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,NODE:.spec.nodeName,HOSTIP:.status.hostIP,PHASE:.status.phase,START_TIME:.metadata.creationTimestamp --sort-by=.metadata.creationTimestamp

- list images
  : kubectl get pod -o json -A | jq --raw-output '.items[].spec.containers[].image'

- run pod with networking utilities for debuging
  : kubectl run --rm -it netshoot --image=nicolaka/netshoot /bin/bash

- run pod on specific node with =kubectl run=
  : kubectl run mypod --image alpine --overrides='{"apiVersion": "v1", "spec": {"nodeSelector": { "kubernetes.io/hostname": "kube1" }}}' --command -- sleep 100000000000000
  : kubectl run --rm -it netshoot --image=nicolaka/netshoot --overrides='{"apiVersion": "v1", "spec": {"nodeSelector": { "kubernetes.io/hostname": "kube1" }}}' /bin/bash

- mariadb client
  : kubectl run --rm -it mysql-client --image=nixery.dev/shell/less/mycli -- /bin/bash

- run with specific multus network
  : kubectl run --rm -it netshoot --image=nicolaka/netshoot --overrides='{"apiVersion": "v1", "metadata": {"annotations": {"k8s.v1.cni.cncf.io/networks": "default/macvlan-br253"}}, "spec": {"nodeSelector": { "kubernetes.io/hostname": "kube6" }}}' /bin/bash

- iperf
  : kubectl run iperf --image nixery.dev/shell/tmux/iperf --overrides='{"apiVersion": "v1", "spec": {"nodeSelector": { "kubernetes.io/hostname": "kube6" }}}' --command -- sleep 100000000000000

- expose
  : kubectl expose --port=5201 --type=LoadBalancer pod iperf

- run nginx on specific node
  : kubectl run --overrides='{"apiVersion": "v1", "spec": {"nodeSelector": { "kubernetes.io/hostname": "kube6" }}}' --image nginx nginx

- run wrk
  : kubectl run --restart=Never -n example-hpa --rm -it wrk --image=nixery.dev/shell/wrk -- /bin/bash

- expose to specific address
  : kubectl expose --overrides='{"spec": {"loadBalancerIP": "78.108.82.13"}}' --port=80 --type=LoadBalancer pod nginx-76d6c9b8c-6hlfw

- restart coredns deployment
  : kubectl -n kube-system rollout restart deployment coredns

- show authentication privileges
  : kubectl auth can-i get pods

- compare nodes labels
  : diff -u <(kubectl get nodes kube7 -o json | jq --raw-output '.metadata.labels | keys[] | .' | sort -u) <(kubectl get nodes kube8 -o json | jq --raw-output '.metadata.labels | keys[] | .' | sort -u)

- Sort nodes by:
  - age
    : kubectl get nodes --sort-by=".metadata.creationTimestamp"
  - last transition time
    : kubectl get nodes --sort-by=".status.conditions[?(@.reason == 'KubeletReady' )].lastTransitionTime"
  - IP range
    : kubectl get nodes --sort-by=".status.addresses[?(@.type == 'InternalIP' )].address"

- continuous print logs all pods matching label
  : kubectl logs --max-log-requests=9 --namespace example-egress-gateway-kube11 -l app.kubernetes.io/name=curl --tail 10 -f

- [[https://www.ibm.com/docs/en/cloud-paks/cp-management/2.0.0?topic=kubectl-using-service-account-tokens-connect-api-server][Using service account tokens to connect with the API server - IBM Documentation]]
  : curl --cacert /run/secrets/kubernetes.io/serviceaccount/ca.crt -H "Authorization: Bearer $(cat /run/secrets/kubernetes.io/serviceaccount/token)" https://kubernetes.intr:6443/api/v1/pod/namespaces/jenkins

- KEDA metrics-server with verbose output
  : kubectl get --raw /apis/external.metrics.k8s.io/v1beta1 -v 6

** authentication

[[https://stackoverflow.com/questions/51612976/how-to-view-members-of-subject-with-group-kind][kubernetes - How to view members of subject with Group kind - Stack Overflow]]

#+begin_example
  $ kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.subjects[0].kind=="Group") | select(.subjects[0].name=="system:masters")'
  {
    "apiVersion": "rbac.authorization.k8s.io/v1",
    "kind": "ClusterRoleBinding",
    "metadata": {
      "annotations": {
        "rbac.authorization.kubernetes.io/autoupdate": "true"
      },
      "creationTimestamp": "2022-05-14T12:14:48Z",
      "labels": {
        "kubernetes.io/bootstrapping": "rbac-defaults"
      },
      "name": "cluster-admin",
      "resourceVersion": "201",
      "uid": "df5858b7-b5e4-4796-a167-70cab0fe26a0"
    },
    "roleRef": {
      "apiGroup": "rbac.authorization.k8s.io",
      "kind": "ClusterRole",
      "name": "cluster-admin"
    },
    "subjects": [
      {
        "apiGroup": "rbac.authorization.k8s.io",
        "kind": "Group",
        "name": "system:masters"
      }
    ]
  }
#+end_example

#+begin_example
  $ kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.subjects[0].kind=="Group") | select(.subjects[0].name=="system:masters") | .metadata.name'
  cluster-admin
#+end_example

#+begin_example
  $ kubectl get clusterrolebindings -o go-template='{{range .items}}{{range .subjects}}{{.kind}}-{{.name}} {{end}} {{" - "}} {{.metadata.name}} {{"\n"}}{{end}}' | grep "^Group-system:masters"
  Group-system:masters   -  cluster-admin
#+end_example

Have a read through the Kubernetes 'Authenticating' docs. Kubernetes does not
have an in-built mechanism for defining and controlling users (as distinct
from ServiceAccounts which are used to provide a cluster identity for Pods,
and therefore services running on them).

This means that Kubernetes does not therefore have any internal DB to
reference, to determine and display group membership.

In smaller clusters, x509 certificates are typically used to authenticate
users. The API server is configured to trust a CA for the purpose, and then
users are issued certificates signed by that CA. As you had noticed, if the
subject contains an 'Organisation' field, that is mapped to a Kubernetes
group. If you want a user to be a member of more than one group, then you
specify multiple 'O' fields. (As an aside, to my mind it would have made more
sense to use the 'OU' field, but that is not the case)

In answer to your question, it appears that in the case of a cluster where
users are authenticated by certificates, your only route is to have access to
the issued certs, and to check for the presence of the 'O' field in the
subject. I guess in more advanced cases, Kubernetes would be integrated with a
centralised tool such as AD, which could be queried natively for group
membership.

*** Anonymous access

[[https://kubernetes.io/docs/reference/access-authn-authz/rbac/][Using RBAC Authorization | Kubernetes]]

*** User roles
- [[https://kubernetes.io/docs/reference/access-authn-authz/rbac/#auto-reconciliation][Using RBAC Authorization | Kubernetes]]

*** Cheat sheet

: kubectl get rolebindings,clusterrolebindings --all-namespaces  -o custom-columns='KIND:kind,NAMESPACE:metadata.namespace,NAME:metadata.name,SERVICE_ACCOUNTS:subjects[?(@.kind=="ServiceAccount")].name'

#+begin_example
  $ kubectl get clusterroles system:discovery -o yaml
#+end_example

*** kubectl config generation

#+begin_example
  kubectl config --kubeconfig config set-cluster k8s --server=https://kubernetes.intr:6443 --certificate-authority=ssl/ca.pem --embed-certs
  kubectl config --kubeconfig config set-credentials view --client-key ssl/view-key.pem --client-certificate ssl/view.pem --embed-certs
  kubectl config --kubeconfig config set-context default --cluster=k8s --user=view --namespace default
  kubectl config --kubeconfig config use-context default
#+end_example

** override kubernetes
#+begin_src nix
  kubernetes-wrapper = 
    let
      # Make sure --secure-port flag does not provided to
      # kube-scheduler, because Prometheus Kubernetes Operator
      # cannot connect to kube-scheduler with mTLS.  The
      # kube-scheduler has self-signed certificate which has only
      # 127.0.0.1 and localhost as certificate hostnames.
      kube-scheduler-wrapper = writeScriptBin "kube-scheduler" ''
        #!${runtimeShell} -e
        new_args=()
        for var in "$@"
        do
            if [[ "$var" == *"--secure-port"* ]]
            then
                :
            else
                new_args+=($var)
            fi
        done
        exec ${pkgs.kubernetes}/bin/kube-scheduler "''${new_args[@]}"
      '';
    in
      pkgs.kubernetes.pause //
      pkgs.symlinkJoin {
        name = "kubernetes";
        paths = [ kube-scheduler-wrapper pkgs.kubernetes ];
      };

#+end_src

** Balance
#+begin_src markdown
  [amazon web services - Spread pods over cluster - DevOps Stack Exchange](https://devops.stackexchange.com/questions/12246/spread-pods-over-cluster)

  > Take a look at the [Descheduler](https://github.com/kubernetes-sigs/descheduler). This project runs as a Kubernetes Job that aims at killing pods when it thinks the cluster is unbalanced.
  > 
  > The [`LowNodeUtilization`](https://github.com/kubernetes-sigs/descheduler#lownodeutilization) strategy seems to fit your case:
  > 
  > > This strategy finds nodes that are under utilized and evicts pods, if possible, from other nodes in the hope that recreation of evicted pods will be scheduled on these underutilized nodes.
  > 
  > * * *
  > 
  > Another option is to apply a little of chaos engineering manually, forcing a Rolling Update on your deployment, and hopefully, the scheduler will fix the balance problem when pods are recreated.
  > 
  > You can use the `kubectl rollout restart my-deployment`. It's way better than simply deleting the pods with `kubectl delete pod`, as the rollout will ensure availability during the "rebalancing" (although deleting the pods altogether increases your chances for a better rebalance).
#+end_src

** CronJob on every Kubernetes node
#+begin_src yaml
  apiVersion: batch/v1beta1
  kind: CronJob
  metadata:
    name: mycronjob
    labels:
      jobgroup: parallel
  spec:
    schedule: "*/5 * * * *"
    successfulJobsHistoryLimit: 0
    failedJobsHistoryLimit: 0
    jobTemplate:
      spec:
        template:
          metadata:
            name: kubejob
            labels:
              jobgroup: parallel
          spec:
            topologySpreadConstraints:
              - maxSkew: 2
                topologyKey: kubernetes.io/hostname
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchLabels:
                    jobgroup: parallel
            containers:
            - name: mycron-container
              image: alpine
              imagePullPolicy: IfNotPresent
              command: ['sh', '-c', 'echo Job Pod is Running ; sleep 10']
            restartPolicy: OnFailure
            terminationGracePeriodSeconds: 0
        parallelism: 5
    concurrencyPolicy: Allow
#+end_src

** Prometheus
- [[https://prometheus.kubernetes.corp1.majordomo.ru/graph?g0.expr=topk(10%2C%20sum%20by%20(container)%20(kube_pod_container_resource_limits%7Bresource%3D%22cpu%22%7D))&g0.tab=1&g0.stacked=0&g0.show_exemplars=0&g0.range_input=1h&g1.expr=topk(15%2C%20node_namespace_pod_container%3Acontainer_cpu_usage_seconds_total%3Asum_irate%7B%7D)&g1.tab=1&g1.stacked=0&g1.show_exemplars=0&g1.range_input=2w&g2.expr=topk(15%2C%20sum(container_memory_working_set_bytes%7Bjob%3D%22kubelet%22%2C%20metrics_path%3D%22%2Fmetrics%2Fcadvisor%22%2C%20container!%3D%22%22%2C%20image!%3D%22%22%7D)%20by%20(container))&g2.tab=1&g2.stacked=0&g2.show_exemplars=0&g2.range_input=1h][Prometheus Top pods]]

** [[https://github.com/kubernetes/kubernetes/issues/111699#issuecomment-1291640790][Memory]]
Are you able to share svg's from pprof showing the heaps and allocs? Profiling for kube-apiserver should be enabled by default.

They can be collected as follows.

Before starting, you need Go and =graphviz= installed to use =go tool pprof= .

Create a loopback context. This hack ensures we will be able to get to the kube-apiserver we desire.

#+begin_src bash
  kubectl config get-contexts
  kubectl config set-cluster local-debugging --server=https://localhost:6443 --insecure-skip-tls-verify=true
  kubectl config set-context local-debugging --cluster=local-debugging --user=PUT_AUTHINFO_USER_HERE
#+end_src

Start off in your main context. Identify the kube-apiserver pod you want to connect to. Edit the last line to ensure you connect to the right pod name and port.

#+begin_src bash
  kubectl config use-context REAL_CONTEXT_HERE
  kubectl -n kube-system get pods -l k8s-app=kube-apiserver
  kubectl -n kube-system port-forward pod/kube-apiserver-ip-100-123-321-33.us-east-5.compute.internal 6443:443
#+end_src

With the port-forward running, switch contexts to the debugging context and start a proxy.

#+begin_src bash
  kubectl config use-context local-debugging
  kubectl proxy
#+end_src

These commands will host a web based explorer for you to find details in your browser.

#+begin_src bash
  go tool pprof -http=:6061 http://localhost:8001/debug/pprof/heap
  go tool pprof -http=:6061 http://localhost:8001/debug/pprof/allocs
#+end_src

This will generate svgs that are easy to use in chrome and easy to share with other people.

#+begin_src bash
  go tool pprof -svg http://localhost:8001/debug/pprof/heap
  mv profile001.svg heap.svg

  go tool pprof -svg http://localhost:8001/debug/pprof/allocs
  mv profile001.svg allocs.svg
#+end_src

If you just want to view the svgs in your browser.

#+begin_src bash
  go tool pprof -web http://localhost:8001/debug/pprof/heap
  go tool pprof -web http://localhost:8001/debug/pprof/allocs
#+end_src

See =go tool pprof --help=  for more details. There's a lot of stuff it does.
* CNI
- [[https://github.com/platform9/luigi][platform9/luigi: The plumber you'll hire to install all your Kubernetes network plumbing]]

* Community
- [[https://kubernetes.slack.com/][kubernetes.slack.com]]
  - [[https://slack.kubernetes.io/][slack.kubernetes.io]]
- [[https://discuss.kubernetes.io/]]
- [[https://npwg-team.slack.com/][Multus Slack Invite]]

* Distribution
- [[https://github.com/gardener/gardener][gardener/gardener: Kubernetes-native system managing the full lifecycle of conformant Kubernetes clusters as a service on Alicloud, AWS, Azure, GCP, OpenStack, vSphere, KubeVirt, Hetzner, EquinixMetal, MetalStack, and OnMetal with minimal TCO.]]
- [[https://github.com/labring/sealos][labring/sealos: Sealos is a Kubernetes distribution, a Cloud Operating System designed for managing cloud-native applications. Demo: https://cloud.sealos.io]]

* Helm
- [[https://helm.sh/docs/topics/charts/][Helm Documentation | Charts]]
- [[https://github.com/truecharts/charts][truecharts/charts: Community App Catalog for TrueNAS SCALE]]
- [[https://github.com/twuni/docker-registry.helm][twuni/docker-registry.helm: Helm chart for a Docker registry. Successor to stable/docker-registry chart.]]
** Tools
- [[https://github.com/EladLeev/kubeseal-convert][EladLeev/kubeseal-convert: A tool for importing secrets from a pre-existing secrets management systems (e.g. Vault, Secrets Manager) into a SealedSecret]]
- [[https://github.com/etcd-carry/etcd-carry][etcd-carry/etcd-carry: etcd-carry provides the ability to synchronize resources in the K8s cluster that meet custom rules to the standby k8s cluster in real time.]]
- [[https://github.com/kvesta/vesta][kvesta/vesta: A static analysis of vulnerabilities, Docker and Kubernetes cluster configuration detect toolkit]]
- [[https://github.com/jatalocks/kube-reqsizer][jatalocks/kube-reqsizer: A Kubernetes controller for automatically optimizing pod requests based on their continuous usage. VPA alternative that can work with HPA.]]
- [[https://github.com/databus23/schelm][databus23/schelm: Render a helm manifest to a directory]]
** Cheat sheet
: helm create mychart

* How it works

First of all you should have a working etcd.

#+begin_example
  [root@kube1:~]# ss -tulpn
  Netid          State           Recv-Q          Send-Q                   Local Address:Port                     Peer Address:Port          Process
  tcp            LISTEN          0               4096                         127.0.0.1:38421                         0.0.0.0:*              users:(("containerd",=885,fd=14))
  tcp            LISTEN          0               128                       172.16.103.9:22                            0.0.0.0:*              users:(("sshd",pid=911,fd=3))
  tcp            LISTEN          0               4096                      172.16.103.9:443                           0.0.0.0:*              users:(("kube-apiserver",pid=3104049,fd=7))
  tcp            LISTEN          0               4096                      172.16.103.9:2379                          0.0.0.0:*              users:(("etcd",pid=3104054,fd=9))
  tcp            LISTEN          0               4096                      172.16.103.9:2380                          0.0.0.0:*              users:(("etcd",pid=3104054,fd=8))
#+end_example

After that you should start kube-apiserver, which allows to use kubectl
utility, which connects you to the cluster.

#+begin_example
  [root@kube1:~]# kubectl --insecure-skip-tls-verify get -A all
  NAMESPACE   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
  default     service/kubernetes   ClusterIP   10.0.0.1     <none>        443/TCP   10h
#+end_example

=kubelet= will start itself and =containerd= service

starting the following units: systemd-modules-load.service, systemd-sysctl.service
the following new units were started: containerd.service, kubelet.service

* How-to
- [[https://habr.com/ru/post/569124/][Cам себе облако: установка ELK и TICK стеков в Kubernetes / Хабр]]

* Ingress

- install
  : kubectl create namespace ingress-nginx
  : helm install -n ingress-nginx ingress-nginx ingress-nginx/ingress-nginx
* Installation
- [[https://github.com/adieu/nixos-k8s-flake][adieu/nixos-k8s-flake: Kubernetes Flake for NixOS with full control]]
- [[https://github.com/alexellis/k8s-on-raspbian][alexellis/k8s-on-raspbian: Kubernetes on Raspbian (Raspberry Pi)]]
- [[https://github.com/banzaicloud/pke][banzaicloud/pke: PKE is an extremely simple CNCF certified Kubernetes installer and distribution, designed to work on any cloud, VM or bare metal.]]
- [[https://github.com/bbigras/k3s-on-fly.io][bbigras/k3s-on-fly.io]]
- [[https://github.com/c3os-io/c3os][c3os-io/c3os: Linux Immutable OS for Automated (Decentralized) Kubernetes clusters with k3s]]
- [[https://github.com/canonical/microk8s][canonical/microk8s: MicroK8s is a small, fast, single-package Kubernetes for developers, IoT and edge.]]
- [[https://github.com/containerd/containerd/tree/main/contrib/ansible][containerd/contrib/ansible at main · containerd/containerd]]
- [[https://github.com/devtron-labs/devtron][devtron-labs/devtron: Tool integration platform for Kubernetes]]
- [[https://github.com/edgelesssys/constellation][edgelesssys/constellation: Constellation is the first Confidential Kubernetes. Constellation shields entire Kubernetes clusters from the (cloud) infrastructure using confidential computing.]]
- [[https://kubernetes.io/docs/setup/][Getting started | Kubernetes]]
  - [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/][Creating a cluster with kubeadm | Kubernetes]]
  - [[https://github.com/vmware-tanzu/sonobuoy][vmware-tanzu/sonobuoy: Sonobuoy is a diagnostic tool that makes it easier to understand the state of a Kubernetes cluster by running a set of Kubernetes conformance tests and other plugins in an accessible and non-destructive manner.]]
- [[https://github.com/giantswarm/mayu][giantswarm/mayu: Mayu helps you to provision a cluster of bare metal machines with CoreOS Container Linux]]
- [[https://github.com/justinas/nixos-ha-kubernetes][justinas/nixos-ha-kubernetes: Toy highly-available Kubernetes cluster on NixOS]]
- [[https://github.com/k0sproject/k0s][k0sproject/k0s: k0s - The Zero Friction Kubernetes by Team Lens]]
- [[https://github.com/karmada-io/karmada][karmada-io/karmada: Open, Multi-Cloud, Multi-Cluster Kubernetes Orchestration]]
- [[https://github.com/kelseyhightower/standalone-kubelet-tutorial][kelseyhightower/standalone-kubelet-tutorial: Standalone Kubelet Tutorial]]
- [[https://kubernetes.io/docs/reference/setup-tools/kubeadm/][Kubeadm | Kubernetes]]
  - [[https://github.com/jpetazzo/ampernetacle][jpetazzo/ampernetacle deploy a Kubernetes cluster on Oracle Cloud Inafrastructure]]
- [[https://github.com/kubealex/libvirt-k8s-provisioner][kubealex/libvirt-k8s-provisioner: Automate your k8s installation]]
- [[https://collabnix.com/3-node-kubernetes-cluster-on-bare-metal-system-in-5-minutes/][Kubernetes Cluster on Bare Metal System Made Possible using MetalLB – Collabnix]]
- [[https://blog.alexellis.io/kubernetes-in-10-minutes/][Kubernetes on bare-metal in 10 minutes]]
- [[https://jamesdefabia.github.io/docs/getting-started-guides/docker/][Kubernetes - Running Kubernetes Locally via Docker]]
- [[https://github.com/kubesphere/kubesphere][kubesphere/kubesphere: The container platform tailored for Kubernetes multi-cloud, datacenter, and edge management ⎈ 🖥 ☁️]]
- [[https://github.com/kvaps/kubefarm][kvaps/kubefarm: Automated Kubernetes deployment and the PXE-bootable servers farm]]
- [[https://github.com/kvaps/kubernetes-in-kubernetes][kvaps/kubernetes-in-kubernetes: Deploy Kubernetes in Kubernetes using Helm]]
- [[https://github.com/miekg/vks][miekg/vks: Virtual Kubelet Provider for Systemd]]
- [[https://github.com/MusicDin/kubitect][MusicDin/kubitect: Kubitect provides a simple way to set up a highly available Kubernetes cluster across multiple hosts.]]
- [[https://github.com/NetBook-ai/spawner][NetBook-ai/spawner: Mulit-Cloud infrastructure orchestrator for kubernetes first development.]]
- [[https://github.com/nocalhost/nocalhost][nocalhost/nocalhost: Nocalhost is Cloud Native Dev Environment.]]
- [[https://www.cloudbees.com/blog/one-command-to-kubernetes-with-docker-compose][One Command to Kubernetes with Docker Compose | Cloudbees Blog]]
- [[https://github.com/poseidon/typhoon][poseidon/typhoon: Minimal and free Kubernetes distribution with Terraform]]
- [[https://github.com/rancher/k3os][rancher/k3os: Purpose-built OS for Kubernetes, fully managed by Kubernetes.]]
- [[https://github.com/techno-tim/k3s-ansible][techno-tim/k3s-ansible: A fully automated HA k3s etcd install with kube-vip, MetalLB, and more]]
- [[https://balaskas.gr/blog/2022/08/31/creating-a-kubernetes-cluster-with-kubeadm-on-ubuntu-2204-lts/][Ευάγγελος Μπαλάσκας - Creating a kubernetes cluster with kubeadm on Ubuntu 22.04 LTS » Evaggelos Balaskas - System Engineer]]
** Installation on Debian

#+begin_example
  sudo apt-get update

  sudo apt-get install -y apt-transport-https ca-certificates curl gpg

  # If the folder `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
  # sudo mkdir -p -m 755 /etc/apt/keyrings
  curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

  # This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
  echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

  sudo apt-get update
  sudo apt-get install -y kubeadm kubelet kubectl
#+end_example

** Rancher

- [[https://rancher.com/quick-start][Getting Started with Kubernetes | Rancher Quick Start]]

#+begin_example
  01 Prepare a Linux Host
  Prepare a Linux host with supported version of Docker on the host.

  02 Start the server
  To install and run Rancher, execute the following Docker command on your host:

  $ sudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher

  To access the Rancher server UI, open a browser and go to the hostname or
  address where the container was installed. You will be guided through setting
  up your first cluster.
#+end_example

* Issues

** "Pod sandbox changed, it will be killed and re-created."
Make sure /etc/containerd/config.toml does not differ from other nodes, which
do not have this issue.

** 
*** [[https://stackoverflow.com/questions/66252195/kubernetes-pod-wont-start-1-nodes-had-a-volume-affinity-conflict][Question]]

I have a pod that won't start with a volume affinity conflict. This is a
bare-metal cluster so it's unrelated to regions. The pod has 4 persistent
volume claims which are all reporting bound so I'm assuming it's not one of
those. There are 4 nodes, one of them is tainted so that the pod will not
start on it, one of them is tainted specifically so that the pod WILL start on
it. That's the only affinity I have set up to my knowledge. The message looks
like this:

0/4 nodes are available: 1 node(s) had taint {XXXXXXX}, 
that the pod didn't tolerate, 1 node(s) had volume node 
affinity conflict, 2 Insufficient cpu, 2 Insufficient memory.

This is what I would have expected apart from the volume affinity
conflict. There are no other affinities set other than to point it at this
node. I'm really not sure why it's doing this or where to even begin. The
message isn't super helpful. It does NOT say which node or which volume there
is a problem with. The one thing I don't really understand is how binding
works. One of the PVC's is mapped to a PV on another node however it is
reporting as bound so I'm not completely certain if that's the problem. I am
using local-storage as the storage class. I'm wondering if that's the problem
but I'm fairly new to Kubernetes and I'm not sure where to look.

*** Answer

You got 4 Nodes but none of them are available for scheduling due to a
different set of conditions. Note that each Node can be affected by multiple
issues and so the numbers can add up to more than what you have on total
nodes. Let's try to address these issues one by one:

    Insufficient memory: Execute kubectl describe node <node-name> to check
    how much free memory is available there. Check the requests and limits of
    your pods. Note that Kubernetes will block the full amount of memory a pod
    requests regardless how much this pod uses.

    Insufficient cpu: Analogical as above.

    node(s) had volume node affinity conflict: Check out if the nodeAffinity
    of your PersistentVolume (kubectl describe pv) matches the node label
    (kubectl get nodes). Check if the nodeSelector in your pod also
    matches. Make sure you set up the Affinity and/or AntiAffinity rules
    correctly. More details on that can be found here.

    node(s) had taint {XXXXXXX}, that the pod didn't tolerate: You can use
    kubectl describe node to check taints and kubectl taint nodes <node-name>
    <taint-name>- in order to remove them. Check the Taints and Tolerations
    for more details.

* Katacoda Kubernetes

Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.

More details can be found at https://github.com/kubernetes/minikube

** Step 1 - Start Minikube
Minikube has been installed and configured in the environment. Check that it is properly installed, by running the minikube version command:

minikube version

Start the cluster, by running the minikube start command:

minikube start --wait=false

Great! You now have a running Kubernetes cluster in your online terminal. Minikube started a virtual machine for you, and a Kubernetes cluster is now running in that VM.

** Step 2 - Cluster Info
The cluster can be interacted with using the kubectl CLI. This is the main approach used for managing Kubernetes and the applications running on top of the cluster.

Details of the cluster and its health status can be discovered via kubectl cluster-info

To view the nodes in the cluster using kubectl get nodes

If the node is marked as NotReady then it is still starting the components.

This command shows all nodes that can be used to host our applications. Now we have only one node, and we can see that it’s status is ready (it is ready to accept applications for deployment).

** Step 3 - Deploy Containers
With a running Kubernetes cluster, containers can now be deployed.

Using kubectl run, it allows containers to be deployed onto the cluster - kubectl create deployment first-deployment --image=katacoda/docker-http-server

The status of the deployment can be discovered via the running Pods - kubectl get pods

Once the container is running it can be exposed via different networking options, depending on requirements. One possible solution is NodePort, that provides a dynamic port to a container.

kubectl expose deployment first-deployment --port=80 --type=NodePort

The command below finds the allocated port and executes a HTTP request.

export PORT=$(kubectl get svc first-deployment -o go-template='{{range.spec.ports}}{{if .nodePort}}{{.nodePort}}{{"\n"}}{{end}}{{end}}')
echo "Accessing host01:$PORT"
curl host01:$PORT

The result is the container that processed the request.

** Step 4 - Dashboard
Enable the dashboard using Minikube with the command minikube addons enable dashboard

Make the Kubernetes Dashboard available by deploying the following YAML definition. This should only be used on Katacoda.

kubectl apply -f /opt/kubernetes-dashboard.yaml

The Kubernetes dashboard allows you to view your applications in a UI. In this deployment, the dashboard has been made available on port 30000 but may take a while to start.

To see the progress of the Dashboard starting, watch the Pods within the kube-system namespace using kubectl get pods -n kubernetes-dashboard -w

Once running, the URL to the dashboard is https://2886795308-30000-simba08.environments.katacoda.com/

* kubespray
- [[https://github.com/pulumi/kubespy][pulumi/kubespy: Tools for observing Kubernetes resources in real time, powered by Pulumi.]]

#+begin_src bash
  kubectl -n backup delete job/kubespray

  cat << EOF | kubectl apply -f -
  apiVersion: batch/v1
  kind: Job
  metadata:
    labels:
      app: kubespray
    name: kubespray
    namespace: backup
  spec:
    backoffLimit: 1
    completions: 1
    parallelism: 1
    template:
      metadata:
        labels:
          app: kubespray
      spec:
        containers:
        - command:
          - /usr/bin/ansible-playbook
          - -i
          - /tmp/project-kubespray/inventory/project/inventory.ini
          - cluster.yml
          image: registry.gitlab.com/egeneralov/aio-bundle:a06acba5
          name: kubespray
          volumeMounts:
          - mountPath: /root/.ssh/id_rsa
            name: kubespray
            subPath: id_rsa
          - mountPath: /root/.ssh/config
            name: kubespray
            subPath: config
          - mountPath: /tmp/project-kubespray
            name: kubespray-files
          workingDir: /tmp/project-kubespray
        restartPolicy: Never
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 384
            items:
            - key: id_rsa
              path: id_rsa
            - key: id_rsa.pub
              path: id_rsa.pub
            - key: config
              path: config
            name: backup
          name: kubespray
        - hostPath:
            path: /tmp/project-kubespray
            type: DirectoryOrCreate
          name: kubespray-files
  EOF
#+end_src
* kubevirt

#+begin_example
  oleg@guixsd ~$ kubectl get pods -n kubevirt
  NAME                               READY   STATUS    RESTARTS        AGE
  virt-api-8986f8d94-m5zr4           1/1     Running   9 (99m ago)     6h56m
  virt-handler-lvzs4                 1/1     Running   28 (101m ago)   2d10h
  virt-controller-5599f57bf4-rjsr9   1/1     Running   9 (101m ago)    6h56m
  virt-controller-5599f57bf4-5qljd   1/1     Running   9 (101m ago)    6h56m
  virt-api-8986f8d94-gww4h           1/1     Running   9 (99m ago)     6h56m
  virt-operator-5df564dc7c-258xk     1/1     Running   9 (101m ago)    6h56m
  virt-operator-5df564dc7c-q7mj2     1/1     Running   9 (101m ago)    6h56m
#+end_example

* kubelet
** [[https://github.com/kubernetes/kubernetes/issues/42125][No way to configure ExternalIP addresses for a node with kubelet · Issue #42125 · kubernetes/kubernetes]]
#+begin_example
  If someone is still looking for a way to define ExternalIP in environment where you don't use any cloud provider, you can set ~--cloud-provider=external~  in Kubelets and update it with a PATCH request, the same way how the cloud providers does it.

  ,#+begin_src bash
  # (Add token or certificates, depending on how you authenticate to Kube)

  curl -k -v -XPATCH \
    -H "Accept: application/json" \
    -H "Content-Type: application/json-patch+json" \
    https://<YOUR KUBE ADDRESS>/api/v1/nodes/<YOUR NODE NAME>/status \
    --data '[{"op":"add","path":"/status/addresses/-", "value": {"type": "ExternalIP", "address": "<ADDRESS HERE>"} }]' 

  # Remove the uninitialized taint what external cloud provider set
  kubectl taint node <YOUR NODE NAME> node.cloudprovider.kubernetes.io/uninitialized-
  ,#+end_src

  +edit: Actually above didn't work after removed the =--cloud-provider=  flag because then kubelet start to overwrite the addresses. Still looking a way to not use any cloud-provider (we're on bare-metal but configure the ExternalIP.+

  edit2: ~--cloud-provider=external~  disables the kubelet to overwrite the address information
#+end_example

* kustomize

#+begin_src bash
  for image in $(for commit in $(git branches-by-date-origin | awk '/apps-cluster1-hms-development/ { print $1 }'); do PAGER=cat git show "$commit" | grep -A 1 '^+-' | awk '{ print $NF }' | xargs echo; done | sed 's/\s/:/' | xargs echo); do kustomize edit set image "$image"; done
#+end_src

#+begin_src bash
  for image in $(for commit in $(git branches-by-date-origin | awk '/apps-cluster1-hms-development/ { print $1 }'); do PAGER=cat git show "$commit" | grep -A 1 '^-  newTag' | awk '{ print $NF }' | xargs echo; done | sed 's/\s/:/' | xargs echo); do eval "$(echo "sed -i 's:${image}:'" kustomization.yaml)"; done
#+end_src

* Learning
- [[https://www.youtube.com/watch?v=yaJnT6DNHHc][(1) Single Sign-On for Kubernetes - Joel Speed, Pusher - YouTube]]
  - [[https://github.com/oauth2-proxy/oauth2-proxy][oauth2-proxy/oauth2-proxy: A reverse proxy that provides authentication with Google, Azure, OpenID Connect and many more identity providers.]]
  - [[https://dexidp.io/][Dex]]
  - [[https://github.com/pusher/k8s-auth-example][pusher/k8s-auth-example: Example Kubernetes Authentication helper. Performs OIDC login and configures Kubectl appropriately.]]
- [[https://www.youtube.com/watch?v=n4zxKk2an3U][(28) Learn Kubernetes with this Introductory Course in One Hour (All Core Components and Hands On!) - YouTube]]
- [[https://www.youtube.com/playlist?list=PLxNYxgaZ8Rscf-XJ5VfXgbDAk1vL4xaMl][(2) Learn Kubernetes with Google - YouTube]]
- [[https://www.youtube.com/watch?v=lkXLsD6-4jA&list=LL&index=1][(3) Kubernetes kube-proxy Modes: iptables and ipvs, Deep Dive - YouTube]]
- [[https://www.youtube.com/watch?v=AaJUKPiLp7c][(51) How to Use SSO for Kubernetes Authentication with Dex | Mirantis Labs - Tech Talks - YouTube]]
- [[https://www.youtube.com/watch?v=xhva6DeKqVU][(7) How load balancing and service discovery works in Kubernetes - YouTube]]
- [[https://www.youtube.com/watch?v=A-tVEoYqQsk][(7) What I Learnt Fixing 50+ Broken Kubernetes Clusters — David Flanagan 2.1.4 - YouTube]]
- [[https://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/][Accessing Kubernetes Pods from Outside of the Cluster - Ales Nosek - The Software Practitioner]]
- [[https://suraj.io/post/add-new-k8s-node-bootstrap-token/][Add new Node to k8s cluster with Bootstrap token - Suraj Deshmukh]]
- [[https://github.com/ahmetb/kubernetes-network-policy-recipes][ahmetb/kubernetes-network-policy-recipes: Example recipes for Kubernetes Network Policies that you can just copy paste]]
- [[https://opensource.com/article/19/6/introduction-kubernetes-secrets-and-configmaps][An Introduction to Kubernetes Secrets and ConfigMaps | Opensource.com]]
- [[https://unofficial-kubernetes.readthedocs.io/en/latest/admin/authorization/][Authorization - Unofficial Kubernetes]]
  - [[https://unofficial-kubernetes.readthedocs.io/en/latest/admin/authentication/][Authentication - Unofficial Kubernetes]]
    - [[https://unofficial-kubernetes.readthedocs.io/en/latest/admin/accessing-the-api/][Accessing the api - Unofficial Kubernetes]]
- [[https://scribe.bus-hit.me/m/global-identity?redirectUrl=https%3A%2F%2Fitnext.io%2Fbreaking-down-and-fixing-kubernetes-4df2f22f87c3][Breaking down and fixing Kubernetes]]
- [[https://www.bsdstore.ru/ru/articles/cbsd_k8s_part1.html][CBSD — FreeBSD Jail and Bhyve Management Tools]]
- [[https://polarsquad.com/blog/check-your-kubernetes-deployments][Check your Kubernetes deployments! — Polar Squad]]
- [[https://kubernetes.io/docs/tasks/configure-pod-container/][Configure Pods and Containers | Kubernetes]]
- [[https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#clean-up][Configure Quality of Service for Pods | Kubernetes]]
- [[https://habr.com/ru/company/flant/blog/329830/][Container Networking Interface (CNI) — сетевой интерфейс и стандарт для Linux-контейнеров / Хабр]]
- [[https://github.com/container-storage-interface/spec][container-storage-interface/spec: Container Storage Interface (CSI) Specification.]]
- [[https://stackoverflow.com/questions/43189487/create-kubernetes-nginx-ingress-without-gcp-load-balancer][Create kubernetes nginx ingress without GCP load-balancer - Stack Overflow]]
- [[https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/][Create static Pods | Kubernetes]]
- [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/][Creating a cluster with kubeadm | Kubernetes]]
- [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/][Creating Highly Available Clusters with kubeadm | Kubernetes]]
- [[https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF][CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF]]
- [[https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/][Debugging Kubernetes nodes with crictl | Kubernetes]]
- [[https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container][Debug Running Pods | Kubernetes]]
- [[https://github.com/defo89/kubernetes-the-hard-way-lab][defo89/kubernetes-the-hard-way-lab: Kubernetes The Hard Way in Lab (on VMware ESXi)]]
- [[https://github.com/dgkanatsios/CKAD-exercises][dgkanatsios/CKAD-exercises: A set of exercises to prepare for Certified Kubernetes Application Developer exam by Cloud Native Computing Foundation]]
- [[https://github.com/diegolnasc/kubernetes-best-practices][diegolnasc/kubernetes-best-practices: A cookbook with the best practices to working with kubernetes.]]
- [[https://engineering.empathy.co/distributed-load-testing-with-k6/][Distributed Load Testing with K6]]
- [[https://github.com/eon01/kubernetes-workshop][eon01/kubernetes-workshop: A Gentle introduction to Kubernetes with more than just the basics. Give it a star if you like it.]]
- [[https://github.com/eranyanay/cni-from-scratch][eranyanay/cni-from-scratch: Kubernetes networking: CNI plugin from scratch, in bash]]
- [[https://github.com/erebe/personal-server][erebe/personal-server: Personal server configuration with k3s]]
- [[https://blog.container-solutions.com/fluxcd-argocd-jenkins-x-gitops-tools][FluxCD, ArgoCD or Jenkins X: Which Is the Right GitOps Tool for You?]]
- [[https://agones.dev/site/docs/faq/#how-is-traffic-routed-from-the-allocated-port-to-the-gameserver-container][Frequently Asked Questions | Agones]]
- [[https://scribe.citizen4.eu/m/global-identity?redirectUrl=https%3A%2F%2Fitnext.io%2Fgitopsify-cloud-infrastructure-with-crossplane-and-flux-d605d3043452][GitOpsify Cloud Infrastructure with Crossplane and Flux]]
- [[https://kubernetes.io/docs/reference/glossary/?fundamental=true][Glossary | Kubernetes]]
- [[https://github.com/golang-design/under-the-hood][golang-design/under-the-hood: 📚 Go: Under The Hood | https://golang.design/under-the-hood]]
- [[https://martinheinz.dev/blog/100][Goodbye etcd, Hello PostgreSQL: Running Kubernetes with an SQL Database | Martin Heinz | Personal Website & Blog]]
- [[https://github.com/hobby-kube/guide][hobby-kube/guide: Kubernetes clusters for the hobbyist.]]
- [[https://stackoverflow.com/questions/53545732/how-do-i-access-a-private-docker-registry-with-a-self-signed-certificate-using-k][How do I access a private Docker registry with a self signed certificate using Kubernetes? - Stack Overflow]]
- [[https://jvns.ca/blog/2017/08/05/how-kubernetes-certificates-work/][How Kubernetes certificate authorities work]]
- [[https://opensource.com/article/19/11/how-contribute-kubernetes][How to contribute to Kubernetes if you have a full-time job | Opensource.com]]
- [[https://www.digitalocean.com/community/tutorials/how-to-progressively-deliver-releases-using-flagger-on-digitalocean-kubernetes][How To Progressively Deliver Releases Using Flagger on DigitalOcean Kubernetes | DigitalOcean]]
- [[https://itnext.io/kubernetes-cluster-creation-on-baremetal-host-using-cluster-api-1c2373230a17][How to write a Cluster API provider for running Kubernetes cluster on baremetal | by Himani Agrawal | ITNEXT]]
- [[https://github.com/kubernetes/ingress-nginx/tree/main/docs/examples/canary][ingress-nginx/docs/examples/canary at main · kubernetes/ingress-nginx]]
  - [[https://habr.com/ru/companies/flant/articles/697030/][Canary-релизы в Kubernetes на базе Ingress-NGINX Controller / Хабр]]
- [[https://dockerlabs.collabnix.com/kubernetes/beginners/Install-and-configure-a-multi-master-Kubernetes-cluster-with-kubeadm.html][Install and configure a multi-master Kubernetes cluster with kubeadm | dockerlabs]]
- [[https://kubernetes.io/docs/concepts/cluster-administration/addons/][Installing Addons | Kubernetes]]
- [[https://docs.openshift.com/dedicated/3/dev_guide/integrating_external_services.html][Integrating External Services | Developer Guide | OpenShift Dedicated 3]]
- [[https://www.cncf.io/wp-content/uploads/2020/08/2020_04_Introduction-to-Kubernetes-RBAC.pdf][Introduction to Kubernetes RBAC]]
- [[https://www.spectrocloud.com/blog/k3os-alternatives-the-best-container-os-for-edge-kubernetes][k3OS alternatives: the best container OS for edge Kubernetes - Spectro Cloud]]
- [[https://github.com/kelseyhightower/kubernetes-the-hard-way][kelseyhightower/kubernetes-the-hard-way: Bootstrap Kubernetes the hard way on Google Cloud Platform. No scripts.]]
- [[https://github.com/kodekloudhub/certified-kubernetes-administrator-course][kodekloudhub/certified-kubernetes-administrator-course: Certified Kubernetes Administrator - CKA Course]]
- [[https://github.com/kr8s-org/kr8s][kr8s-org/kr8s: A batteries-included Python client library for Kubernetes that feels familiar for folks who already know how to use kubectl]]
- [[https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#options-for-software-load-balancing][kubeadm/ha-considerations.md at main · kubernetes/kubeadm]]
- [[https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/][Kubernetes 1.24: Volume Expansion Now A Stable Feature | Kubernetes]]
- [[https://kubernetes.io/blog/2023/04/21/node-log-query-alpha/][Kubernetes 1.27: Query Node Logs Using The Kubelet API | Kubernetes]]
- [[https://iximiuz.com/en/posts/kubernetes-api-structure-and-terminology/][Kubernetes API Basics - Resources, Kinds, and Objects]]
- [[https://wiki.archlinux.org/title/Kubernetes][Kubernetes - ArchWiki]]
- [[https://www.weave.works/blog/kubernetes-authorization][Kubernetes Authorization]]
- [[https://k8s.af/][Kubernetes Failure Stories]]
  - [[https://codeberg.org/hjacobs/kubernetes-failure-stories][hjacobs/kubernetes-failure-stories: Compilation of public failure/horror stories related to Kubernetes - kubernetes-failure-stories - Codeberg.org]]
- [[https://serverfault.com/questions/955293/how-to-increase-disk-size-in-a-stateful-set][kubernetes - How to increase disk size in a stateful set - Server Fault]]
- [[https://www.linuxjournal.com/content/kubernetes-identity-management-authentication][Kubernetes Identity Management: Authentication | Linux Journal]]
- [[https://fabianlee.org/2022/04/18/kubernetes-kustomize-with-helm-charts/][Kubernetes: kustomize with Helm charts | Fabian Lee : Software Engineer]]
- [[https://www.youtube.com/watch?v=K3jNo4z5Jx8][Kubernetes Namespaces Explained in 15 mins | Kubernetes Tutorial 21]]
  : kubectl api-resources --namespaced=true
- [[https://www.weave.works/blog/kubernetes-pod-resource-limitations-and-quality-of-service][Kubernetes Pod Resource Limitations and Quality of Service]]
- [[https://github.com/kubernetes/sample-controller][kubernetes/sample-controller: Repository for sample controller. Complements sample-apiserver]]
- [[https://habr.com/ru/companies/otus/articles/761186/][Kubernetes Scheduler в Деталях: Важные Аспекты. Часть 1 / Хабр]]
- [[https://habr.com/ru/companies/otus/articles/771136/][Kubernetes Scheduler в Деталях: Ключевые Концепции. Часть 2 / Хабр]]
  - [[https://github.com/kubernetes-sigs/kube-scheduler-simulator][GitHub - kubernetes-sigs/kube-scheduler-simulator: The simulator for the Kubernetes scheduler]]
- [[https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-volumes-example-nfs-persistent-volume.html][Kubernetes Volumes Guide – Examples for NFS and Persistent Volume - Kubernetes Book]]
- [[https://rtfm.co.ua/kubernetes-znakomstvo-chast-1-arxitektura-i-osnovnye-komponenty-obzor/][Kubernetes: знакомство, часть 1 – архитектура и основные компоненты, обзор]]
- [[https://habr.com/ru/company/southbridge/blog/334846/][Kubernetes на голом железе за 10 минут / Хабр]]
- [[https://www.youtube.com/watch?v=Z9hBIoO4KOs][KubeVirt deep dive - YouTube]]
  - [[https://github.com/davidvossel/kubevirt-demo-manifests][davidvossel/kubevirt-demo-manifests]]
- [[https://scribe.citizen4.eu/kubernetes-tutorials/learn-how-to-assign-pods-to-nodes-in-kubernetes-using-nodeselector-and-affinity-features-e62c437f3cf8][Learn How to Assign Pods to Nodes in Kubernetes Using nodeSelector and Affinity Features]]
- [[https://learnkubernetes.withgoogle.com/#multicluster-services-api][Learn Kubernetes with Google]]
- [[https://learnk8s.io/rbac-kubernetes][Limiting access to Kubernetes resources with RBAC]]
- [[https://github.com/magnologan/awesome-k8s-security][magnologan/awesome-k8s-security: A curated list for Awesome Kubernetes Security resources]]
- [[https://www.starkandwayne.com/blog/managing-your-kubernetes-cluster-with-daemonsets/][Managing Your k8s Cluster via DaemonSets - Stark & Wayne]]
- [[https://kubernetes.io/docs/reference/tools/map-crictl-dockercli/][Mapping from dockercli to crictl | Kubernetes]]
- [[https://www.cncf.io/blog/2023/03/07/my-first-experience-with-kyverno-%f0%9f%a4%af%f0%9f%a4%a9/][My first experience with Kyverno: 🤯🤩 | Cloud Native Computing Foundation]]
- [[https://github.com/neargle/my-re0-k8s-security][neargle/my-re0-k8s-security: [WIP] 整理过去的分享，从零开始的Kubernetes攻防 🧐]]
- [[https://github.com/omerbsezer/Fast-Kubernetes][omerbsezer/Fast-Kubernetes: This repo covers Kubernetes with LABs: Kubectl, Pod, Deployment, Service, PV, PVC, Rollout, Multicontainer, Daemonset, Taint-Toleration, Job, Ingress, Kubeadm, Helm, etc.]]
- [[https://www.youtube.com/watch?v=nPZ8QDZXtLI][OpenID Connect and OAuth 2 explained in under 10 minutes! - YouTube]]
- [[https://scribe.citizen4.eu/aspecto/opentelemetry-operator-for-kubernetes-practical-guide-part-4-1d8d86eb2bb][OpenTelemetry Operator for Kubernetes: Practical Guide | Part 4]]
- [[https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster][Operating etcd clusters for Kubernetes | Kubernetes]]
- [[https://www.tigera.io/lp/kubernetes-security-and-observability-ebook/][O'Reilly eBook: Kubernetes Security and Observability]]
- [[http://ceur-ws.org/Vol-2839/paper11.pdf][paper11.pdf]]
- [[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#recovering-from-failure-when-expanding-volumes][Persistent Volumes | Kubernetes]]
- Pin pod to specific node:
  - Affinity
  - Toleration
  - Taint
- [[https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/][Pod Lifecycle | Kubernetes]]
- Port forwarding
  - =kubectl port-forward my-deployment-xxxxxxxxxx-xxxxx 8005:80 &=
- [[https://piotrminkowski.com/2022/12/02/renew-certificates-on-kubernetes-with-cert-manager-and-reloader/][Renew Certificates on Kubernetes with Cert Manager and Reloader]]
- [[https://github.com/rootsongjc/kubernetes-handbook][rootsongjc/kubernetes-handbook: Kubernetes中文指南/云原生应用架构实战手册 - https://jimmysong.io/kubernetes-handbook]]
- [[https://github.com/run-x/awesome-kubernetes][run-x/awesome-kubernetes: A curated list for awesome kubernetes projects, tools and resources.]]
- [[https://kubernetes.io/docs/concepts/scheduling-eviction/_print/][Scheduling, Preemption and Eviction | Kubernetes]]
- [[https://devopstales.github.io/kubernetes/k8s-metallb-bgp-pfsense/][Self-hosted Load Balancer for bare metal Kubernetes - devopstales]]
- [[https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/][Set Kubelet parameters via a config file | Kubernetes]]
- [[https://gabrieltanner.org/blog/ha-kubernetes-cluster-using-k3s/][Setting up a HA Kubernetes cluster using K3S]]
- [[https://medium.com/@sven_50828/setting-up-a-high-availability-kubernetes-cluster-with-multiple-masters-31eec45701a2][Setting up a high-availability Kubernetes cluster with multiple masters | by Sven Hakvoort | Medium]]
- Show documentation
  - =kubectl explain deployment=
  - =kubectl explain deployment.spec=
- [[https://www.cncf.io/blog/2021/04/12/simplifying-multi-clusters-in-kubernetes/][Simplifying multi-clusters in Kubernetes | Cloud Native Computing Foundation]]
- [[https://github.com/stefanprodan/podinfo][stefanprodan/podinfo: Go microservice template for Kubernetes]]
- [[https://shivering-isles.com/2022/03/store-kubernetes-credentials-pass][Store Kubernetes Credentials in pass]]
- [[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/][Taints and Tolerations | Kubernetes]]
- [[https://github.com/techiescamp/kubernetes-learning-path][techiescamp/kubernetes-learning-path: A roadmap to learn Kubernetes from scratch (Beginner to Advanced level)]]
- [[https://habr.com/ru/company/nixys/blog/649663/][Terraform модуль для создания RBAC-правил / Хабр]]
- [[https://www.freecodecamp.org/news/the-kubernetes-handbook/][The Kubernetes Handbook – Learn Kubernetes for Beginners]]
- [[https://kubernetes.io/docs/tasks/debug/debug-cluster/][Troubleshooting Clusters | Kubernetes]]
- [[https://kubernetes.io/blog/2023/09/13/userns-alpha/][User Namespaces: Now Supports Running Stateful Pods in Alpha! | Kubernetes]]
- [[https://www.alibabacloud.com/help/en/container-service-for-kubernetes/latest/use-the-host-network][Use the host network]]
  #+begin_src yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      hostNetwork: true
      containers:
      - name: nginx
        image: nginx
  #+end_src
- [[https://piotrminkowski.com/2023/03/20/vault-with-secrets-store-csi-driver-on-kubernetes/][Vault with Secrets Store CSI Driver on Kubernetes]]
- [[https://github.com/Vinum-Security/kubernetes-security-checklist][Vinum-Security/kubernetes-security-checklist: Kubernetes Security Checklist and Requirements - All in One (authentication, authorization, logging, secrets, configuration, network, workloads, dockerfile)]]
- [[https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist][walidshaari/Certified-Kubernetes-Security-Specialist: Curated resources help you prepare for the CNCF/Linux Foundation CKS 2021 "Kubernetes Certified Security Specialist" Certification exam. Please provide feedback or requests by raising issues, or making a pull request. All feedback for improvements are welcome. thank you.]]
- [[https://github.com/walidshaari/Kubernetes-Certified-Administrator][walidshaari/Kubernetes-Certified-Administrator: Online resources that will help you prepare for taking the CNCF CKA 2020 "Kubernetes Certified Administrator" Certification exam. with time, This is not likely the comprehensive up to date list - please make a pull request if there something that should be added here.]]
- [[https://www.techtarget.com/searchitoperations/tip/When-to-use-Kubernetes-operators-vs-Helm-charts][When to use Kubernetes operators vs. Helm charts]]
- [[https://scribe.citizen4.eu/@senthilrch/woot-kubernetes-adds-support-for-swap-memory-92541aad01a0][Woot…Kubernetes Adds Support for Swap Memory]]
- [[https://eltonminetto.dev/en/post/2023-10-05-k8s-operator-test/][Writing tests for a Kubernetes Operator]]
- [[https://github.com/wrmilling/k3s-gitops][wrmilling/k3s-gitops: GitOps principles to define kubernetes cluster state via code]]
- [[https://developers.redhat.com/blog/2020/11/10/you-probably-need-liveness-and-readiness-probes#example_1__a_static_file_server__nginx_][You (probably) need liveness and readiness probes | Red Hat Developer]]
- [[https://habr.com/ru/companies/slurm/articles/764740/][Автоскейлинг контроллеров Ingress в Kubernetes / Хабр]]
- [[https://habr.com/ru/company/qiwi/blog/585392/][Безопасность Kubernetes — это просто / Хабр]]
- [[https://habr.com/ru/company/nixys/blog/658985/][Введение в k3s: познакомимся с младшим братом k8s / Хабр]]
- [[https://habr.com/ru/companies/flant/articles/541642/][Вертикальное автомасштабирование pod'ов в Kubernetes: полное руководство / Хабр]]
- [[https://habr.com/ru/companies/oleg-bunin/articles/761662/][Дополнительные контейнеры в Kubernetes и где они обитают: от паттернов к автоматизации управления / Хабр]]
- [[https://habr.com/ru/company/flant/blog/420813/][За кулисами сети в Kubernetes / Хабр]]
- [[https://habr.com/ru/post/657641/][Как Kubernetes создает и запускает контейнеры: Иллюстрированное руководство / Хабр]]
- [[https://habr.com/ru/company/flant/blog/521406/][Как pod в Kubernetes получает IP-адрес / Хабр]]
- [[https://habr.com/ru/articles/737968/][Коварство метрик памяти Kubernetes (и cgroups) / Хабр]]
- [[https://serveradmin.ru/kubernetes-ingress/][Настройка Ingress controller в Kubernetes — Server Admin]]
- [[https://habr.com/ru/company/nixys/blog/652925/][О границах масштабируемости Kubernetes / Хабр]]
- [[https://habr.com/ru/companies/oleg-bunin/articles/766898/][Операторы в Kubernetes / Хабр]]
- [[https://habr.com/ru/companies/timeweb/articles/787774/][Опыт масштабирования Kubernetes на 2k узлов и на 400k подов / Хабр]]
- [[https://habr.com/ru/company/nixys/blog/664660/][Ошибочные шаблоны при построении образов контейнеров / Хабр]]
- [[https://habr.com/ru/companies/slurm/articles/779986/][Пишем оператор Kubernetes: руководство для начинающих / Хабр]]
- [[https://habr.com/ru/company/netologyru/blog/580902/][Планирование подов для узлов Kubernetes: развёрнутое руководство / Хабр]]
- [[https://habr.com/ru/companies/selectel/articles/777414/][Правильные, но (не)простые бэкапы. Как настроить резервное копирование в Kubernetes? / Хабр]]
- [[https://dotsandbrackets.com/kubernetes-example-ru/][Разбираем Kubernetes пример - Dots and Brackets]]
- [[http://itisgood.ru/2019/11/18/raspredelennoe-hranenie-dannyh-v-kubernetes/][☸️ Распределенное хранение данных в Kubernetes — IT is good]]
- [[https://habr.com/ru/post/673730/][Сертификаты K8S или как распутать вермишель Часть 1 / Хабр]]
- [[https://habr.com/ru/company/flant/blog/676678/][Снапшоты в Kubernetes: что это и как ими пользоваться / Хабр]]
- [[https://habr.com/ru/company/otus/blog/653973/][Тестирование сетевых политик с помощью Minikube / Хабр]]
- [[https://habr.com/ru/company/nixys/blog/657359/][Универсальный Helm-чарт для ваших приложений / Хабр]]
- [[https://serveradmin.ru/kubernetes-ustanovka/][Установка кластера Kubernetes | serveradmin.ru]]
- [[https://habr.com/ru/companies/flant/articles/353410/][Устройство и механизм работы Prometheus Operator в Kubernetes / Хабр]]

** [[https://shivering-isles.com/2023/09/about-read-only-containers-ruby-and-emptydir][About Read-Only Containers, Ruby and EmptyDir]]
I’m running the majority of my workloads on Kubernetes these days, including my Mastodon instance. For many years I’ve been running Mastodon in read-only mode for the container filesystem. However, I somehow missed the Sidekiq container.

** Addendum

Following up on the conversation mentioned above, I dove into the kubelet source code[[https://github.com/kubernetes/kubernetes/blob/622509830c1038535e539f7d364f5cd7c3b38791/pkg/volume/emptydir/empty_dir.go#L46][¹]] [[https://github.com/kubernetes/kubernetes/blob/622509830c1038535e539f7d364f5cd7c3b38791/pkg/volume/emptydir/empty_dir.go#L441-L474][²]] [[https://github.com/kubernetes/kubernetes/blob/622509830c1038535e539f7d364f5cd7c3b38791/pkg/volume/emptydir/empty_dir.go#L272-L281][³]], to figure out how the kubelet creates it’s =emptyDir=. There I learned that the behaviour with forced =0777= only exists for directory-based =emptyDir=. Therefore if you don’t want to use Generic Ephemeral Volumes, you can also just use in-memory =emptyDir= :

#+begin_src c
- name: tmp
  emptyDir:
    medium: Memory

#+end_src

And after figuring that out, I decided to check the issues tracker, turns out, it’s [[https://github.com/kubernetes/kubernetes/issues/110835][a well-known bug]], that is probably becoming a =wontfix= .
** [[https://blog.thomas.maurice.fr/posts/authenticate-with-kubernetes-using-vault/#creating-the-oidc-endpoint][Authenticate to Kubernetes using Hashicorp Vault]]
📅 Oct 14, 2021
 ·  ☕ 23 min read

    🏷️
    #security
    #vault
    #kubernetes

Authenticating with Kubernetes can be done in a wide variety of ways. You can use user certificates, you can use service account tokens, you can use IAM on Google GKE, you can also use AWS IAM on EKS and whatever the equivalent is on Azure AKS. But there are not many easy options to choose from if you are not using a cloud provider.

We are going to explore how to use Hashicorp Vault to serve as an OpenID connect provider that will let you authenticate your users using Vault, and set up some basic Role Based Access Control (RBAC) for it.
Few words of warning

⚠️ this is intended to linux users, I have no idea if the hacky shit that happens here (not specific to k8s, but required to make this setup work unfortunately) is going to work on OSX.

⚠️ This is a long-ass article, in fact it is the longest piece of documentation material I have ever written in my whole life. Feel free to read it in several sittings.

⚠️ I am demonstrating doing this with Vault because I already have a Vault setup at home I use for a variery of things. But really you could use any sort of IDP such as Dex or Hydra if you feel like it. If you don’t have a Vault setup already, worry not! This article will explain to you how to setup one from scratch for this usecase.

⚠️ This article is going to require a very (very) basic understanding of Kubernetes, Terraform and Vault, if it is not the case, mild brain damage and cerebral fluid leakages may happen. I will not be responsible for any of those.

⚠️ If you are a user of EKS/GKE, you can actually change the apiserver’s configuration to add your own OIDC providers to it, so you would be able to use the method described here to authenticate to your EKS cluster from your very own Vault!

⚠️ I compiled all the code snippets, configs and such used in this article here on github so you can have access to all of it for reference !
Authentication in Kubernetes

In Kubernetes you can identify to the apiserver in a variety of way, like x509 certificates, user static token files, service account tokens, bootstrap tokens, OIDC connect but really it boils down to mainly using access tokens.

A token is usually a piece of signed information containing identity data about the person calling a service, which are going to be mainly about a user ID, a list of groups that maybe you belong to, an expiration date (so the credentials do not live forever) and a few pieces of metadata. This format is a standard and is commonly referred to as JWT or JSON web tokens.

Kubernetes is capable of understanding these to assert whether or not you use a valid identity and to further perform authorization on the request you wish to perform. Coincidentally, it just so happens that Hashicorp Vault is capable of serving as an identity provider (the docs might look overwhelming, it’s fine we will do it step by step), so why not mix the two ?

⚠️ I already wrote a somewhat relevant post about using JWTs and Vault if you want a deeper understanding of how it works, so there it is
What we will end up achieving today

The goal of this article is to make you able to authenticate to Kubernetes with an identity you got from Vault, who belongs to a defined set of groups, and demonstrate you basic RBAC rules that would grant, or deny you access to certain ressources depending on these. Ultimately, you will be able to add and delete users easily to grant or revoke access to your cluster in a somewhat simple fashion.

Now enough talking and more doing !
Setting up Vault

First we need to have a Vault setup going. We want to setup a couple of things

    A user backend. Now there are a lot of ways you can get users going in Vault. For the sake of simplicity we are going to setup a very simple userpass backend. But you could as well use an existing one like AWS/Azure/GCP auth, TLS certificates authentication as well as LDAP and many more.
    The Vault entities that map to these users
    The groups that these entities will belong to
    An OIDC endpoint so authenticated users may get their OIDC tokens
    A few policies to cobble all that together

You could do that using the vault command line utility and talk to the vault server directly but I personally dislike it as it is not the most user friendly command line tool. So Instead we are going to use Hashicorp’s Terraform to set all that up.
Fire up a Vault server

There are a few ways you can do that, we are going to use Vault in devmode, which means that every change you make to vault is going to be lost every time you restart the server, so be aware of it. You can start a developement server using docker like so

docker run --net host --cap-add IPC_LOCK vault vault server -dev -dev-root-token-id=devtoken

or simply using the vault command line

vault server -dev -dev-root-token-id=devtoken

Note that devtoken is going to be the root token of the new vault server (you can see it as the “root password for Vault”), which will grant you super user rights on it. Try to access vault at http://localhost:8200 and login using the token, just to make sure it works.
Housekeeping

For the rest of this article you might want to export the VAULT_ADDR variable so you don’t get randomly fucked if you use another Vault deployment.

$ export VAULT_ADDR=http://127.0.0.1:8200

⚠️ all of your vault lives in memory. If for some reason you kill the process, you will have to re-apply all the terraform code to restore it. You can go around it by setting up a more permanent vault installation but this is out of scope for this article.
Add the userpass authentication backend and the users

Now we want to have a bunch of users inside of Vault that can login and take actions, using a username and password combo. In a real world production setup, this would probably be replaced by some sensible identity provider like Auth0 or Okta but in our case, a password will do.

Create a new directory and a new file, name it something like vault.tf and pop the following in there

terraform {
  backend "local" {
    path = "terraform.tfstate"
  }
}

provider "vault" {
  address = "http://127.0.0.1:8200"
  token   = "devtoken"
}

The first block tells you where terraform is going to store its state (in a terraform.tfstate file) and a second one that tells terraform that the vault server it will talk to is the one we just started and it should use the devtoken to login.

⚠️ if you restart the vault server, the state will be out of date, since the new server will be clean and brand new, so you should delete the statefile before trying to run anything else.

⚠️ all the terraform code is available here

You can now go ahead and initialise Terraform

$ terraform init

Initializing the backend...

Successfully configured the backend "local"! Terraform will automatically
use this backend unless the backend configuration changes.

Initializing provider plugins...
- Finding latest version of hashicorp/vault...
- Installing hashicorp/vault v2.24.1...
- Installed hashicorp/vault v2.24.1 (self-signed, key ID 34365D9472D7468F)

Partner and community providers are signed by their developers.
If you'd like to know more about provider signing, you can read about it here:
https://www.terraform.io/docs/plugins/signing.html

Terraform has created a lock file .terraform.lock.hcl to record the provider
selections it made above. Include this file in your version control repository
so that Terraform can guarantee to make the same selections by default when
you run "terraform init" in the future.

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.

Next up we are going to create the userpass backend, append the following to your file

resource "vault_auth_backend" "userpass" {
  type = "userpass"
  path = "userpass"
}

This will allow to create users that are authenticated with a username/password combo.

Run terraform plan then terraform apply

$ terraform apply
terraform apply

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # vault_auth_backend.userpass will be created
  + resource "vault_auth_backend" "userpass" {
      + accessor                  = (known after apply)
      + default_lease_ttl_seconds = (known after apply)
      + id                        = (known after apply)
      + listing_visibility        = (known after apply)
      + max_lease_ttl_seconds     = (known after apply)
      + path                      = "userpass"
      + tune                      = (known after apply)
      + type                      = "userpass"
    }

Plan: 1 to add, 0 to change, 0 to destroy.

Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

  Enter a value: yes

vault_auth_backend.userpass: Creating...
vault_auth_backend.userpass: Creation complete after 0s [id=userpass]

Apply complete! Resources: 1 added, 0 changed, 0 destroyed.

The state of your infrastructure has been saved to the path
below. This state is required to modify and destroy your
infrastructure, so keep it safe. To inspect the complete state
use the `terraform show` command.

State path: terraform.tfstate

⚠️ now when I write apply your plan it refers to entering the terraform plan and terraform apply commands. It will be much easier.

We now need to create two users, they will be named user1 and user2 and will have respective passwords password1 and password2.

resource "vault_generic_endpoint" "user1" {
  depends_on           = [vault_auth_backend.userpass]
  path                 = "auth/userpass/users/user1"

  data_json = <<EOT
{
  "password": "password1"
}
EOT
}

resource "vault_generic_endpoint" "user2" {
  depends_on           = [vault_auth_backend.userpass]
  path                 = "auth/userpass/users/user2"

  data_json = <<EOT
{
  "password": "password2"
}
EOT
}

Same as before, plan and apply. Next you should verify that you can login as one of these users. Your output should look something like the following:

$ vault login -method=userpass username=user1 password=password1
Success! You are now authenticated. The token information displayed below
is already stored in the token helper. You do NOT need to run "vault login"
again. Future Vault requests will automatically use this token.

Key                    Value
---                    -----
token                  s.7cVUe23ytt2X3k2o5FRsZeYg
token_accessor         xyaTfw2hZdmpNuG9Bs34WKcf
token_duration         768h
token_renewable        true
token_policies         ["default"]
identity_policies      []
policies               ["default"]
token_meta_username    user1

Now that it is all good, let’s move on.
Create the entities and aliases

We are now going to create 2 things per user

    An entity, which is the internal representation of the users within vault. An entity is used to attach policies to a user, allowing them to do things within Vault.
    An entity alias, which links the internal entity we created, to for instance user1 in Vault. This allows you to map the same vault entity to several auth backends, allowing you to have scenarios where you could authenticate to vault with both gsuite and ldap

The code will look like this

resource "vault_identity_entity" "user1" {
  name      = "user1"
  policies  = ["kubernetes-policy-test"]
}

resource "vault_identity_entity" "user2" {
  name      = "user2"
  policies  = ["kubernetes-policy-test"]
}

resource "vault_identity_entity_alias" "user1" {
  name            = "user1"
  mount_accessor  = vault_auth_backend.userpass.accessor
  canonical_id    = vault_identity_entity.user1.id
}

resource "vault_identity_entity_alias" "user2" {
  name            = "user2"
  mount_accessor  = vault_auth_backend.userpass.accessor
  canonical_id    = vault_identity_entity.user2.id
}

so basically one entity and alias per user. Note that I added a kubernetes-policy-test policy. This does not exist but thay will allow you to test your setup actually works. Try to login again and check you are assigned the proper policy

$ vault login -method=userpass username=user1 password=password1
Success! You are now authenticated. The token information displayed below
is already stored in the token helper. You do NOT need to run "vault login"
again. Future Vault requests will automatically use this token.

Key                    Value
---                    -----
token                  s.2Ks6LpmWndf7miF7Tqu962kH
token_accessor         2lHLWc1ZmB1bnHgjE6BXhr30
token_duration         768h
token_renewable        true
token_policies         ["default"]
identity_policies      ["kubernetes-policy-test"]
policies               ["default" "kubernetes-policy-test"]
token_meta_username    user1

Amazing!

⚠️ when in the future I will say “log into vault” it means running vault login -method=userpass username=user1 password=password1 or the equivalent with user2
Creating groups

Creating groups work pretty much the same way as adding entities. We will create a group containing the user1 that will be the cluster admin, as well as a red only group that will contain the user2. Both groups are going to be assigned a policy kubernetes-access that will be internal to vault and will allow both users to read an OIDC token from vault.

⚠️ in practice you might want an “umbrella” group that will contain this policy, with all the various RBAC groups as its children but we will not cover it here for the sake of simplicity.

It looks like this:

resource "vault_identity_group" "kubernetes-admin" {
  name     = "kubernetes-admin"
  type     = "internal"

  policies = ["kubernetes-access"]

  member_entity_ids = [
    vault_identity_entity.user1.id,
  ]
}

resource "vault_identity_group" "kubernetes-user-readonly" {
  name     = "kubernetes-user-readonly"
  type     = "internal"

  policies = ["kubernetes-access"]

  member_entity_ids = [
    vault_identity_entity.user2.id,
  ]
}

Plan and apply, and you are done !
Creating the OIDC endpoint

Now you need to create the OIDC endpoint that will allow your users to fetch a token. For that you need to setup Vault in an OIDC provider mode. This is done like so

resource "vault_identity_oidc" "oidc_server" {
  # Do not change this, you will see in the next sections why it matters
  issuer = "https://vault.example.com"
}

resource "vault_identity_oidc_key" "key" {
  name             = "key"
  algorithm        = "ES256"
  rotation_period  = 24 * 3600
  verification_ttl = 24 * 3600
}

# will create a path at v1/identity/oidc/token/k8s-token
resource "vault_identity_oidc_role" "k8s-token" {
  name     = "k8s-token"
  key      = vault_identity_oidc_key.key.name
  template = <<EOF
{
  "groups": {{identity.entity.groups.names}},
  "nbf": {{time.now}}
}
EOF
}

# Allow the role "k8s-token" to use the key
resource "vault_identity_oidc_key_allowed_client_id" "oidc_key" {
  key_name          = vault_identity_oidc_key.key.name
  allowed_client_id = vault_identity_oidc_role.k8s-token.client_id
}

This looks scary, but it is not. First we tell vault it’s name (issuer) for the tokens. Then create a crypto key, rotated every day, to sign issued tokens. Next you create an OIDC role (which would be the equivalent of creating an OIDC application in simpler terms) then we allow the role to get tokens issued with the above signing key. You might have noticed the template in the user creation, it is extra information that vault will insert into the token when it creates it, here we add the groups a user belongs to. More info on token templates here

Then again, run and apply!
Policies !

Now the last bit we need to do is to write the kubernetes-access policy that we have used in the Vault groups before. This is a very simple bit of terraform code that looks like this.

resource "vault_policy" "kubernetes-access" {
  name = "kubernetes-access"

  policy = <<EOT
path "identity/oidc/token/k8s-token" {
  capabilities = ["read"]
}
EOT
}

This basically says that every user that has this policy affected can read the identity/oidc/token/k8s-token path that will serve our freshly minted tokens.

Plan and apply !
Testing it out

Testing is straightforward, login as user1 as we have done previously

$ vault login -method=userpass username=user1 password=password1
[...]

Key                    Value
---                    -----
token                  s.dHegU8fjEPHB6VwfP38Wm6qq
token_accessor         6JPJT3B2r9TbUUc9YXcdjkgn
token_duration         768h
token_renewable        true
token_policies         ["default"]
identity_policies      ["kubernetes-access" "kubernetes-policy-test"]
policies               ["default" "kubernetes-access" "kubernetes-policy-test"]
token_meta_username    user1

Now use the vault token, s.dHegU8fjEPHB6VwfP38Wm6qq here, to try and read the kubernetes token

$ VAULT_TOKEN=s.dHegU8fjEPHB6VwfP38Wm6qq vault read identity/oidc/token/k8s-token
Key          Value
---          -----
client_id    pzi1boK6Nfft91Em7NW3k62HUX
token        eyJhbGciOiJFUzI1NiIsImtpZCI6IjQ0NWY2NTNjLWEyZjctZmVmMi0wNzk5LTI0YmU2MjkwOGY4MiJ9.eyJhdWQiOiJwemkxYm9LNk5mZnQ5MUVtN05XM2s2MkhVWCIsImV4cCI6MTYzNDIxMjAzNCwiZ3JvdXBzIjpbImt1YmVybmV0ZXMtYWRtaW4iXSwiaWF0IjoxNjM0MTI1NjM0LCJpc3MiOiJodHRwOi8vMTI3LjAuMC4xOjgyMDAvdjEvaWRlbnRpdHkvb2lkYyIsIm5hbWVzcGFjZSI6InJvb3QiLCJuYmYiOjE2MzQxMjU2MzQsInN1YiI6ImRjYmNhM2NlLTgxZTQtYWRmOC1mNTA5LTRlNTM5MmY2MGVkZCJ9.wNiMPHwYVVW_-HPujEWFBRsv5e7ZGrhpOtjCEuIVtJRbzHVMTj2vHWB8BGnRW98LjVsK1NOmwn8WLetvDTI4Nw
ttl          24h

Success ! You now have an OIDC token. But what does it contain ? Head out to the debugger at jwt.io and let’s find out. Upon inspection the body of the token looks like this:

{
  "alg": "ES256",
  "kid": "445f653c-a2f7-fef2-0799-24be62908f82"
}
{
  "aud": "pzi1boK6Nfft91Em7NW3k62HUX",
  "exp": 1634212034,
  "groups": [
    "kubernetes-admin"
  ],
  "iat": 1634125634,
  "iss": "https://vault.example.com/v1/identity/oidc",
  "namespace": "root",
  "nbf": 1634125634,
  "sub": "dcbca3ce-81e4-adf8-f509-4e5392f60edd"
}

The things you want to note are:

    alg which is the algorithm used to sign the token, we will need it later
    aud which is the clientid of the “app”
    iss which identifies which server issued the token
    groups which contains the list of your vault groups
    sub which is the ID of your entity in vault

Now that it is all setup on the vault side, let’s setup a kubernetes!
(optional) add outputs to terraform

You can optionally add these few lines to terraform to output variables you care about after each apply. But this is purely optional

output "oidc_client_id" {
  value = vault_identity_oidc_role.k8s-token.client_id
}

output "k8s_command_token" {
  value = "vault read identity/oidc/token/${vault_identity_oidc_role.k8s-token.name}"
}

output "user1_sub" {
  value = vault_identity_entity.user1.id
}

output "user2_sub" {
  value = vault_identity_entity.user2.id
}

Setting up a TLS reverse proxy

Remember when I told you a paragraph ago that the next step will be setting up Kubernetes ? Well, I lied. You need another thing, which is a TLS communication channel between kubernetes and your Vault. Eventhough this is a very sound thing to consider from a basic security point of view, it is annoying when you just want to get something up and running. So we are going to speedrun this one because it does not add any real value to the article. We are going to use traefik as our reverse and TLS proxy.
Setting up the dummy interface and domain name

You cannot just point the kubernetes cluster to talk to vault on localhost, because localhost in the container (we are going to run k8s in containers!!) is going to be very different from localhost on your machine. Hence you need to make Traefik listen on a specific address (that is not localhost) to make it all work. We are going to say that our vault is going to be at vault.example.com which will be address 10.10.10.10/32. Note that this address can really be whatever you want it to as long as the /etc/hosts entry matches the address you set on the interface, and as long as the address you choose is not in the 127.0.0.0/8 range. So first add the following line in your /etc/hosts file.

10.10.10.10   vault.example.com

Good now create the interface

$ sudo ip link add dummyIface type dummy
$ sudo ip link set up dev dummyIface
$ sudo ip address add 10.10.10.10/32 dev dummyIface

All set! Now when Kubernetes will want to contact vault.example.com, since the /etc/hosts is shared, it will force the apiserver network call to get out of the container and actually reach the proxy.

This is a hack, and I am embarassed to put into writting how long it took me to come up with it to make it all work.
Setting up Traefik

In the working directory you have been using for this article (which i hope you did), create a script to generate our TLS certificates. Let’s call it certs.sh

#!/bin/bash

if ! [ -d ./ssl ]; then mkdir ./ssl; fi;

openssl \
    req \
    -new \
    -nodes \
    -days 365 \
    -x509 \
    -newkey rsa:4096 \
    -keyout ./ssl/cert.key \
    -out ./ssl/cert.crt \
    -subj "/CN=vault.example.com" \
    -addext "subjectAltName = DNS:vault.example.com"

Run it

$ ./cert.sh 
Generating a RSA private key
...............+++++
.+++++
writing new private key to './ssl/cert.key'
-----

All good. Then create the traefik.toml config file. Trust me, this one works

[log]
  level = "debug"

[providers]
    [providers.file]
        directory = "/etc/traefik"

[entryPoints]
  [entryPoints.web]
    address = ":80"
    [entryPoints.web.http.redirections.entryPoint]
      to = "websecure"
      scheme = "https"

  [entryPoints.websecure]
    address = ":443"
[http.services]
  [http.services.vault.loadBalancer]
    [[http.services.vault.loadBalancer.servers]]
      url = "http://127.0.0.1:8200/"
[http.routers]
  [http.routers.vault]
    rule = "Host(`vault.example.com`)"
    service = "vault"
    [http.routers.vault.tls]
[[tls.certificates]]
  certFile = "/etc/traefik/ssl/cert.crt"
  keyFile = "/etc/traefik/ssl/cert.key"

Next you need a script to start the reverse proxy. This is done like so

#!/bin/bash

docker run \
    --net host \
    -v ${PWD}/traefik.toml:/etc/traefik/traefik.toml \
    -v ${PWD}/ssl:/etc/traefik/ssl \
    -it traefik

It should work, then we can actually set up kubernetes (i am not lying this time).
Setting up a Kubernetes

To setup kubernetes we are going to use kind, which you can download here.

What kind does in a nutshell is spin-up a fully functional kubernetes cluster locally, for testing purposes, inside of Docker. Do grab the latest binary and let’s do it!
Creating the kind config file

We are going to configure kind a bit, because by default it does not allow you to use OIDC authentication. So to do that, in your working directory create a cluster.yaml file with the following content:

---
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
    image: kindest/node:v1.21.2
    extraMounts:
    # CHANGE THIS
    - hostPath: /home/thomas/vaultarticle/ssl
      containerPath: /etc/ssl/certs/oidc
      readOnly: true
      propagation: HostToContainer
    kubeadmConfigPatches:
      - |
        kind: ClusterConfiguration
        apiServer:
            extraArgs:
              # CHANGE THIS
              oidc-client-id: pzi1boK6Nfft91Em7NW3k62HUX
              oidc-groups-claim: groups
              oidc-groups-prefix: "vault:"
              oidc-issuer-url: "https://vault.example.com/v1/identity/oidc"
              oidc-username-claim: sub
              oidc-username-prefix: "vault:"
              oidc-signing-algs: "ES256,RS256"
              oidc-ca-file: "/etc/ssl/certs/oidc/cert.crt"        
  - role: worker
    image: kindest/node:v1.21.2

⚠️ replace /home/thomas/vaultarticle/ssl by the directory you put your TLS certificates in.

This is fairly simple config. We create a 2 nodes cluster with a worker and a control plane. We also pass extra arguments to the apiserver in the extraArgs section of the config. These are equivalent to adding --oidc-client-id=pzi1boK6Nfft91Em7NW3k62HUX and so on to the apiserver upon startup or changing the config in the apiserver configuration file.

The arguments are fairly easy to understand, but here is the breakdown:

    oidc-client-id: It is the aud that you find in your JWT token that you found above
    oidc-groups-claim: Is the name of the JSON field that contains the list of groups the user is mapped to
    oidc-username-claim: Same but with your username
    oidc-signing-algs: It is the signing algorithm that the key we defined in vault uses, you can put several separated by a coma
    oidc-issuer-url: It is the URL of your vault server, with the v1/identity/oidc path appended for openid connect config discovery
    oidc-groups-prefix and oidc-username-prefix are prefixes that kubernetes is going to prepend to your groups and users informations. For instance if your user1 belongs in the kubernetes-admin group, then in your kubernetes RBAC policies you will need to reference it as vault:kubernetes-admin
    oidc-ca-file the root CA file. If you use a legit provider like Letsencrypt, you will not need this. But with our home-baked certificate we need to give this.

Create the kind cluster

Creating the kind cluster is as simple as running

$ kind create cluster --config cluster.yaml

It will download images, output a few things and you should be ready to go.

Creating cluster "kind" ...
 ✓ Ensuring node image (kindest/node:v1.21.2) 🖼
 ✓ Preparing nodes 📦 📦  
 ✓ Writing configuration 📜 
 ✓ Starting control-plane 🕹️ 
 ✓ Installing CNI 🔌 
 ✓ Installing StorageClass 💾 
 ✓ Joining worker nodes 🚜 
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Have a nice day! 👋

Backup an admin kubeconfig

Run the following to get a kubeconfig that will retain your admin access in case you break it (you probably will)

$ kind get kubeconfig > kubeconfig

Setup a few namespaces and a few rolebindings

Let us create a few ClusterRoles and a few bindings, along with namespaces. Create a file named rbac.yaml containing the following

############## NAMESPACES
---
kind: Namespace
apiVersion: v1
metadata:
    name: admin-only
---
kind: Namespace
apiVersion: v1
metadata:
    name: user2
---
############## CLUSTER ROLES
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
    name: admin
rules:
  - apiGroups: ["*"]
    resources: ["*"]
    verbs: ["*"]
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
    name: ro
rules:
  - apiGroups: ["*"]
    resources: ["*"]
    verbs:
    - get
    - list
    - watch
---
############## CLUSTER ROLE BINDINGS
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
    name: admin
subjects:
- kind: Group
  name: 'vault:kubernetes-admin'
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: admin
  apiGroup: rbac.authorization.k8s.io
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
    name: admin
    namespace: user2
subjects:
- kind: Group
  name: 'vault:kubernetes-user-readonly'
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: ro
  apiGroup: rbac.authorization.k8s.io

Aight it is a mouthful, I should be sorry but i cannot, kubernetes RBAC is hard. There are three part to this utter shitshow of yaml

    Creating namespaces admin-only that will only be accessible to admin users, and user2 that will be available to the user2 user we created earlier.
    Creating two cluster roles. These are RBAC permission sets that will be available everywhere on the cluster. It basically defines an admin policy that means you can do whatever, and an ro one that only allows looking at stuff.
    Creating a ClusterRoleBinding to grant anyone in the Vault group kubernetes-admin god access to anything, and a RoleBinding which is a namespaced-scoped equivalent to a ClusterRoleBinding to the user2 namespace.

Essentially in this setup, user1 is god, and user2 can only read stuff in the user2 namespace and nothing else.

All good ? All good ?

⚠️ if your brain is leaking right now, call 112, or something like 911 if you are not in the EU.

Now apply your brand new policies:

$ KUBECONFIG=kubeconfig kubectl apply -f rbac.yaml
namespace/admin-only created
namespace/user2 created
clusterrole.rbac.authorization.k8s.io/admin created
clusterrole.rbac.authorization.k8s.io/ro created
clusterrolebinding.rbac.authorization.k8s.io/admin created
rolebinding.rbac.authorization.k8s.io/admin created

Now lets put it to practice

You now need to be authenticated to vault to do stuff on the cluster (the admin kubeconfig will still work fine, but we want to be able to use Vault as our identity provider). We are going to use the exec part of the kubeconfig file for that. Essentially we are going to tell kubectl to ask for an authentication token to Vault before contacting the apiserver.

⚠️ when you inevitably break the kubeconfig, run kind get kubeconfig > kubeconfig to reset it.

So now, edit the kubeconfig file to add the following to the user section:

- name: vault
  user:
    exec:
      provideClusterInfo: true
      apiVersion: client.authentication.k8s.io/v1alpha1
      command: bash
      args:
        - -c
        - |
          #!/bin/bash

          # This script basically asks Vault for a token and fills out the
          # proper json that kubectl is expecting as a return

          set -euo pipefail
          tokenData=$(vault read -format json identity/oidc/token/k8s-token | jq -c .)
          cat <<EOF
          {
              "kind": "ExecCredential",
              "apiVersion": "client.authentication.k8s.io/v1alpha1",
              "spec": {},
              "status": {
              "expirationTimestamp": "$(date -d@$(( $(echo ${tokenData} | jq -r .data.ttl) + $(date +%s) )) +%FT%TZ)",
              "token": "$(echo ${tokenData} | jq -r .data.token)"
              }
          }
          EOF          

Now make sure you are logged in vault using the following command

VAULT_ADDR=http://localhost:8200 vault login -method=userpass username=user1 password=password1

I would also recommend exporting the VAULT_ADDR environment variable to make sure you authenticate to the right Vault server if like me you have your own

$ export VAULT_ADDR=http://localhost:8200

Do not forget to export the KUBECONFIG variable to point to the one you just edited

export KUBECONFIG=kubeconfig

Now run kubectl

$ kubectl get pods --user vault
No resources found in default namespace.

It does not seem like much, but it actually worked. You can check it works the same if you login as user2:

$ VAULT_ADDR=http://localhost:8200 vault login -method=userpass username=user2 password=password2
$ kubectl get pods --user vault
Error from server (Forbidden): pods is forbidden: User "vault:e1f134a8-81fd-94a6-440c-8012da4d1657" cannot list resource "pods" in API group "" in the namespace "default"

You got denied access because you do not belong in the right group. If you try again, but in namespace user2, you will see it works like a charm:

$ kubectl get pods --namespace user2 --user vault
No resources found in user2 namespace.

I guess our work here is done ?
Conclusion !

thumbs up

TADAM you managed to setup an OIDC authentication using Vault and its group system for your Kubernetes cluster!
** [[https://www.elvinefendi.com/2018/11/25/canary-deployment-with-ingress-nginx.html][Canary deployment with ingress-nginx]]

Note: Make sure you’re using ingress-nginx not older than version 0.22.0. The initial implementation of canary feature had serious flaws that got fixed in 0.22.0.

Canary and Blue/Green deployment, A/B testing are all known techniques to safely rollout a new version of service. Most of the times it’s required to configure them at load balancer level. https://thenewstack.io/deployment-strategies has done good job at explaining what they are.

In this article I’m going to show how ingress-nginx can be used to do canary deployments. But the feature ingress-nginx provides can also be used to do Blue/Green deployment as well as A/B testing.

In the version of 0.21.0 ingress-nginx introduces a new Canary feature that can be used to configure more than a single backend service for an ingress and few more annotations to also describe traffic distribution amongst the backend services. I’m going to show how to use that new feature with an example below. I’ll first deploy a service and configure it’s ingress. Then will deploy a new version of it under a different namesapce as canary. We will then use new ingress-nginx annotations to steer 10% of traffic to the canary backend. In the end of the article I’ll also talk about how to always force requests to be proxied by canary service using a HTTP header or cookie and briefly explain how this feature can be used for configuring Blue/Green deployment and A/B testing.

We are going to use echoserver service as an example app from https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/http-svc.yaml and minikube as local Kubernetes cluster.

Create production namespace:

> kubectl create ns echo-production
namespace "echo-production" created

Deploy the app:

> ~$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/docs/examples/http-svc.yaml -n echo-production
deployment.extensions "http-svc" created
service "http-svc" created

Here’s the raw version of manifest applied above:

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: http-svc
spec:
  replicas: 1
  selector:
    matchLabels:
      app: http-svc
  template:
    metadata:
      labels:
        app: http-svc
    spec:
      containers:
      - name: http-svc
        image: gcr.io/kubernetes-e2e-test-images/echoserver:2.1
        ports:
        - containerPort: 8080
        env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP

---

apiVersion: v1
kind: Service
metadata:
  name: http-svc
  labels:
    app: http-svc
spec:
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: http-svc

We will now expose the app using the below ingress manifest:

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: http-svc
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: echo.com
    http:
      paths:
      - backend:
          serviceName: http-svc
          servicePort: 80

After saving the above YAML in http-svc.ingress, we do following to create the ingress:

> ~$ kubectl apply -f http-svc.ingress -n echo-production
ingress.extensions "http-svc" created

We now have the production version of our echoserver service running:

> ~$ curl -H "Host: echo.com" http://192.168.99.100:31988


Hostname: http-svc-686644794d-hv4rt

Pod Information:
	node name:	minikube
	pod name:	http-svc-686644794d-hv4rt
	pod namespace:	echo-production
	pod IP:	172.17.0.2

Server values:
	server_version=nginx: 1.12.2 - lua: 10010

Request Information:
	client_address=172.17.0.9
	method=GET
	real path=/
	query=
	request_version=1.1
	request_scheme=http
	request_uri=http://echo.com:8080/

Request Headers:
	accept=*/*
	host=echo.com
	user-agent=curl/7.54.1
	x-forwarded-for=172.17.0.1
	x-forwarded-host=echo.com
	x-forwarded-port=80
	x-forwarded-proto=http
	x-original-uri=/
	x-real-ip=172.17.0.1
	x-request-id=8055cb031471bfe034f4beaee7d7302b
	x-scheme=http

Request Body:
	-no body in request-

http://192.168.99.100:31988 is obtained using:

> ~$ minikube service list

Now that we have production app running, I’m going to deploy canary version of the same service:

Create namesapce for the canary deployment:

> ~$ kubectl create ns echo-canary
namespace "echo-canary" created

Deploy the app under this namespace (in reality this should be a different - canary version of echoserver service, but for simplicity I’m deploying the exact app):

> ~$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/docs/examples/http-svc.yaml -n echo-canary
deployment.extensions "http-svc" created
service "http-svc" created

Now comes the moment! This is the step where we use Canary feature of ingress-nginx and configure load balancer to proxy 10% of traffic to the canary backend deployed above:

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: http-svc
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "10"
spec:
  rules:
  - host: echo.com
    http:
      paths:
      - backend:
          serviceName: http-svc
          servicePort: 80

Create http-svc.ingress.canary with the above content and apply:

> ~$ kubectl apply -f http-svc.ingress.canary -n echo-canary
ingress.extensions "http-svc" created

nginx.ingress.kubernetes.io/canary: "true" tells ingress-nginx to treat this ingress differently and mark it as “canary”. What this means internall is the controller is not going to try to configure a new Nginx virtual host for this ingress as it would normally do. Instead it will find the main ingress for this using host and path pair and associate this with the main ingress.

As you can guess nginx.ingress.kubernetes.io/canary-weight: "10" is what tells the ingress-nginx to configure Nginx in a way that it proxies 10% of total requests destined to echo.com to echoserver service under echo-canary namespace, a.k.a to the canary version of our app.

Now let’s see whether it works as expected. To test this I’m goig to run a small script that sends 1000 requests and then will look at pod namespace in the response to count what percentage of total requests were processed by the canary version of our app.

Here’s the script:

counts = Hash.new(0)

1000.times do
  output = `curl -s -H "Host: echo.com" http://192.168.99.100:31988 | grep 'pod namespace'`
  counts[output.strip.split.last] += 1
end

puts counts

And here are the outputs of two executions:

> ~$ ruby count.rb
{"echo-production"=>896, "echo-canary"=>104}
> ~$ ruby count.rb
{"echo-production"=>882, "echo-canary"=>118}

As you can see it worked as expected! Now if I edit the canary ingress and bump the weight up to 50:

> ~$ ruby count.rb
{"echo-canary"=>485, "echo-production"=>515}
> ~$ ruby count.rb
{"echo-production"=>494, "echo-canary"=>506}

🎉

Note that weight is not a precise percentage, it’s rather a probability at which a given request will be proxied to the canary backend service. Therefore in the above tests we see only approximate distributions.
But how can I always manually force my request to hit canary backend service?

This can be useful for example when developers want to tophat their change. ingress-nginx provides two annotations for this: nginx.ingress.kubernetes.io/canary-by-header and nginx.ingress.kubernetes.io/canary-by-cookie. In this article I’m going to show how it’s done using nginx.ingress.kubernetes.io/canary-by-header. To do so we need to edit the canary ingress and add following annotation:

nginx.ingress.kubernetes.io/canary-by-header: "you-can-use-anything-here"

Then when sending the request pass HTTP header you-can-use-anything-here set to always:

> ~$ curl -s -H "you-can-use-anything-here: always" -H "Host: echo.com" http://192.168.99.100:31988 | grep 'pod namespace'
	pod namespace:	echo-canary

If you want your request to be never proxied to the canary backend service then you can set the header to never:

> ~$ curl -s -H "you-can-use-anything-here: never" -H "Host: echo.com" http://192.168.99.100:31988 | grep 'pod namespace'
	pod namespace:	echo-production

When the value is absent or set anything other than never, or always then proxying by weight will be used. This works exactly the same for nginx.ingress.kubernetes.io/canary-by-cookie. Note that canary-by-header takes presedence over canary-by-cookie. In general the order of application of the rules is: canary-by-header > canary-by-cookie > canary-weight.
Blue/Green deployment

This can be achieved by setting the weight to 0 or 100. For example you can make your green deployment as main and configure blue deployment’s ingress to be canary. Initially you’ll set the weight to 0 so no traffic will be proxied to blue deployment. Once the new version of your app is ready, you deploy it as blue deployment and only after that set the weight to 100, which will result in steering all traffic away from green deployment to blue. And for the next deployment you can again deploy green and switch the weight back to 0 etc.
A/B testing

This can be achieved using mix of business logic implemented in the backend service and cookie-by-cookie annotation. For example let’s say you want to show the new version of your app to only users under 30 years old. You can then configure

nginx.ingress.kubernetes.io/canary-by-cookie: "use_under_30_feature"

and implement a simple change in your backend service that for a given requests checks the age of signed-in user and if the age is under 30 sets cookie use_under_30_feature to always. This will then make sure all subsequent requests by those users will be proxied by the new version of the app.

** [[https://support.huaweicloud.com/intl/en-us/bestpractice-cce/cce_bestpractice_10003.html][Using Nginx Ingress to Implement Grayscale Release and Blue-Green Deployment]]
Updated on 2023-08-07 GMT+08:00
View PDF

This section describes the scenarios and practices of using Nginx Ingress to implement grayscale release and blue-green deployment.
Scenario

Nginx Ingress supports three traffic division policies based on the header, cookie, and service weight. Based on these policies, the following two release scenarios can be implemented:

    Scenario 1: Split some user traffic to the new version.

    Assume that Service A that provides layer-7 networking is running. A new version is ready to go online, but you do not want to replace the original Service A. You want to forward the user requests whose header or cookie contains foo=bar to the new version of Service A. After the new version runs stably for a period of time, you can gradually bring the new version online and smoothly bring the old version offline. The following figure shows the process:

    Scenario 2: Split a certain proportion of traffic to the new version.

    Assume that Service B that provides layer-7 services is running. After some problems are resolved, a new version of Service B needs to be released. However, you do not want to replace the original Service B. Instead, you want to switch 20% traffic to the new version of Service B. After the new version runs stably for a period of time, you can switch all traffic from the old version to the new version and smoothly bring the old version offline.

Annotations

Nginx Ingress supports release and testing in different scenarios by configuring annotations for grayscale release, blue-green deployment, and A/B testing. The implementation process is as follows: Create two ingresses for the service. One is a common ingress, and the other is an ingress with the annotation nginx.ingress.kubernetes.io/canary: "true", which is called a canary ingress. Configure a traffic division policy for the canary ingress. The two ingresses cooperate with each other to implement release and testing in multiple scenarios. The annotation of Nginx Ingress supports the following rules:

    nginx.ingress.kubernetes.io/canary-by-header

    Header-based traffic division, which is applicable to grayscale release. If the request header contains the specified header name and the value is always, the request is forwarded to the backend service defined by the canary ingress. If the value is never, the request is not forwarded and a rollback to the source version can be performed. If other values are used, the annotation is ignored and the request traffic is allocated according to other rules based on the priority.
    nginx.ingress.kubernetes.io/canary-by-header-value

    This rule must be used together with canary-by-header. You can customize the value of the request header, including but not limited to always or never. If the value of the request header matches the specified custom value, the request is forwarded to the corresponding backend service defined by the canary ingress. If the values do not match, the annotation is ignored and the request traffic is allocated according to other rules based on the priority.
    nginx.ingress.kubernetes.io/canary-by-header-pattern

    This rule is similar to canary-by-header-value. The only difference is that this annotation uses a regular expression, not a fixed value, to match the value of the request header. If this annotation and canary-by-header-value exist at the same time, this one will be ignored.
    nginx.ingress.kubernetes.io/canary-by-cookie

    Cookie-based traffic division, which is applicable to grayscale release. Similar to canary-by-header, this annotation is used for cookies. Only always and never are supported, and the value cannot be customized.
    nginx.ingress.kubernetes.io/canary-weight

    Traffic is divided based on service weights, which is applicable to blue-green deployment. This annotation indicates the percentage of traffic allocated by the canary ingress. The value ranges from 0 to 100. For example, if the value is set to 100, all traffic is forwarded to the backend service backing the canary ingress.

NOTE:

    The preceding annotation rules are evaluated based on the priority. The priority is as follows: canary-by-header -> canary-by-cookie -> canary-weight.
    When an ingress is marked as a canary ingress, all non-canary annotations except nginx.ingress.kubernetes.io/load-balance and nginx.ingress.kubernetes.io/upstream-hash-by are ignored.
    For more information, see Annotations.

Prerequisites

    To use Nginx Ingress to implement grayscale release of a cluster, you need to install the nginx-ingress add-on as the Ingress Controller and expose a unified traffic entrance externally. For details, see Installing the Add-on.
    The Nginx image has been uploaded to SWR. The Nginx images have two versions. The welcome pages are Old Nginx and New Nginx.

Resource Creation

You can use YAML to deploy Deployments and Services in either of the following ways:

    On the Create Deployment page, click Create YAML on the right and edit the YAML file in the window.
    Save the sample YAML file in this section as a file and use kubectl to specify the YAML file. For example, run the kubectl create -f xxx.yaml command.

Step 1: Deploy Services of Two Versions

Two versions of Nginx are deployed in the cluster, and Nginx Ingress is used to provide layer-7 domain name access for external systems.

    Create a Deployment and Service for the first version. This section uses old-nginx as an example. Example YAML:

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: old-nginx
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: old-nginx
      template:
        metadata:
          labels:
            app: old-nginx
        spec:
          containers:
          - image: {your_repository}/nginx:old  # The image used by the container is nginx:old.
            name: container-0
            resources:
              limits:
                cpu: 100m
                memory: 200Mi
              requests:
                cpu: 100m
                memory: 200Mi
          imagePullSecrets:
          - name: default-secret

    ---

    apiVersion: v1
    kind: Service
    metadata:
      name: old-nginx
    spec:
      selector:
        app: old-nginx
      ports:
      - name: service0
        targetPort: 80
        port: 8080
        protocol: TCP
      type: NodePort

Create a Deployment and Service for the second version. This section uses new-nginx as an example. Example YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: new-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: new-nginx
  template:
    metadata:
      labels:
        app: new-nginx
    spec:
      containers:
      - image: {your_repository}/nginx:new  # The image used by the container is nginx:new.
        name: container-0
        resources:
          limits:
            cpu: 100m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
      imagePullSecrets:
      - name: default-secret

---

apiVersion: v1
kind: Service
metadata:
  name: new-nginx
spec:
  selector:
    app: new-nginx
  ports:
  - name: service0
    targetPort: 80
    port: 8080
    protocol: TCP
  type: NodePort

You can log in to the CCE console to view the deployment status.

Create an ingress to expose the service and point to the service of the old version. Example YAML:

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: gray-release
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx # Use the Nginx ingress.
    kubernetes.io/elb.port: '80'
spec:
  rules:
    - host: www.example.com
      http:
        paths:
          - path: '/'
            backend:
              serviceName: old-nginx      # Specify old-nginx as the backend service.
              servicePort: 80

Run the following command to verify the access:

curl -H "Host: www.example.com"  http://<EXTERNAL_IP>

In the preceding command, <EXTERNAL_IP> indicates the external IP address of the Nginx ingress.

Expected outputs:

Old Nginx

Step 2: Launch the New Version of the Service in Grayscale Release Mode

Set the traffic division policy for the service of the new version. CCE supports the following policies for grayscale release and blue-green deployment:

Header-based, cookie-based, and weight-based traffic division rules

Grayscale release can be implemented based on all these policies. Blue-green deployment can be implemented by adjusting the new service weight to 100%. For details, see the following examples.
CAUTION:

Pay attention to the following:

    Only one canary ingress can be defined for the same service so that the backend service supports a maximum of two versions.
    Even if the traffic is completely switched to the canary ingress, the old version service must still exist. Otherwise, an error is reported.

    Header-based rules

    In the following example, only the request whose header contains Region set to bj or gz can be forwarded to the service of the new version.
        Create a canary ingress, set the backend service to the one of the new versions, and add annotations.

        apiVersion: networking.k8s.io/v1beta1
        kind: Ingress
        metadata:
          name: canary-ingress
          namespace: default
          annotations:
            kubernetes.io/ingress.class: nginx
            nginx.ingress.kubernetes.io/canary: "true"                       # Enable canary.
            nginx.ingress.kubernetes.io/canary-by-header: "Region"
            nginx.ingress.kubernetes.io/canary-by-header-pattern: "bj|gz"    # Requests whose header contains Region with the value bj or gz are forwarded to the canary ingress.
            kubernetes.io/elb.port: '80'
        spec:
          rules:
            - host: www.example.com
              http:
                paths:
                  - path: '/'
                    backend:
                      serviceName: new-nginx      # Specify new-nginx as the backend service.
                      servicePort: 80

Run the following command to test the access:

$ curl -H "Host: www.example.com" -H "Region: bj" http://<EXTERNAL_IP>
New Nginx
$ curl -H "Host: www.example.com" -H "Region: sh" http://<EXTERNAL_IP>
Old Nginx
$ curl -H "Host: www.example.com" -H "Region: gz" http://<EXTERNAL_IP>
New Nginx
$ curl -H "Host: www.example.com" http://<EXTERNAL_IP>
Old Nginx

    In the preceding command, <EXTERNAL_IP> indicates the external IP address of the Nginx ingress.

    Only requests whose header contains Region with the value bj or gz are responded by the service of the new version.

Cookie-based rules

In the following example, only the request whose cookie contains user_from_bj can be forwarded to the service of the new version.

    Create a canary ingress, set the backend service to the one of the new versions, and add annotations.
    NOTE:

    If you have created a canary ingress in the preceding steps, delete it and then perform this step to create a canary ingress.

    apiVersion: networking.k8s.io/v1beta1
    kind: Ingress
    metadata:
      name: canary-ingress
      namespace: default
      annotations:
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/canary: "true"                       # Enable canary.
        nginx.ingress.kubernetes.io/canary-by-cookie: "user_from_bj"    # Requests whose cookie contains user_from_bj are forwarded to the canary ingress.
        kubernetes.io/elb.port: '80'
    spec:
      rules:
        - host: www.example.com
          http:
            paths:
              - path: '/'
                backend:
                  serviceName: new-nginx      # Specify new-nginx as the backend service.
                  servicePort: 80

Run the following command to test the access:

$ curl -s -H "Host: www.example.com" --cookie "user_from_bj=always" http://<EXTERNAL_IP>
New Nginx
$ curl -s -H "Host: www.example.com" --cookie "user_from_gz=always" http://<EXTERNAL_IP>
Old Nginx
$ curl -s -H "Host: www.example.com" http://<EXTERNAL_IP>
Old Nginx

    In the preceding command, <EXTERNAL_IP> indicates the external IP address of the Nginx ingress.

    Only requests whose cookie contains user_from_bj with the value always are responded by the service of the new version.

Service weight-based rules

Example 1: Only 20% of the traffic is allowed to be forwarded to the service of the new version to implement grayscale release.

    Create a canary ingress and add annotations to import 20% of the traffic to the backend service of the new version.
    NOTE:

    If you have created a canary ingress in the preceding steps, delete it and then perform this step to create a canary ingress.

    apiVersion: networking.k8s.io/v1beta1
    kind: Ingress
    metadata:
      name: canary-ingress
      namespace: default
      annotations:
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/canary: "true"         # Enable canary.
        nginx.ingress.kubernetes.io/canary-weight: "20"    # Forward 20% of the traffic to the canary ingress.
        kubernetes.io/elb.port: '80'
    spec:
      rules:
        - host: www.example.com
          http:
            paths:
              - path: '/'
                backend:
                  serviceName: new-nginx      # Specify new-nginx as the backend service.
                  servicePort: 80

Run the following command to test the access:

$ for i in {1..20}; do curl -H "Host: www.example.com" http://<EXTERNAL_IP>; done;
Old Nginx
Old Nginx
Old Nginx
New Nginx
Old Nginx
New Nginx
Old Nginx
New Nginx
Old Nginx
Old Nginx
Old Nginx
Old Nginx
Old Nginx
New Nginx
Old Nginx
Old Nginx
Old Nginx
Old Nginx
Old Nginx
Old Nginx

    In the preceding command, <EXTERNAL_IP> indicates the external IP address of the Nginx ingress.

    It can be seen that there is a 4/20 probability that the service of the new version responds, which complies with the setting of the service weight of 20%.
    NOTE:

    After traffic is divided based on the weight (20%), the probability of accessing the new version is close to 20%. The traffic ratio may fluctuate within a small range, which is normal.

Example 2: Allow all traffic to be forwarded to the service of the new version to implement blue-green deployment.

    Create a canary ingress and add annotations to import 100% of the traffic to the backend service of the new version.
    NOTE:

    If you have created a canary ingress in the preceding steps, delete it and then perform this step to create a canary ingress.

    apiVersion: networking.k8s.io/v1beta1
    kind: Ingress
    metadata:
      name: canary-ingress
      namespace: default
      annotations:
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/canary: "true"          # Enable canary.
        nginx.ingress.kubernetes.io/canary-weight: "100"    # All traffic is forwarded to the canary ingress.
        kubernetes.io/elb.port: '80'
    spec:
      rules:
        - host: www.example.com
          http:
            paths:
              - path: '/'
                backend:
                  serviceName: new-nginx      # Specify new-nginx as the backend service.
                  servicePort: 80

Run the following command to test the access:

$ for i in {1..10}; do curl -H "Host: www.example.com" http://<EXTERNAL_IP>; done;
New Nginx
New Nginx
New Nginx
New Nginx
New Nginx
New Nginx
New Nginx
New Nginx
New Nginx
New Nginx

In the preceding command, <EXTERNAL_IP> indicates the external IP address of the Nginx ingress.

All access requests are responded by the service of the new version, and the blue-green deployment is successfully implemented.

** [[https://discuss.kubernetes.io/t/high-availability-host-numbers/13143][High Availability Host Numbers - General Discussions - Discuss Kubernetes]]
K8s uses the RAFT consensus algorithm for quorum. In order to maintain quorum, you will need floor(n/2)+1 healthy master nodes.

Practicaly this means:

    1 master node: you will require 1 healthy master node for quorum, the loss of the master node will render the cluster headless.
    2 master nodes: you will require 2 healthy master nodes for quorum, the loss of either master node will render the cluster headless.
    3 master nodes: you will require 2 healty master nodes for quorum, the loss of one of the master nodes can be compensated.
    4 master nodes: you will require 3 healty master nodes for quorum, the loss of one on the master nodes can be compensated. A setup with 4 master nodes has no advantage over a 3 master nodes setup.
    5 master nodes: you will require 3 healthy master nodes for quorum, the loss of up to two master nodes can be compsensated.
    6 master nodes: you will require 4 healty master nodes for quorum, the loss of up to two master nodes can be compensated. No advantage compared to 5 master nodes.
    7 master nodes: you will require 4 healthy master nodes for quorum, the loss of up to three master nodes can be compsensated.

This is the reason why it is recommended to use an odd number of master nodes for the control plane. More then 7 master nodes will result in a overhead for determining cluster membership and quorum, it is not recommended. Depending on your needs, you typically end up with 3 or 5 master nodes.

** [[https://shivering-isles.com/2022/05/mirroring-your-cluster-images][Mirroring your cluster images]]
Use [[https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/][this snippet from the Kubernetes manual]] which will give you all images currently running in your cluster and check the registries used:
#+begin_src bash
  kubectl get pods --all-namespaces -o jsonpath="{.items[*].spec.containers[*].image}" | tr -s '[[:space:]]' '\n' | sort | uniq -c
#+end_src

Alternatively, you can ask your local Prometheus instance for the images used in the cluster using following query:
#+begin_example
  count(label_replace(kube_pod_container_info, "image", "$1", "image", "([^/]+)/.*")) by (image)
#+end_example
** [[https://scribe.rip/secure-kubernetes-clusters-with-oauth2-openid-connect-and-cidaas-ffc58b7274b7][Secure Kubernetes clusters with OAuth2/OpenID Connect and cidaas]]
This article focuses on securing a Kubernetes cluster with OAuth2 and OpenID Connect using cidaas Cloud Identity & Access Management.

cidaas (https://www.cidaas.com) is a modern Cloud Identity & Access Management solution developed by Widas ID GmbH located in Germany. The cidaas feature set includes:

    Single Sign On (SSO) based on OAuth2, OpenID Connect, SAML 2.0
    Multi-Factor-Authentication (MFA) with more than 14 authentication methods, including TOTP and FIDO2
    password-less authentication
    social login (e.g. Facebook, Google, LinkedIn, and more) as well as enterprise identity provider (e.g. SAML or AD)
    security in Machine-to-Machine communications (M2M) and IoT

A short introduction on OAuth2, OpenID Connect and Kubernetes

OAuth2 is the industry-standard protocol for authorization, and it provides different authorization flows depending on the application or client type, be it for example web or mobile applications. If you want to learn more about OAuth2 have a look at the official website at https://oauth.net/2/.

OpenID Connect (OIDC) is a standard authentication protocol that adds an identification layer to the previous OAuth 2.0 standard. OIDC provides information to the end user in the form of an id token, through which the identity is verified. The standard is controlled by the OpenID Foundation (https://openid.net).

Kubernetes is an open-source orchestration platform for the management of containerized applications. It offers many features for an automated and stable operation of modern software architectures, including automated deployments and scaling. (https://kubernetes.io)
Kubernetes OpenID Connect Tokens

The OpenID Connect Token (Kubernetes Docs) feature of Kubernetes allows an out-of-the-box integration with cidaas or any other identity provider supporting the OpenID Connect protocol.

The integration of cidaas requires two steps of configuration:

First, the configuration of the API Server

    oidc parameters
    Kubespray
    Kops
    Kubeadm

Second, the configuration of the Kubernetes Clients

    Kubectl
    Kubernetes Dashboard

Kubernetes API Server Configuration

We will show 4 different ways to configure the API Server. First we will discuss the plain configuration of the API Server, additionally we will show how a configuration can be achieved with commonly used orchestration solutions Kubespray, Kops, and Kubeadm.

Plain Configuration via oidc parameters

The OpenID Connect Feature of Kubernetes provides several parameter options, which need to be passed while starting the Kubernetes API Server. Two of these are required:

— oidc-issuer-url and — oidc-client-id

These are also the only ones required for an integration with cidaas. Pass the cidaas issuer URL (the base URL of your cidaas instance) and the client ID of your cidaas application. If you need some information how to create an app (OAuth2 / OpenID Connect Client) in cidaas please refer to the cidaas documentation: docs.cidaas.com

Kubespray

If you are using Kubespray to configure your Kubernetes Cluster, You can add following configuration to `group_vars/k8s_cluster/k8s-cluster.yml` file :

k8s-cluster.yml

kube_oidc_url: https://{yourtenant}.cidaas.de 
kube_oidc_client_id: xxxx-xxxx-xxxx-xxxx-xxx 
## Optional settings for OIDC 
# kube_oidc_ca_file: "{{ kube_cert_dir }}/ca.pem" 
kube_oidc_username_claim: sub 
kube_oidc_username_prefix: cidaas_oidc: 
kube_oidc_groups_claim: roles 
kube_oidc_groups_prefix: "cidaas_oidc:" 

Kops

If you are using kops to configure your Kubernetes Cluster, you can add following configuration to cluster manifest file as explained here :

kops.yml

spec: 
  kubeAPIServer: 
    oidcIssuerURL: https://your-oidc-provider.svc.cluster.local 
    oidcClientID: kubernetes 
    oidcUsernameClaim: sub 
    oidcUsernamePrefix: "oidc:" 
    oidcGroupsClaim: user_roles 
    oidcGroupsPrefix: "oidc:" 
    oidcCAFile: /etc/kubernetes/ssl/kc-ca.pem 
    oidcRequiredClaim: 
    	- "key=value" 

Kubeadm

Finally, if you are using kubeadm to configure cluster add following files to cluster manifest file:

kubeadm.yml

apiVersion: kubeadm.k8s.io/v1beta2 
kind: ClusterConfiguration 
apiServer: 
  extraArgs: 
    oidc-issuer-url: https://{tenant}.cidaas.de 
    oidc-username-claim: sub 
    oidc-client-id: kubernetes-cluster 
    oidc-ca-file: /root/myCA.pem 

Kubernetes Clients Configuration

After we have configured the Kubernetes API Server, we now need to have a closer look at the Kubernetes clients, in particular the user configuration in $HOME/.kube/config used by kubectl.
Kubectl

kubectl is a commandline application used to run commands and apply configuration on the Kubernetes cluster .

There are multiple ways to configure kubectl to integrate cidaas for authentication for example OIDC Authenticator or kubelogin.

Both work perfectly well with cidaas, below you will find a short description for both options.

OIDC Authenticator:

kubectl.yml

kubectl config set-credentials USER_NAME \ 
   --auth-provider=oidc \ 
   --auth-provider-arg=idp-issuer-url=( issuer url ) \ 
   --auth-provider-arg=client-id=( your client id ) \ 
   --auth-provider-arg=client-secret=( your client secret ) \ 
   --auth-provider-arg=refresh-token=( your refresh token ) \ 
   --auth-provider-arg=idp-certificate-authority=( path to your ca certificate ) \ 
   --auth-provider-arg=id-token=( your id_token ) 

Kubelogin:

You can install kubelogin via following commands

# Homebrew (macOS and Linux)

brew install int128/kubelogin/kubelogin

# Krew (macOS, Linux, Windows and ARM)

kubectl krew install oidc-login

# Chocolatey (Windows)

choco install kubelogin

You can use following command to setup kubelogin to use cidaas

kubectl oidc-login setup --oidc-issuer-url {{ issuer_url}}--oidc-client-id {{ clientid }} --oidc-client-secret {{ clientsecret}}

Optional: Kubernetes Dashboard

When operating a Kubernetes Cluster a Kubernetes Dashboard is often used to manage the cluster and its services/applications. To secure the Kubernetes Dashboard we will use OAuth2-proxy. In the scenario shown below, the kubernetes-dashboard is running in a separate namespace called “kubernetes-dashboard” namespace and will be exposed on the URL kube-dashboard.cluster.de. To achieve this, the following configuration steps need to be executed:

    Deploy OAuth2-proxy
    Deploy the Kubernetes Ingress for the Kubernetes Dashboard

The OAuth2-proxy can be deployed using below configuration via kubectl:

kube-oauth2-proxy.yml

apiVersion: apps/v1 
kind: Deployment 
metadata: 
  labels: 
    k8s-app: oauth2-proxy 
  name: oauth2-proxy 
spec: 
  replicas: 1 
  selector: 
    matchLabels: 
      k8s-app: oauth2-proxy 
  template: 
    metadata: 
      labels: 
        k8s-app: oauth2-proxy 
    spec: 
      containers: 
        - args: 
            - --cookie-secure=false 
            - --upstream=file:///dev/null 
            - --http-address=0.0.0.0:4180 
            - --cookie-secret=cnlIM0xwNnFXY1M5UmMvOGwyUDMxUT09 
            - --client-id=xxxx-xxx-xxx-xxx 
            - --client-secret=xxxx-xxx-xxx-xxx 
            - --email-domain=* 
            - --provider=oidc 
            - --oidc-issuer-url=https://{tenant}.cidaas.de 
          image: quay.io/pusher/oauth2_proxy:v5.1.0 
          imagePullPolicy: Always 
          name: oauth2-proxy 
          ports: 
            - containerPort: 4180 
              protocol: TCP 
--- 
kind: Service 
apiVersion: v1 
metadata: 
  name: oauth2-proxy 
  namespace: kubernetes-dashboard 
spec: 
  ports: 
    - name: http 
      protocol: TCP 
      port: 4180 
      targetPort: 4180 
  selector: 
    k8s-app: oauth2-proxy 
  type: ClusterIP 
--- 
kind: Ingress 
apiVersion: extensions/v1beta1 
metadata: 
  name: oauth2-proxy 
  namespace: kubernetes-dashboard 
  annotations: 
    nginx.ingress.kubernetes.io/ingress.class: nginx 
    nginx.ingress.kubernetes.io/proxy-buffer-size: 16k 
spec: 
  tls: 
    - hosts: 
        - kube-dashboard.cluster.de 
      secretName: wildcard.cluster.de 
  rules: 
    - host: kube-dashboard.cluster.de 
      http: 
        paths: 
          - path: /oauth2 
            backend: 
              serviceName: oauth2-proxy 
              servicePort: 4180 

The Kubernetes Ingress can be deployed using below configuration:

kube-ingress.yml

kind: Ingress 
apiVersion: extensions/v1beta1 
metadata: 
  name: kubernetes-dashboard-ingress 
  namespace: kubernetes-dashboard 
  annotations: 
    nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=https://$host$request_uri$is_args$args" 
    nginx.ingress.kubernetes.io/auth-url: "https://$host/oauth2/auth" 
    nginx.ingress.kubernetes.io/backend-protocol: HTTPS 
    nginx.ingress.kubernetes.io/configuration-snippet: | 
      auth_request_set $token $upstream_http_authorization; 
      proxy_set_header Authorization $token; 
    nginx.ingress.kubernetes.io/ingress.class: nginx 
    nginx.ingress.kubernetes.io/proxy-buffer-size: 16k 
spec: 
  tls: 
    - hosts: 
        - kube-dashboard.cluster.de 
      secretName: wildcard.cluster.de 
  rules: 
    - host: kube-dashboard.cluster.de 
      http: 
        paths: 
          - backend: 
              serviceName: kubernetes-dashboard 
              servicePort: 443 

Access Management (RBAC — Role-based Access Control)

Basic login access to Kubernetes using cidaas is now configured. Now we will have a look at access management, in particular how to manage individual user access to Kubernetes. Kubernetes is using role-based access control (RBAC), which can be integrated with the cidaas role management using Role/CusterRoles and RoleBinding/ClusterRoleBinding.

Let’s assume an easy user role setup:

    users with the KUBE_ADMIN role have full Kubernetes cluster access
    users with the KUBE_READ role have limited read-only access to the Kubernetes cluster

For KUBE_ADMIN role we create a new ClusterRoleBinding and refer to an existing role of the Kubernetes cluster using a “roleRef” to the role “cluster-admin”. This ClusterRoleBinding is the mapping between the Kubernetes ClusterRole (cluster-admin) and the cidaas role (KUBE_ADMIN). The ClusterRoleBinding in the Kubernetes config for the KUBE_ADMIN role will look like this:

kube-rolebinding.yml

# Admin Role setup 
apiVersion: rbac.authorization.k8s.io/v1 
kind: ClusterRoleBinding 
metadata: 
  name: oidc-cluster-admin 
roleRef: 
  apiGroup: rbac.authorization.k8s.io 
  kind: ClusterRole 
  name: cluster-admin 
subjects: 
- apiGroup: rbac.authorization.k8s.io 
  kind: Group 
  name: "cidaas_oidc:KUBE_ADMIN" 

Since there is no existing read role in the Kubernetes default setup, we create a new ClusterRole named “read-only-cluster-role” and define the access of this ClusterRole via the rules section. So in our use case the ClusterRole should be

    only allowed to perform the actions “get”, “list” and “watch” (verbs)
    on all apis [“*”] (apiGroups) and
    all resources [“*”] (resource)

After we have defined the ClusterRole we do the same as we did for the KUBE_ADMIN mapping, we assign the ClusterRole via the ClusterRoleBinding to the KUBE_READ role in cidaas.

The ClusterRole and ClusterRoleBinding in the Kubernetes Config for the KUBE_READ role will look like this:

kube-clusterrole-binding.yml

# Admin Role setup 
#read-only-cluster-role 
apiVersion: rbac.authorization.k8s.io/v1 
kind: ClusterRole 
metadata: 
  name: read-only-cluster-role 
rules: 
- apiGroups: ["*"] 
  resources: ["*"] 
  verbs: ["get", "list", "watch"] 
 
 
# ClusterRoleBinding 
--- 
apiVersion: rbac.authorization.k8s.io/v1 
kind: ClusterRoleBinding 
metadata: 
  name: cluster-read-only-binding 
roleRef: 
  apiGroup: rbac.authorization.k8s.io 
  kind: ClusterRole 
  name: read-only-cluster-role 
subjects: 
- apiGroup: rbac.authorization.k8s.io 
  kind: Group 
  name: "cidaas_oidc:KUBE_READ" 

Now we can assign the roles KUBE_ADMIN or KUBE_READ to users in cidaas to assign the required access in Kubernetes to them. If you do not know how to create roles in cidaas you can find the documentation here: docs.cidaas.com

That’s it, we made it! We now integrated cidaas, successfully secured a Kubernetes cluster and are able to manage the access control to and in our Kubernetes cluster via the RBAC approach.

Hint: If you want to have a look what is in the token, we get back in the callback method, you can decode your Access Token easily with the https://authkit.io/ online jwt decoder.

Securing a Kubernetes Cluster and managing the users as well as their access rights is crucial for a secure and productive operation. With the OAuth2 and OpenID Connect Standards it becomes easy to achieve this. This example is based on cidaas — Cloud Identity & Access Management and the OpenID Connect Token feature of Kubernetes.

If you have any questions or suggestions, please leave a comment.
** The problem

This became obvious, when someone [[https://microblog.shivering-isles.com/@mhamzahkhan@intahnet.co.uk/111032533934904655][threw a question to =#mastoadmin=  about it]].

#+begin_quote


Is it possible to make =#mastodon= write its temporary files to somewhere else other than =/opt/mastodon= ?


#+end_quote

=/opt/mastodon=, for those unaware, is the working directory and application directory of Mastodon in the official Mastodon containers. Why would the app use the current working directory instead of =/tmp/=  for temporary files?

It turns out, it doesn’t. At least not by default and when the container filesystem is writeable. However, when adjusting the =securityContext= of the container to contain =readOnlyRootFilesystem: true= , media processing fails, as it tries to write temporary files.

No problem, that’s why [[https://kubernetes.io/docs/concepts/storage/volumes/#emptydir][=emptyDir=]] exist. Mounting an =emptyDir= volume to =/tmp=  should solve it, however, it doesn’t.

Instead of using =/tmp=, suddenly mastodon tries to write its temporary files to =/opt/mastodon= or rather the current working directory. Somehow there was a fallback to the current working directory, somewhere in the stack and it was triggered. Diving down into the stack, it became apparent, that it’s actually just the =paperclip=  dependency that causes the issue, and even further down, it turns out, it’s Ruby itself.

It turns out, Ruby assesses temp directories for suitability before using them:

[[https://github.com/ruby/ruby/blob/16882d4ebbb157515f95b2b3280d78dfb64ab8f7/lib/tmpdir.rb#L36-L42][https://shivering-isles.com/assets/images/2023/09/ruby-tempdir-check.png]]

One of the criteria is that it’s world writeable with a [[https://www.redhat.com/sysadmin/suid-sgid-sticky-bit][sticky bit]] or not world writeable at all. And here =emptyDir= comes back into play. =emptyDirs=  in Kubernetes are creating world writeable directories without sticky bit and therefore this check fails and ruby falls back to its fallback directory, the current working directory.

So in Ruby =emptyDir=  is unsuitable for temporary files created using the native API.

** The solution

Today I scrolled through =#mastoadmin= and came back across the conversation about the problem. And today, I knew how to solve it! Meet [[https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes][Generic Ephemeral Volumes]]!

Generic Ephemeral Volumes are Kubernetes Persistent Volume Claims, that only exist for the lifetime of a Pod and are only attached to this Pod. PVCs have the nice benefit, that other than =emptyDir= they aren’t mounted with world-writeable permissions, but rather just your regular =fsGroup=  shenanigans.

So the solution to the problem of =emptyDir= being unsuitable for ruby temporary files is to use generic ephemeral volumes, to create PVCs that can be mounted under =/tmp= and work like a =/tmp= directory. Be aware, they do not share the exact attributes and capability of your regular =/tmp=  directory, but for this use-case, they are good enough.

PS: If you are curious about how I’m running Mastodon, you can have a look at the [[https://artifacthub.io/packages/helm/si-gitops/mastodon][Helm Chart]].

-----

** [[https://scribe.citizen4.eu/using-vault-as-an-openid-connect-identity-provider-ee0aaef2bba2][Using Vault as an OpenID Connect Identity Provider]]

Brian Candler on 2021-12-10

In a previous article I wrote about using Hashicorp Vault as an SSH certificate authority. As of version 1.9, Vault has gained the ability to act as an OpenID Connect Identity Provider as well.

This is an exciting development which greatly increases the scope and utility of Vault. The feature is in “Technical Preview” so may be subject to change, but I can confirm that it works in its current state.
What this means

You can now configure your local web applications (and some other systems like Kubernetes) to perform single sign-on authentication against Vault — as long as they support OpenID Connect (OIDC).

The user experience is:

    The user connects to a web app.
    The web app redirects them to Vault’s web interface, where they can choose from available upstream identity providers (IDPs), and login with that provider. If the user has previously authenticated to Vault (i.e. has a cookie to prove it) then this step completes automatically with no further interaction.
    The user is redirected back to the web app, and Vault sends a token back to the application which proves that the user is who they say they are.
    The web app accepts the user and continues as normal

Compared to making those applications authenticate directly against Google or Office365 or whatever, there is a great amount of flexibility:

    You can add and remove client applications without having to making any changes at the upstream IDPs; and you can add and remove IDPs without touching the client applications.
    You can choose which upstream IDP account(s) each user can authenticate with.
    You have full control over group membership. You can create and manage groups manually, by adding individual users (entities); and/or you can honour group claims from the upstream identity platforms.
    You have full control over authorization. You can add claims to individual users via entity metadata (regardless of which upstream account they authenticated from)
    You can control the names and format of the OpenID claims which are sent to your applications. This is important, since some applications have fixed requirements.

Competing alternatives

Apart from the commercial cloud services like Okta and Auth0, I think the software which comes closest is Keycloak. This is a mature open source project which has the weight of Red Hat behind it, and has a wider range of SSO functionality (e.g. it can do SAML). I also find its web interface is much clearer for managing users, groups and clients than Vault’s rather technical web UI. Vault’s CLI is even lower level, only a thin wrapper around the API.

However, Vault has its own advantages:

    Vault has a small resource footprint, written in Go rather than Java
    Vault has a wide range of storage backends, from simple file storage and robust HA raft implementation, to SQL and noSQL databases
    Vault has a very good story around data security and encryption. Regardless of which backend you use, all the secret data is encrypted with the master (“unseal”) key. Whenever you restart Vault, you have to provide a subset of the unseal key shards; the contents of the database by itself are not usable to an attacker.
    Vault can provide other identity services, in particular issuing SSH certificates and X509 certificates
    Vault also provides its original core functionality of API-based “secret store” for managing credentials used by applications to communicate with each other

(If you are feeling more conservative, you could use Vault for SSH certificates whilst using Keycloak as its upstream IDP)
Proof-of-concept

Now I’ll go through the steps I did to demonstrate this in action.
Install and configure Vault

I already had an existing installation of Vault, running with HTTPS using a LetsEncrypt certificate (in my case using dehydrated). Having a real certificate is strongly recommended, since each of your users will need to interact directly with Vault’s web interface to login.

Here is my /etc/vault/vault-conf.hcl with the domain name changed:

# https://www.vaultproject.io/docs/configuration/storage/filesystem
storage "file" {
  path = "/var/lib/vault"
}

listener "tcp" {
  address = "[::]:8200"
  tls_cert_file = "/etc/dehydrated/certs/vault1.example.net/fullchain.pem"
  tls_key_file = "/etc/dehydrated/certs/vault1.example.net/privkey.pem"
}

cluster_addr = "https://vault1.example.net:8201"
api_addr = "https://vault1.example.net:8200"
ui = "true"

# The following setting is not recommended, but you may need
# it when running in an unprivileged lxd container
disable_mlock = "true"

For convenience I also have /etc/profile.d/vault.sh:

VAULT_ADDR=https://vault1.example.net:8200
export VAULT_ADDR

To make the config changes below, you’ll need to be able to authenticate to Vault with sufficient privileges, e.g. with a root token.

    Aside: in a production environment, you would avoid using root tokens if possible. Instead you might define a vaultadmin group with rights to update policies, and authenticate as a member of that group.

Upgrade Vault to v1.9+

If you’ve installed a fresh Vault v1.9, skip this section.

Otherwise, you need to upgrade Vault to v1.9+ before continuing. Choose the latest available v1.9.x release. Restart and unseal.

You will also need to replace the “default” policy, inherited by all users unless you specify otherwise, to allow users to access the OIDC authorization endpoint. Upgrading Vault doesn’t automatically change the existing policy.

Get the new default policy by firing up a temporary dev server with 1.9:

vault server -dev -dev-listen-address 127.0.0.1:8299 \
  -dev-root-token-id root

# in another window:
VAULT_ADDR=http://127.0.0.1:8299 VAULT_TOKEN=root \
  vault policy read default >policy-default-1.9

Then apply this to your main running instance, after you’ve upgraded it to 1.9, unsealed, and have authenticated to it:

vault policy read default >policy-default-old
vault policy write default - <policy-default-1.9

Configure user authentication

I had already configured users to be able to login to Vault, using userpass and OIDC, as per the previous SSH Certificate Authority article.

To check this is working, users should be able to go to the Vault URL, select a login method, and login:

After logging in, they should see their ID alias by clicking the top-right corner:

In essence, what we’re going to do next is to allow the Vault login process to extend to other web apps that you have.

You’ll need to know how to find the entity IDs of your users, and/or the ID of a group to which they all belong. Use the web interface, or the CLI, e.g:

vault list identity/entity/id
vault list identity/group/id

vault read -format=json \
  identity/group/id/ae911de1-e52a-ab87-3dfe-5e7b1d88c643

Configure a sample web app

The first client app I tested is the “example-app” supplied with the Dex IDP: this performs an OIDC exchange and decodes and displays the identity token. Note that you need at least Vault v1.9.1 to work with this app, due to a spec compliance issue in v1.9.0.

You need to compile the Dex example-app from source. Then you run it like this:

bin/example-app --debug \
  --client-id XXXXXXXX \
  --client-secret YYYYYYYY \
  --issuer https://vault1.example.net:8200/v1/identity/oidc/provider/my-provider \
  --listen http://0.0.0.0:5555 \
  --redirect-uri http://dex1.example.net:5555/callback

Note that we won’t know the client-id and client-secret until later. You need to give the application a resolvable hostname — it could be on the same host as Vault itself, but in my case I chose to run it elsewhere to be more realistic, with DNS name dex1.example.net. You could refer to it by IP address, and if you’re running this app on the same PC where you’re running your web browser, then “localhost” is sufficient.

It would be better practice to use HTTPS of course, which is easy enough, but you need to provide a suitable key and certificate.
Create OpenID Connect Provider

    Update: this process has been much simplified in Vault 1.10. You automatically get a key “default”, a provider “default”, and an assignment “allow_all”. The following steps are kept for reference only.

Now I’m more or less following the steps from the OIDC Identity Provider Secrets Engine documentation.

Create a key called “my-key”:

vault write identity/oidc/key/my-key \
  allowed_client_ids="*" \
  verification_ttl="1h" \
  rotation_period="1h" \
  algorithm="RS256"

Create an assignment. This specifies which Vault entities and groups are authorized to use a specific OIDC client for authentication flows. Here you can provide a space or comma-separated list of entity IDs (e.g. the entity IDs for “alice” and “bob”), and/or one or more group_ids that alice and bob belong to.

vault write identity/oidc/assignment/my-assignment \
  group_ids="ae911de1-e52a-ab87-3dfe-5e7b1d88c643" \
  entity_ids="d2f5ddbb-d6d9-e951-7585-78dadd492da9"

Optionally create a custom scope, here called “user”, which can provide additional information in the identity token. The default scope “openid” is always available.

vault auth list
export MOUNT_ACCESSOR=auth_userpass_bbcef7b5   # example

TOKEN_TEMPLATE=$(cat << EOF
{
    "username": {{identity.entity.aliases.$MOUNT_ACCESSOR.name}},
    "contact": {
        "email": {{identity.entity.metadata.email}},
        "phone_number": {{identity.entity.metadata.phone_number}}
    },
    "groups": {{identity.entity.groups.names}}
}
EOF
)

vault write identity/oidc/scope/user \
  description="Scope for user metadata" \
  template="$(echo $TOKEN_TEMPLATE | base64 -)"

Create an OIDC client:

vault write identity/oidc/client/my-webapp \
  redirect_uris="http://dex1.example.net:5555/callback" \
  assignments="my-assignment" \
  key="my-key" \
  id_token_ttl="30m" \
  access_token_ttl="1h"

vault read identity/oidc/client/my-webapp
...
client_id           VwxGdHaLCa7gOfoauWSaK8cua8kfecEU
client_secret       hvo_secret_yh3hDNBocUZRxxxxxxxxx

You can see that the callback URL of the example app is included here. Make a note of the client_id and client_secret returned.

Create an OIDC provider:

vault write identity/oidc/provider/my-provider \
  allowed_client_ids="*" \
  scopes_supported="user"

(It’s possible to restrict the provider to specific client-ids, should you need to, but "*" is simpler)

You can now check the OpenID Provider configuration:

curl -Ss $VAULT_ADDR/v1/identity/oidc/provider/my-provider/.well-known/openid-configuration

Pipe the output to jq . or python3 -m json.tool to get neater formatting.
Test the application

Now we’re ready to go! Return to the example-app and run it with the correct client_id and client_secret. Point a browser at the example-app, e.g. http://dex1.example.net:5555/
Login to example-app

Enter “user” as the extra scope, then Login. This will take you to the Vault UI, where you can login as alice. If all goes well then you’ll be returned to the app, where it will show the ID and Access tokens and the decoded claims:
Response from example-app after logging in

The “aud” (audience) is the client_id, and “sub” (subscriber) is the Vault entity ID. In this case the “contact.email” claim has been picked up from entity metadata, “groups” from the entity group memberships, and the “username” claim from the entity alias.

For debugging, check the console output from the app. If it’s still not clear what the problem is, you may need to set log_level="trace" in Vault’s config, and/or turn on audit logging in Vault.
Apache

The second web app that I tested, a more realistic example, was Apache with mod_auth_openidc. Installation under Ubuntu:

apt-get install apache2 libapache2-mod-auth-openidc

Create /etc/apache2/conf-available/openidc.conf:

OIDCProviderMetadataURL https://vault1.example.net:8200/v1/identity/oidc/provider/my-provider/.well-known/openid-configuration
OIDCClientID XXXXXXXX
OIDCClientSecret YYYYYYYY

OIDCRedirectURI /protected/callback
OIDCCryptoPassphrase W1Qnf4qG2kVo36A1hvDhxDkj9319ewUNBbC7/uvUrj2r
OIDCScope "openid user"
OIDCSessionInactivityTimeout 43200

# Required when using HTTP rather than HTTPS
OIDCCookieSameSite On

#OIDCRemoteUserClaim email

<Location /protected>
        AuthType openid-connect
        Require valid-user
        #Require claim groups:engineers
</Location>

(use any long, random string for OIDCCryptoPassphrase)

Enable the configuration:

a2enconf openidc
systemctl reload apache2

Again, making the server run HTTPS would be best, but for simplicity I allowed HTTP on port 80.

In order to obtain the OIDCClientID and OIDCClientSecret, you can either:

    Create a new client — this is the proper approach.
    Use the same client_id and client_secret as you did for the Dex example-app.

However if you do the latter, you’ll have to extend the list of allowed redirect_uris for this client, so that both clients can use it:

vault write identity/oidc/client/my-webapp \
redirect_uris="http://dex1.example.net:5555/callback,http://dex1.example.net/protected/callback" \
  assignments="my-assignment" \
  key="my-key" \
  id_token_ttl="30m" \
  access_token_ttl="1h"

Connect to the web server’s protected area (http://dex1.example.net/protected/) and it should go through the OpenID authentication process. To logout, go to http://dex1.example.net/protected/callback?logout=/ (however, a future access to /protected/ will log you back in automatically, unless you logout of Vault itself)

For debugging, check the Apache error logs at /var/log/apache2/error.log
kubelogin

For kubernetes login via OIDC, you will need kubelogin. The OIDC client in Vault needs to be configured with:

redirect_uris="http://localhost:8000"

The kube config will contain something like this:

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: ...
    server: ...
  name: my-cluster
contexts:
- context:
    cluster: my-cluster
    user: oidc
  name: my-cluster
current-context: my-cluster
kind: Config
preferences: {}
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      command: kubectl
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=https://vault1.example.net:8200/v1/identity/oidc/provider/my-provider
      - --oidc-client-id=VwxGdHaLCa7gOfoauWSaK8cua8kfecEU
      - --oidc-client-secret=hvo_secret_yh3hDNBocUZRxxxxxxxxx

Issuing a command like “kubectl get pod” will then perform the OIDC authentication dance to get the identity token, before contacting the Kubernetes API. The token is cached as a file under ~/.kube/cache/oidc-login/ and you can delete this if you wish to repeat the authentication.

Configuring the Kubernetes API to accept this token is outside the scope of this document, but is described here.
Conclusion

Vault v1.9 now provides a working OpenID Connect Identity Provider!
References and notes

The Hashicorp documentation for OIDC Provider is somewhat spread around, but these are the bookmarks I’ve kept:

Tutorial: Vault as an OIDC Identity Provider Concepts: OIDC Provider Secrets Engine: OIDC Identity Provider Secrets Engine: Identity Token Templates API (OIDC Provider): Create or Update a Provider API (Identity Tokens): Create a Named Key

For my testing I followed the steps in the “Secrets Engine” section, as I hadn’t come across the “tutorial” section at that point. I found a few niggles which weren’t obvious at first:

    You should set MOUNT_ACCESSOR to an appropriate value, as shown by vault auth list. e.g. export MOUNT_ACCESSOR=auth_userpass_bbcef7b5 (It’s not a big problem if you don’t, but the ID tokens will contain an empty “username” claim)
    After creating a client (say “my-webapp”), to find the client_id and client_secret you need to do vault read identity/oidc/client/my-webapp
    issuer can only be set to scheme://host:port (no path). If you don’t set issuer, it defaults to api_addr. Note that api_addr can be a hostname, not just an IP address, and indeed really should be a hostname if you’re using proper certificates. The full issuer URL generated is scheme://host:port/v1/identity/oidc/provider/provider-name, to which you add /.well-known/openid-configuration to get the OpenID configuration URL.
    Remember to change the redirect_uri to suit your application, and change assignment (i.e. which entities and/or groups are permitted to authenticate with this provider)
    You need to set the correct allowed_client_ids in key and provider — or more simply, set them to * (Note that you can’t create a client until you have created a key, and you can’t choose your own client_id, so you can’t restrict a key to specific clients until after the clients have been created). I made a suggestion here about how the documentation could be improved. The “tutorial” document handles it better, by setting allowed_client_ids="*"
    If you are upgrading from an older version of Vault, you need to upgrade the default policy (which allows users to access the /authorize endpoint)
    In Vault v1.9.0, the nonce parameter was mandatory, which prevented the Dex example-app from working. This was a spec violation, and has been fixed in vault v1.9.1 (showing a very rapid response from Hashicorp!)
    If using mod_auth_openidc, but the web server is using HTTP rather than HTTPS, then you need to set OIDCCookieSamesite on. Thanks to: https://medium.com/swlh/7-keys-to-the-mystery-of-a-missing-cookie-fdf22b012f09
** [[https://ealebed.github.io/posts/2019/приоритетность-подов-в-kubernetes/][Приоритетность подов в Kubernetes · Yevhen Lebid's website]]

Поды ([[https://ealebed.github.io/posts/2018/знакомство-с-kubernetes-часть-3-pods/][=Pods=]]) могут иметь приоритет. Приоритет отображает важность пода относительно других подов в кластере. Если под не может быть запущен на подходящем узле из-за нехватки ресурсов, то планировщик ( =scheduler= ) пытается “вытеснить” поды с более низким приоритетом и переместить их на другие узлы кластера, чтобы освободить ресурсы и запустить ожидающий под. Давайте разберемся!

Приоритет подов (Pod Priority) и вытеснение (Preemption) включены по умолчанию начиная с версии =Kubernetes=  1.11. Для их использования необходимо:

- создать один или несколько объектов =PriorityClass=  в кластере;
- добавить в описание пода поле =priorityClassName=  с именем созданного ранее класса.

=PriorityClass= - это объект, определяющий соответствие между именем класса приоритета и его целочисленным значением. Имя задается в поле =name= объекта, значение - в поле =value=. Чем больше значение, тем выше приоритет запуска у пода с указанным классом. В поле =value= можно указать целочисленное 32-битное значение меньше или равное 1000000000 - более высокие значения зарезервированы для критически важных системных подов, которые, как правило, не могут быть вытеснены. Кроме того, =PriorityClass= имеет два опциональных поля - =globalDefault= и =description=. Поле =globalDefault= со значением установленным в =true= указывает, что данный класс приоритета будет использоваться по умолчанию для всех подов кластера, в описании которых нет поля =priorityClassName=. В системе может существовать один и только один класс приоритета, в котором =globalDefault= установлено в =true=; если же во всех классах кластера это значение будет установлено в =false=, то по умолчанию приоритет всех подов в спецификации которых нет поля =priorityClassName=  будет равен нулю (0).

*Примечание.* Если вы создаете объект(ы) =PriorityClass= в кластере =Kubernetes=, в котором уже запущены поды, то следует помнить, что для существующих подов приоритет будет равняться 0 (даже если вы добавили класс с иным значением и полем =globalDefault= равным =true=) - значения определенные в классе приоритетов будут применяться только к подам, которые были созданы после создания объекта(ов) =PriorityClass= .

Пример объекта =PriorityClass=  выглядит так:

#+begin_src yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "This priority class should be used for very important services/pods only"
#+end_src

В версии =Kubernetes= 1.15 (в качестве альфа-фичи) в объект =PriorityClass= добавлено поле =PreemptionPolicy=, в котором по умолчанию установлено значение =PreemptLowerPriority=, что позволяет подам данного класса приоритета “вытеснять” поды с более низким классом приоритета для освобождения ресурсов. В поле =PreemptionPolicy= также можно установить значение =Never=  - в данном случае поды будут размещены в очереди планировщика перед подами с более низким приоритетом, но не смогут вытеснять другие поды, ожидая “естественного” освобождения ресурсов.

Пример объекта =PriorityClass=  с отключенным вытеснением будет таким:

#+begin_src yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority-nonpreempting
value: 1000000
preemptionPolicy: Never
globalDefault: false
description: "This priority class will not cause other pods to be preempted"
#+end_src

После создания одного или нескольких классов приоритета необходимо добавить поле =priorityClassName= с именем соответствующего класса в спецификацию пода (или [[https://ealebed.github.io/posts/2018/знакомство-с-kubernetes-часть-4-replicaset/][=ReplicaSet=]], или [[https://ealebed.github.io/posts/2018/знакомство-с-kubernetes-часть-5-deployments/][=Deployments=]]), например:

#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox
    command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600']
  priorityClassName: high-priority
#+end_src

В результате под с более высоким приоритетом будет поставлен в очередь и запущен раньше подов с приоритетом ниже. Если же под с высоким приоритетом не может быть запущен в данный момент времени, то планировщик ( =scheduler= ) попробует запустить его позднее, таким образом давая возможность стартонуть подам с более низким приоритетом.

Итак, когда под ([[https://ealebed.github.io/posts/2018/знакомство-с-kubernetes-часть-3-pods/][=Pods=]]) создан, он помещается в очередь и ожидает, пока планировщик определит ему подходящий узел кластера для запуска. Если планировщик не может найти ни одного подходящего узла, соответствующего всем требованиям запускаемого пода, то для ожидающего пода запускается процесс вытеснения. Допустим, =P= - это под в очереди, тогда логика вытеснения пытается найти подходящую ноду в кластере, на которой удаление одного или нескольких подов =p0,1,2-...= с более низким приоритетом, чем у =P=, позволило бы запустить под =P= на этом узле. Когда такая нода найдена, один или несколько подов =p0,1,2-...= “выселяются” с нее, освобождая необходимые ресурсы для запуска пода =P=  из очереди планировщика.

Когда под =P= “выселяет” один или несколько подов =p0,1,2-...= с узла =N=, в поле =nominatedNodeName= пода =P= появляется имя узла =N= - именно это поле помогает планировщику отслеживать необходимое количество ресурсов для пода =P=  и дает пользователю информацию о вытеснениях подов в кластере.

Стоит отметить, что под =P= не обязательно будет запущен на ноде, имя которой указано в поле =nominatedNodeName= - дело в том, что все поды имеют время для корректного завершения работы (graceful termination period). Если в течение этого периода на какой-либо другой ноде кластера появится достаточное количество ресурсов для запуска пода =P=, то он будет запущен на подходящей ноде, не дожидаясь вытеснения подов =p0,1,2-...= и освобождения ресурсов с ноды из поля =nominatedNodeName=. Второй случай когда имена узлов в полях =nominatedNodeName= и =nodeName= могут не совпадать - если в процессе освобождения ресурсов для пода =P=  в очереди планировщика появится под с еще более высоким приоритетом, то именно он будет запущен раньше.

Напоследок замечу, что класс приоритета (PriorityClass) для пода и класс качества сервиса (Qos), который рассматривали в статьях о ресурсах в =Kubernetes=  ([[https://ealebed.github.io/posts/2019/ресурсы-в-kubernetes-часть-1-memory/][раз]], [[https://ealebed.github.io/posts/2019/ресурсы-в-kubernetes-часть-2-cpu/][два]]) это совершенно разные вещи, их необходимо различать.
* Misc
- [[https://github.com/aquasecurity/kube-bench][aquasecurity/kube-bench: Checks whether Kubernetes is deployed according to security best practices as defined in the CIS Kubernetes Benchmark]]
- [[https://github.com/kubevirt/kubevirt][kubevirt/kubevirt: Kubernetes Virtualization API and runtime in order to define and manage virtual machines.]]
- [[https://github.com/kubernetes-sigs/kind][kubernetes-sigs/kind: Kubernetes IN Docker - local clusters for testing Kubernetes]]
- [[https://github.com/criticalstack/quake-kube][quake-kube Quake 3 on Kubernetes]]
- [[https://github.com/oneinfra/oneinfra][oneinfra/oneinfra: Kubernetes as a Service]]
- [[https://github.com/hjacobs/kubernetes-failure-stories][hjacobs/kubernetes-failure-stories: Compilation of public failure/horror stories related to Kubernetes]]
- [[https://github.com/kubernetes/kops][kubernetes/kops: Kubernetes Operations (kops) - Production Grade K8s Installation, Upgrades, and Management]]
- [[https://github.com/argoproj/argo-cd/][argoproj/argo-cd: Declarative continuous deployment for Kubernetes.]]
- [[https://fluxcd.io/][Flux]]
- [[https://github.com/criticalstack/quake-kube][criticalstack/quake-kube: Quake 3 on Kubernetes]]
- [[https://github.com/flyteorg/flyte][flyteorg/flyte: Kubernetes-native workflow automation platform for complex, mission-critical data and ML processes at scale. It has been battle-tested at Lyft, Spotify, Freenome, and others and is truly open-source.]]
- [[https://github.com/storax/kubedoom][storax/kubedoom: Kill Kubernetes pods by playing Id's DOOM!]]
- [[https://opensource.com/article/21/6/kube-doom][Play Doom on Kubernetes | Opensource.com]]
- [[https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner][kubernetes-sigs/sig-storage-lib-external-provisioner]]

* Monitoring
- [[https://habr.com/ru/post/586206/][Kubernetes monitoring от простого к сложному (Николай Храмчихин) / Хабр]]

* Networking

: $ kubectl describe node | grep '^Name\|PodCIDR'
#+begin_example
  Name:               kube1
  PodCIDR:                      10.1.0.0/24
  PodCIDRs:                     10.1.0.0/24
  ...
  Name:               kube7
  PodCIDR:                      10.1.6.0/24
  PodCIDRs:                     10.1.6.0/24
#+end_example

** [[https://github.com/cilium/k8s-iptables-diagram][cilium/k8s-iptables-diagram: Diagram of Kubernetes / kube-proxy iptables rules architecture]]

** [[https://old.reddit.com/r/kubernetes/comments/11pgmsa/cilium_vs_calico_k3s_what_do_you_use_and_why/jc459ka/][NeuVector]]
If your focus is securing the cluster and applying enforceable network policies, with possibly a requirement high node count performance (1000ish nodes).
NeuVector (free to use unsupported, no limits, no paywall, no ads) can create enforceable zero-trust network policies for you automatically:
- CNI agnostic, cloud agnostic, and Kubernetes platform agnostic (K8's, Rancher, Openshift, Tanzu)
- Is network centric security, does not use agents, sidecards, or kernel syscall filtering (eBPF)
- Creates Layer 7 network policies automatically for all network traffic East/West within the cluster as well as North/South by inspecting/validating traffic off of the node vNIC - does NOT use IPTables - and gives you an independent source of truth of cluster network traffic activity.
- works with service meshes that use side cars like istio or linkerD and automatically creates the network policies to protect the container/side-car traffic Istio tells us they cannot secure or encrypt via the proxy.
- Also generates container process security policies to enforce zero trust network and process segmentation.
- Performance tested in 1000 node clusters
- Both Alert-only and blocking modes for enforceable network policies
- Live network packet capture within the cluster for forensics and additional "observability"

* Operators
- [[https://github.com/flant/shell-operator][flant/shell-operator: Shell-operator is a tool for running event-driven scripts in a Kubernetes cluster]]
- [[https://github.com/nolar/kopf][nolar/kopf: A Python framework to write Kubernetes operators in just a few lines of code]]
- [[https://operatorhub.io/][OperatorHub.io | The registry for Kubernetes Operators]]
- [[https://github.com/strimzi/strimzi-kafka-operator][strimzi/strimzi-kafka-operator: Apache Kafka® running on Kubernetes]]
- [[https://habr.com/ru/company/southbridge/blog/556860/][Пишем оператор Kubernetes с нуля / Хабр]]

** Prometheus Operator
- [[https://cloud.tencent.com/developer/article/2208456][Prometheus Operator中探针的使用 - 腾讯云开发者社区-腾讯云]]

* Problems

#+begin_example
  E1111 19:31:15.276111       1 reflector.go:127] pkg/mod/k8s.io/client-go@v0.19.1/tools/cache/reflector.go:156: Failed to watch *v1.Endpoints: failed to list *v1.Endpoints: endpoints is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "endpoints" in API group "" at the cluster scope
#+end_example

from [[https://giters.com/ivanfioravanti/kubernetes-the-hard-way-on-azure/issues/54][Recent update seems to be missing ClusterRole for CoreDNS - Giters]]

#+begin_src bash
  cat <<EOF | kubectl apply -f -
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: coredns
  rules:
  - apiGroups:
    - ""
    resources:
    - endpoints
    - services
    - pods
    - namespaces
    verbs:
    - list
    - watch
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: coredns
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: coredns
  subjects:
  - kind: ServiceAccount
    name: coredns
    namespace: kube-system
  EOF
#+end_src

then redeploy with:
: kubectl -n kube-system rollout restart deployment coredns

** kubevirt

*** unable to schedule after reboot
#+begin_example
  kubectl describe node kube8
  ...
  Allocatable:
    cpu:                            8
    devices.kubevirt.io/kvm:        0
    devices.kubevirt.io/sev:        0
    devices.kubevirt.io/tun:        0
    devices.kubevirt.io/vhost-net:  0
    ephemeral-storage:              441863943051
    hugepages-1Gi:                  0
    hugepages-2Mi:                  0
    memory:                         65826164Ki
    pods:                           110
#+end_example
* Prometheus
- pod resources
  : https://prometheus.kubernetes.corp1.majordomo.ru/graph?g0.expr=topk(10%2C%20sum%20by%20(container)%20(kube_pod_container_resource_limits%7Bresource%3D%22cpu%22%7D))&g0.tab=1&g0.stacked=0&g0.show_exemplars=0&g0.range_input=1h&g1.expr=topk(15%2C%20node_namespace_pod_container%3Acontainer_cpu_usage_seconds_total%3Asum_irate%7B%7D)&g1.tab=1&g1.stacked=0&g1.show_exemplars=0&g1.range_input=2w&g2.expr=topk(15%2C%20sum(container_memory_working_set_bytes%7Bjob%3D%22kubelet%22%2C%20metrics_path%3D%22%2Fmetrics%2Fcadvisor%22%2C%20container!%3D%22%22%2C%20image!%3D%22%22%7D)%20by%20(container))&g2.tab=1&g2.stacked=0&g2.show_exemplars=0&g2.range_input=1h
- persistentce volume size
  : https://prometheus.kubernetes.corp1.majordomo.ru/graph?g0.expr=(sum%20without(instance%2C%20node)%20(kubelet_volume_stats_capacity_bytes%7Bcluster%3D%22%22%2C%20job%3D%22kubelet%22%2C%20metrics_path%3D%22%2Fmetrics%22%7D)%20-%20sum%20without(instance%2C%20node)%20(kubelet_volume_stats_available_bytes%7Bcluster%3D%22%22%2C%20job%3D%22kubelet%22%2C%20metrics_path%3D%22%2Fmetrics%22%7D))&g0.tab=0&g0.stacked=0&g0.show_exemplars=0&g0.range_input=1d
* pv
** dynamic
- [[https://github.com/rancher/local-path-provisioner][rancher/local-path-provisioner: Dynamically provisioning persistent local storage with Kubernetes]]
#+begin_example
  $ kubectl create -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.22/deploy/local-path-storage.yaml
  namespace/local-path-storage created
  serviceaccount/local-path-provisioner-service-account created
  clusterrole.rbac.authorization.k8s.io/local-path-provisioner-role created
  clusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind created
  deployment.apps/local-path-provisioner created
  storageclass.storage.k8s.io/local-path created
  configmap/local-path-config created
#+end_example

#+begin_example
  $ kubectl delete -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.22/deploy/local-path-storage.yaml
  namespace "local-path-storage" deleted
  serviceaccount "local-path-provisioner-service-account" deleted
  clusterrole.rbac.authorization.k8s.io "local-path-provisioner-role" deleted
  clusterrolebinding.rbac.authorization.k8s.io "local-path-provisioner-bind" deleted
  deployment.apps "local-path-provisioner" deleted
  storageclass.storage.k8s.io "local-path" deleted
  configmap "local-path-config" deleted
#+end_example

* Storage
- [[https://vitobotta.com/2020/01/04/linstor-storage-the-kubernetes-way/][Linstor storage for Kubernetes, the Kubernetes way]]
- [[https://github.com/piraeusdatastore/piraeus][piraeusdatastore/piraeus: High Available Datastore for Kubernetes]]
- [[https://temofeev.ru/info/articles/sostoyanie-i-proizvoditelnost-resheniy-dlya-postoyannogo-khraneniya-dannykh-v-kubernetes/][Состояние и производительность решений для постоянного хранения данных в Kubernetes]]

* Tools
- [[https://habr.com/ru/company/southbridge/blog/523358/][11 инструментов, делающих Kubernetes лучше / Блог компании Southbridge / Хабр]]
- [[https://habr.com/ru/company/southbridge/blog/523790/][12 инструментов, делающих Kubernetes легче / Блог компании Southbridge / Хабр]]
- [[https://www.youtube.com/watch?v=CB79eTFbR0w][(2) 10 Must-Have Kubernetes Tools - YouTube]]
- [[https://www.youtube.com/c/%D0%90%D1%80%D1%82%D1%83%D1%80%D0%9A%D1%80%D1%8E%D0%BA%D0%BE%D0%B2/videos][(63) Артур Крюков - YouTube]]
- [[https://github.com/aabouzaid/kustomize-plugin-merger][aabouzaid/kustomize-plugin-merger: 🔀 Merger 🔀 A Kustomize plugin generating manifests seamlessly by extending Kustomize merge strategies (schemaless StrategicMerge).]]
- [[https://github.com/abahmed/kwatch][abahmed/kwatch: monitor & detect crashes in your Kubernetes(K8s) cluster instantly]]
- [[https://github.com/abiosoft/colima][abiosoft/colima: Container runtimes on macOS (and Linux) with minimal setup]]
- [[https://github.com/ahmetb/kubectl-tree][ahmetb/kubectl-tree: kubectl plugin to browse Kubernetes object hierarchies as a tree]]
- [[https://github.com/airwallex/k8s-pod-restart-info-collector][airwallex/k8s-pod-restart-info-collector: Automated troubleshooting of Kubernetes Pods issues. Collect K8s pod restart reasons, logs, and events automatically.]]
- [[https://github.com/airwallex/k8s-pod-restart-info-collector][airwallex/k8s-pod-restart-info-collector: Automated troubleshooting of Kubernetes Pods issues. Collect K8s pod restart reasons, logs, and events automatically.]]
- [[https://github.com/alcideio/rbac-tool][alcideio/rbac-tool: Rapid7 | insightCloudSec | Kubernetes RBAC Power Toys - Visualize, Analyze, Generate & Query]]
- [[https://github.com/alex1989hu/kubelet-serving-cert-approver][alex1989hu/kubelet-serving-cert-approver: Kubelet Serving TLS Certificate Signing Request Approver]]
- [[https://github.com/alexellis/arkade][alexellis/arkade: Your one-stop CLI for Kubernetes]]
- [[https://github.com/alexellis/k3sup][alexellis/k3sup: bootstrap Kubernetes with k3s over SSH < 1 min]]
- [[https://github.com/alexellis/registry-creds][alexellis/registry-creds: Replicate Kubernetes ImagePullSecrets to all namespaces]]
- [[https://github.com/alexellis/run-job][alexellis/run-job: Run a Kubernetes Job and get the logs when it's done 🏃‍♂️]]
- [[https://github.com/alibaba/open-local][alibaba/open-local: cloud-native local storage management system]]
- [[https://blog.flant.com/announcing-shell-operator-to-simplify-creating-of-kubernetes-operators/][Announcing shell-operator to simplify creating of Kubernetes operators – Flant blog]]
  - [[https://github.com/flant/shell-operator][flant/shell-operator: Shell-operator is a tool for running event-driven scripts in a Kubernetes cluster]]
- [[https://github.com/apecloud/kubeblocks][apecloud/kubeblocks: KubeBlocks helps developers and platform engineers manage database workloads (MySQL, PostgresSQL, Redis, MongoDB, Kafka and vector databases) on K8s inside your own cloud account. It supports multiple clouds, including AWS, Azure, GCP, and Alibaba Cloud.]]
- [[https://github.com/apecloud/kubeblocks][apecloud/kubeblocks: KubeBlocks is an open-source control plane that runs and manages databases, message queues and other data infrastructure on K8s.]]
- [[https://www.apiclarity.io/][APIClarity | API traffic visibility in K8s clusters]]
- [[https://microcks.io/][API Mocking and Testing | Microcks.io]]
- [[https://github.com/appvia/krane][appvia/krane: Kubernetes RBAC static analysis & visualisation tool]]
- [[https://github.com/aquasecurity/starboard][aquasecurity/starboard: Kubernetes-native security toolkit]]
- [[https://github.com/aquasecurity/trivy-operator][aquasecurity/trivy-operator: Kubernetes-native security toolkit]]
- [[https://github.com/ArctarusLimited/Fractal][ArctarusLimited/Fractal: Zero-compromise Kubernetes resource generation with Nix, Jsonnet and Kustomize. Planned Helm support.]]
- [[https://github.com/armosec/kubescape][armosec/kubescape: kubescape is the first tool for testing if Kubernetes is deployed securely as defined in Kubernetes Hardening Guidance by to NSA and CISA (https://www.nsa.gov/News-Features/Feature-Stories/Article-View/Article/2716980/nsa-cisa-release-kubernetes-hardening-guidance/)]]
- [[https://github.com/arnehilmann/k8s-auto-updater][arnehilmann/k8s-auto-updater: auto-update your cluster: sync your docker images and restart pods running on outdated images]]
- [[https://github.com/arttor/helmify][arttor/helmify: Creates Helm chart from Kubernetes yaml]]
- [[https://github.com/asobti/kube-monkey][asobti/kube-monkey: An implementation of Netflix's Chaos Monkey for Kubernetes clusters]]
- [[https://github.com/astefanutti/kubebox][astefanutti/kubebox: ⎈❏ Terminal and Web console for Kubernetes]]
- [[https://github.com/aws/karpenter][aws/karpenter: Kubernetes Node Autoscaling: built for flexibility, performance, and simplicity.]]
- [[https://github.com/backube/snapscheduler][backube/snapscheduler: Scheduled snapshots for Kubernetes persistent volumes]]
- [[https://github.com/backube/volsync][backube/volsync: Asynchronous data replication for Kubernetes volumes]]
- [[https://github.com/backube/volsync][backube/volsync: Asynchronous data replication for Kubernetes volumes]]
- [[https://github.com/bank-vaults/bank-vaults][bank-vaults/bank-vaults: A Vault swiss-army knife: A CLI tool to init, unseal and configure Vault (auth methods, secret engines).]]
- [[https://github.com/banzaicloud/jwt-to-rbac][banzaicloud/jwt-to-rbac: JWT-to-RBAC lets you automatically generate RBAC resources based on JWT tokens]]
- [[https://github.com/banzaicloud/koperator][banzaicloud/koperator: Oh no! Yet another Apache Kafka operator for Kubernetes]]
- [[https://github.com/benc-uk/kubeview][benc-uk/kubeview: Kubernetes cluster visualiser and graphical explorer]]
- [[https://github.com/bergerx/kubectl-status][bergerx/kubectl-status: A kubectl plugin to print a human-friendly output that focuses on the status fields of the resources in kubernetes.]]
- [[https://github.com/BeryJu/korb][BeryJu/korb: Move Kubernetes PVCs between Storage Classes and Namespaces]]
- [[https://github.com/bitnami-labs/charts-syncer][bitnami-labs/charts-syncer: Tool for synchronizing Helm Chart repositories.]]
- [[https://github.com/bitnami-labs/healthcheck-tools][bitnami-labs/healthcheck-tools: Set of Go tools to check different elements of your stack (SSL, SMTP, Permissions...)]]
- [[https://github.com/bitnami-labs/sealed-secrets][bitnami-labs/sealed-secrets: A Kubernetes controller and tool for one-way encrypted Secrets]]
- [[https://github.com/bitpoke/mysql-operator][bitpoke/mysql-operator: Asynchronous MySQL Replication on Kubernetes using Percona Server and Openark's Orchestrator.]]
- [[https://github.com/bitpoke/wordpress-operator][bitpoke/wordpress-operator: Bitpoke Kubernetes operator for WordPress]]
- [[https://github.com/bloomberg/goldpinger][bloomberg/goldpinger: Debugging tool for Kubernetes which tests and displays connectivity between nodes in the cluster.]]
- [[https://github.com/box/kube-iptables-tailer][box/kube-iptables-tailer: A service for better network visibility for your Kubernetes clusters.]]
- [[https://github.com/boz/kail][boz/kail: kubernetes log viewer]]
- [[https://github.com/brigadecore/brigade][brigadecore/brigade: Event-driven scripting for Kubernetes]]
- [[https://github.com/budimanjojo/talhelper][budimanjojo/talhelper: A tool to help creating Talos kubernetes cluster]]
- [[https://www.heroku.com/careers/builds-pipeline-services-software-engineer-186][Builds & Pipeline Services Software Engineer | Heroku]]
  - [[https://github.com/opencontainers/image-spec][opencontainers/image-spec: OCI Image Format]]
  - [[https://github.com/knative][Knative - Kubernetes-based platform to build, deploy, and manage modern serverless workloads]]
    - [[https://github.com/cppforlife/knctl][cppforlife/knctl: Knative CLI]]
    - [[https://habr.com/ru/company/southbridge/blog/475630/][Knative — платформа как услуга на основе k8s с поддержкой serverless / Хабр]]
    - [[https://www.youtube.com/watch?v=AybvgX3jA5U][KNative scale to 0 - YouTube]]
    - [[https://opensource.com/article/20/11/knative][Create your first Knative app | Opensource.com]]
    - [[https://opensource.com/article/21/2/knative-eventing][A step-by-step guide to Knative eventing | Opensource.com]]
  - [[https://github.com/tektoncd/pipeline][tektoncd/pipeline: A cloud-native Pipeline resource.]]
  - [[https://buildpacks.io/][Cloud Native Buildpacks · Cloud Native Buildpacks]]
- [[https://github.com/bunkerity/bunkerweb][bunkerity/bunkerweb: 🛡️ Make your web services secure by default !]]
- [[https://habr.com/ru/company/flant/blog/485716/][Calico для сети в Kubernetes: знакомство и немного из опыта / Хабр]]
- [[https://github.com/carina-io/carina][carina-io/carina: Carina: an high performance and ops-free local storage for kubernetes]]
- [[https://github.com/carvel-dev/imgpkg][carvel-dev/imgpkg: Store application configuration files in Docker/OCI registries]]
- [[https://github.com/c-bata/kube-prompt][c-bata/kube-prompt: An interactive kubernetes client featuring auto-complete.]]
- [[https://github.com/cdk8s-team/cdk8s][cdk8s-team/cdk8s: Define Kubernetes native apps and abstractions using object-oriented programming]]
- [[https://github.com/cdk8s-team/cdk8s-plus][cdk8s-team/cdk8s-plus: A software development framework that provides high level abstractions for authoring Kubernetes applications.]]
- [[https://github.com/cdk-team/CDK][cdk-team/CDK: 📦 Make security testing of K8s, Docker, and Containerd easier.]]
- [[https://cello-proj.github.io/cello/][Cello - The engine for cloud deployments]]
- [[https://github.com/cerbos/cerbos][cerbos/cerbos: Cerbos is the open core, language-agnostic, scalable authorization solution that makes user permissions and authorization simple to implement and manage by writing context-aware access control policies for your application resources.]]
- [[https://github.com/cerbos/reimage][cerbos/reimage: Tool for mass update of images for kubernetes manifests.]]
- [[https://github.com/chenjiandongx/kubectl-images][chenjiandongx/kubectl-images: Show container images used in the cluster.]]
- [[https://github.com/chgl/kube-powertools][chgl/kube-powertools: An always up to date collection of useful tools for your Kubernetes linting and auditing needs.]]
- [[https://github.com/chitoku-k/healthcheck-k8s][chitoku-k/healthcheck-k8s: Check if the specified Kubernetes node is schedulable and return as HTTP status code]]
- [[https://github.com/cilium/hubble][cilium/hubble: Hubble - Network, Service & Security Observability for Kubernetes using eBPF]]
- [[https://github.com/cloudflare/sciuro][cloudflare/sciuro: Alertmanager to Kubernetes Node conditions bridge]]
- [[https://github.com/cloudnativelabs/kube-router][cloudnativelabs/kube-router: Kube-router, a turnkey solution for Kubernetes networking.]]
- [[https://github.com/cloudnativelabs/kube-shell][cloudnativelabs/kube-shell: Kubernetes shell: An integrated shell for working with the Kubernetes]]
- [[https://landscape.cncf.io/][Cloud Native Landscape]]
- [[https://github.com/cloud-native-skunkworks/ubuntu-operator][cloud-native-skunkworks/ubuntu-operator: Control Ubuntu from Kubernetes.]]
- [[https://github.com/cloudtty/cloudtty][cloudtty/cloudtty: A Friendly Kubernetes CloudShell (Web Terminal) !]]
- [[https://github.com/cloudworkz/kube-eagle][cloudworkz/kube-eagle: A prometheus exporter created to provide a better overview of your resource allocation and utilization in a Kubernetes cluster.]]
- [[https://github.com/clusternet/clusternet][clusternet/clusternet: Managing your Kubernetes clusters (including public, private, edge, etc) as easily as visiting the Internet ⎈]]
- [[https://github.com/cnwizards/node-wizard][cnwizards/node-wizard: Node Wizard is a controller that monitors node readiness. It cordons and drains nodes that are not ready, and uncordons them when they become ready again.]]
- [[https://github.com/containerd/stargz-snapshotter][containerd/stargz-snapshotter: Fast container image distribution plugin with lazy pulling]]
- [[https://github.com/ContainerSolutions/kubernetes-examples][ContainerSolutions/kubernetes-examples: Minimal self-contained examples of standard Kubernetes features and patterns in YAML]]
- [[https://github.com/containers/podman-desktop][containers/podman-desktop: Podman Desktop - A graphical tool for developing on containers and Kubernetes]]
- [[https://github.com/controlplaneio/netassert][controlplaneio/netassert: Network security testing for Kubernetes DevSecOps workflows]]
- [[https://github.com/corneliusweig/ketall][corneliusweig/ketall: Like `kubectl get all`, but get really all resources]]
- [[https://github.com/corneliusweig/rakkess][corneliusweig/rakkess: Review Access - kubectl plugin to show an access matrix for k8s server resources]]
- [[https://github.com/coroot/coroot][coroot/coroot: A monitoring and troubleshooting tool for microservice architectures.]]
- [[https://github.com/corpix/k3s-vm][corpix/k3s-vm: NixOS k3s VM to play & test manifests]]
- [[https://github.com/crowdsecurity/helm-charts][crowdsecurity/helm-charts: CrowdSec community kubernetes helm charts]]
- [[https://github.com/CrunchyData/postgres-operator][CrunchyData/postgres-operator: Production PostgreSQL for Kubernetes, from high availability Postgres clusters to full-scale database-as-a-service.]]
- [[https://github.com/cyberark/KubiScan][cyberark/KubiScan: A tool to scan Kubernetes cluster for risky permissions]]
- [[https://github.com/darkbitio/k8s-mirror][darkbitio/k8s-mirror: Creates a local mirror of a Kubernetes cluster in a docker container to support offline reviewing]]
- [[https://github.com/databricks/click][databricks/click: The "Command Line Interactive Controller for Kubernetes"]]
- [[https://github.com/databus23/helm-diff][databus23/helm-diff: A helm plugin that shows a diff explaining what a helm upgrade would change]]
- [[https://github.com/datreeio/datree][datreeio/datree: Prevent Kubernetes misconfigurations from reaching production (again 😤 )! The CLI integration provides a policy enforcement solution to run automatic checks for rule violations. See our docs: https://hub.datree.io]]
- [[https://github.com/deepflowio/deepflow][deepflowio/deepflow: Application Observability using eBPF]]
- [[https://github.com/deislabs/krustlet][deislabs/krustlet: Kubernetes Rust Kubelet]]
- [[https://github.com/derailed/k9s][derailed/k9s: 🐶 Kubernetes CLI To Manage Your Clusters In Style!]]
- [[https://github.com/derailed/popeye][derailed/popeye: 👀 A Kubernetes cluster resource sanitizer]]
- [[https://github.com/devops-works/scan-exporter][devops-works/scan-exporter: Scans targets and exposes results to Prometheus.]]
- [[https://devpod.sh/][DevPod - Open Source Dev-Environments-As-Code]]
- [[https://github.com/devspace-cloud/devspace][devspace-cloud/devspace: DevSpace - The Fastest Developer Tool for Kubernetes ⚡ Automate your deployment workflow with DevSpace and develop software directly inside Kubernetes.]]
- [[https://www.devspace.sh/][DevSpace is an open-source developer tool for Kubernetes that lets you develop and deploy cloud-native software faster.]]
- [[https://github.com/digitalis-io/vals-operator][digitalis-io/vals-operator: Kubernetes Operator to sync secrets between different secret backends and Kubernetes]]
- [[https://github.com/DocPlanner/rabbit-stalker][DocPlanner/rabbit-stalker: Kubernetes operator to restart workloads under some conditions coming from RabbitMQ]]
- [[https://github.com/doitintl/kube-no-trouble][doitintl/kube-no-trouble: Easily check your clusters for use of deprecated APIs]]
- [[https://www.downloadkubernetes.com/][Download Kubernetes]]
- [[https://github.com/dtan4/k8stail][dtan4/k8stail: tail -f experience for Kubernetes Pods]]
- [[https://github.com/dty1er/kubecolor][dty1er/kubecolor: colorizes kubectl output]]
- [[https://github.com/eclipse/jkube][eclipse/jkube: Build and Deploy java applications on Kubernetes]]
- [[https://github.com/eigengrau/krew2nix][eigengrau/krew2nix: Makes kubectl plug-ins from the Krew repository accessible to Nix]]
- [[https://github.com/eko/monday][eko/monday: ⚡️ A dev tool for microservice developers to run local applications and/or forward others from/to Kubernetes SSH or TCP]]
- [[https://github.com/eldadru/ksniff][eldadru/ksniff: Kubectl plugin to ease sniffing on kubernetes pods using tcpdump and wireshark]]
- [[https://github.com/EmilyShepherd/kiOS][EmilyShepherd/kiOS: Slim Operating System for running Kubernetes with minimal overhead]]
- [[https://github.com/emissary-ingress/emissary][emissary-ingress/emissary: open source Kubernetes-native API gateway for microservices built on the Envoy Proxy]]
- [[https://github.com/enix/x509-certificate-exporter][enix/x509-certificate-exporter: A Prometheus exporter to monitor x509 certificates expiration in Kubernetes clusters or standalone]]
- [[https://github.com/eraser-dev/eraser][eraser-dev/eraser: 🧹 Cleaning up images from Kubernetes nodes]]
- [[https://github.com/estahn/k8s-image-swapper][estahn/k8s-image-swapper: Mirror images into your own registry and swap image references automatically.]]
- [[https://github.com/ExpediaGroup/container-startup-autoscaler][ExpediaGroup/container-startup-autoscaler: A Kubernetes controller that modifies the CPU and/or memory resources of containers depending on whether they're starting up, according to the startup/post-startup settings you supply.]]
- [[https://github.com/explorium-ai/kube-auto-puller][explorium-ai/kube-auto-puller: A Kubernetes Image Puller and Cacher with Automatic Discovery]]
- [[https://github.com/external-secrets/external-secrets][external-secrets/external-secrets: External Secrets Operator reads information from a third-party service like AWS Secrets Manager and automatically injects the values as Kubernetes Secrets.]]
- [[https://github.com/f0xtek/k8sresourceless][f0xtek/k8sresourceless: A utility to find all Kubernetes Pods that have no CPU or memory resource requests defined for their containers.]]
- [[https://github.com/fabric8io/kubernetes-client][fabric8io/kubernetes-client: Java client for Kubernetes & OpenShift]]
- [[https://github.com/FairwindsOps/goldilocks][FairwindsOps/goldilocks: Get your resource requests "Just Right"]]
- [[https://github.com/FairwindsOps/pluto][FairwindsOps/pluto: A cli tool to help discover deprecated apiVersions in Kubernetes]]
- [[https://github.com/FairwindsOps/polaris][FairwindsOps/polaris: Validation of best practices in your Kubernetes clusters]]
- [[https://github.com/FairwindsOps/rbac-manager][FairwindsOps/rbac-manager: A Kubernetes operator that simplifies the management of Role Bindings and Service Accounts.]]
- [[https://github.com/falcosecurity/falco][falcosecurity/falco: Cloud Native Runtime Security]]
- [[https://github.com/farcaller/nix-kube-modules][farcaller/nix-kube-modules: A nixos module to configure helm charts to be installed into k3s]]
- [[https://github.com/flomesh-io/pipy][flomesh-io/pipy: Pipy is a programmable proxy for the cloud, edge and IoT.]]
- [[https://github.com/fluxcd/flagger][fluxcd/flagger: Progressive delivery Kubernetes operator (Canary, A/B Testing and Blue/Green deployments)]]
- [[https://github.com/fluxcd/flux2][fluxcd/flux2: Open and extensible continuous delivery solution for Kubernetes. Powered by GitOps Toolkit.]]
- [[https://github.com/fluxninja/aperture][fluxninja/aperture: Rate limiting, caching, and request prioritization for modern workloads]]
- [[https://github.com/freifunkMUC/wg-access-server-chart?tab=readme-ov-file][freifunkMUC/wg-access-server-chart: Helm Charts for wg-access-server]]
- [[https://github.com/furiko-io/furiko][furiko-io/furiko: Cloud-native, enterprise-level cron job platform for Kubernetes]]
- [[https://github.com/gardener/etcd-backup-restore][gardener/etcd-backup-restore: Collection of components to backup and restore the Etcd of a Kubernetes cluster]]
- [[https://github.com/gardener][Gardener: Universal Kubernetes distribution at Scale.]]
- [[https://github.com/gatblau/onix][gatblau/onix: A reactive configuration manager designed to support Infrastructure as a Code provisioning, and bi-directional configuration management providing a single source of truth across multi-cloud environments.]]
- [[https://gefyra.dev/][Gefyra | Blazingly-fast rocket, rock-solid, local application development arrow_right with Kubernetes. | Gefyra | Blazingly-fast rocket, rock-solid, local application development arrow_right with Kubernetes.]]
- [[https://github.com/gianlucam76/k8s-cleaner][gianlucam76/k8s-cleaner: Cleaner is a Kubernetes controller that proactively identifies and removes unused resources, helping you maintain a streamlined and efficient Kubernetes cluster. It provides flexible scheduling, label filtering, Lua-based selection criteria, resource removal or update and notifications via Slack, Webex and Discord.]]
- [[https://github.com/giantswarm/kvm-operator][giantswarm/kvm-operator: Handles Kubernetes clusters running on a Kubernetes cluster with workers and masters in KVMs on bare metal]]
- [[https://werf.io/][Giterministic CLI tool | werf]]
- [[https://github.com/gitpod-io/gitpod][gitpod-io/gitpod: The developer platform for on-demand cloud development environments to create software faster and more securely.]]
- [[https://github.com/gocrane/crane][gocrane/crane: Crane is a FinOps Platform for Cloud Resource Analytics and Economics in Kubernetes clusters. The goal is not only to help users to manage cloud cost easier but also ensure the quality of applications.]]
- [[https://github.com/GoogleContainerTools/kaniko][GoogleContainerTools/kaniko: Build Container Images In Kubernetes]]
- [[https://github.com/GoogleContainerTools/kpt][GoogleContainerTools/kpt: A package-centric toolchain that enables a WYSIWYG configuration authoring, automation, and delivery experience, which simplifies managing Kubernetes platforms and KRM-driven infrastructure at scale by manipulating declarative Configuration as Data, separated from the code that transforms it.]]
- [[https://github.com/GoogleContainerTools/skaffold][GoogleContainerTools/skaffold: Easy and Repeatable Kubernetes Development]]
- [[https://github.com/GoogleContainerTools/skaffold][GoogleContainerTools/skaffold: Easy and Repeatable Kubernetes Development]]
- [[https://github.com/google/ko][google/ko: Build and deploy Go applications on Kubernetes]]
- [[https://github.com/google/kube-startup-cpu-boost][google/kube-startup-cpu-boost: Kube Startup CPU Boost is a controller that increases CPU resource requests and limits during Kubernetes workload startup time]]
- [[https://github.com/go-sigma/sigma][go-sigma/sigma: OCI artifact manager]]
- [[https://github.com/groundcover-com/caretta][groundcover-com/caretta: Instant K8s service dependency map, right to your Grafana.]]
- [[https://github.com/groundcover-com/murre][groundcover-com/murre: Murre is an on-demand, scaleable source of container resource metrics for K8s.]]
- [[https://github.com/hall/kubenix][hall/kubenix: Kubernetes management with Nix]]
- [[https://goharbor.io/][Harbor cloud native repository for Kubernetes]]
- [[https://github.com/headlamp-k8s/headlamp][headlamp-k8s/headlamp: A Kubernetes web UI that is fully-featured, user-friendly and extensible]]
- [[https://github.com/helmfile/vals][helmfile/vals: Helm-like configuration values loader with support for various sources]]
- [[https://habr.com/ru/post/575646/][Helmwave v0.12.8 / Хабр]]
  - [[https://github.com/helmwave/helmwave][helmwave/helmwave: 🌊 Helmwave is like docker-compose for @helm]]
- [[https://codeberg.org/hjacobs/kube-janitor][hjacobs/kube-janitor: Clean up (delete) Kubernetes resources after a configured TTL (time to live) - kube-janitor - Codeberg.org]]
- [[https://github.com/homeport/havener][homeport/havener: /ˈheɪvənə/ - Think of it as a swiss army knife for Kubernetes tasks]]
- [[https://github.com/hwameistor/hwameistor][hwameistor/hwameistor: Hwameistor is an HA local storage system for cloud-native stateful workloads.]]
- [[https://github.com/imuxin/kubectl-watch][imuxin/kubectl-watch: A kubectl plugin to provide a pretty delta change view of being watched kubernetes resources]]
- [[https://github.com/inercia/k3x][inercia/k3x: A UI for local Kubernetes clusters created with k3d]]
- [[https://github.com/infracloudio/botkube][infracloudio/botkube: An app that helps you monitor your Kubernetes cluster, debug critical deployments & gives recommendations for standard practices]]
- [[https://github.com/inlets/inlets-operator][inlets/inlets-operator: Get public LoadBalancers on your local Kubernetes clusters]]
- [[https://github.com/InseeFrLab/onyxia][InseeFrLab/onyxia: 🔬 Web app to simplify data science environment setup on Kubernetes]]
- [[https://inspektor-gadget.io/][Inspektor Gadget | Inspektor Gadget]]
- [[https://github.com/instrumenta/kubeval/][instrumenta/kubeval: Validate your Kubernetes configuration files, supports multiple Kubernetes versions]]
- [[https://github.com/int128/kauthproxy][int128/kauthproxy: Local authentication proxy for Kubernetes Dashboard (kubectl auth-proxy)]]
- [[https://github.com/int128/kubelogin][int128/kubelogin: kubectl plugin for Kubernetes OpenID Connect authentication (kubectl oidc-login)]]
- [[https://www.youtube.com/watch?v=KIIkVD7gujY][Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant & Doug Hellmann - YouTube]]
- [[https://www.suse.com/c/introduction-k3d-run-k3s-docker-src/][Introduction to k3d: Run K3s in Docker | SUSE Communities]]
- [[https://github.com/istio/istio][istio/istio: Connect, secure, control, and observe services.]]
- [[https://github.com/itaysk/kubectl-neat][itaysk/kubectl-neat: Clean up Kubernetes yaml and json output to make it readable]]
- [[https://github.com/itzg/kube-metrics-exporter][itzg/kube-metrics-exporter: Simple application that accesses the Kubernetes metrics API and exports the pod metrics for Prometheus scraping]]
- [[https://github.com/iximiuz/kexp][iximiuz/kexp: k'exp - Kubernetes Explorer]]
- [[https://github.com/jaegertracing/jaeger][jaegertracing/jaeger: CNCF Jaeger, a Distributed Tracing Platform]]
- [[https://github.com/jcmoraisjr/haproxy-ingress][jcmoraisjr/haproxy-ingress: HAProxy Ingress]]
- [[https://github.com/jdockerty/kubectl-oomd][jdockerty/kubectl-oomd: Show pods/containers which have recently been 'OOMKilled', as a kubectl plugin.]]
- [[https://github.com/jenkinsci/kubernetes-operator][jenkinsci/kubernetes-operator: Kubernetes native Jenkins Operator]]
- [[https://github.com/jeremykross/konstellate][jeremykross/konstellate: Free and Open Source GUI to Visualize Kubernetes Applications.]]
- [[https://github.com/jetstack/version-checker][jetstack/version-checker: Kubernetes utility for exposing image versions in use, compared to latest available upstream, as metrics.]]
- [[https://github.com/jgillich/kubegtk][jgillich/kubegtk: Modern, native Kubernetes desktop client.]]
- [[https://github.com/johanhaleby/kubetail][johanhaleby/kubetail: Bash script to tail Kubernetes logs from multiple pods at the same time]]
- [[https://github.com/jreisinger/kcve][jreisinger/kcve: Kubernetes CVEs]]
  - [[https://kubernetes.io/docs/reference/issues-security/official-cve-feed/][Official CVE Feed | Kubernetes]]
- [[https://github.com/jreisinger/kubectl-modsec][jreisinger/kubectl-modsec: Extract ModSecurity information from Kubernetes]]
  - [[https://kubernetes.github.io/ingress-nginx/user-guide/third-party-addons/modsecurity/][ModSecurity Web Application Firewall - Ingress-Nginx Controller]]
- [[https://github.com/jwcesign/kubespider][jwcesign/kubespider: A global resource download orchestration system, build your home download center.]]
- [[https://github.com/k0sproject/k0s][k0sproject/k0s: k0s - Zero Friction Kubernetes]]
- [[https://github.com/k3d-io/k3d][k3d-io/k3d: Little helper to run CNCF's k3s in Docker]]
- [[https://github.com/k3d-io/vscode-k3d][k3d-io/vscode-k3d: Kubernetes k3d extension for Visual Studio Code]]
- [[https://github.com/k3s-io/kine][k3s-io/kine: Run Kubernetes on MySQL, Postgres, sqlite, dqlite, not etcd.]]
- [[https://k3s.io/][K3s: Lightweight Kubernetes]]
- [[https://github.com/k8s-at-home/charts][k8s-at-home/charts: Helm charts for applications you run at home]]
- [[https://coredns.io/plugins/k8s_external/][k8s_external: Resolves load balancer, external IPs from outside Kubernetes clusters and if enabled headless services.]]
- [[https://github.com/k8sgpt-ai/k8sgpt][k8sgpt-ai/k8sgpt: Giving Kubernetes SRE superpowers to everyone]]
- [[https://github.com/k8snetworkplumbingwg/bridge-operator][k8snetworkplumbingwg/bridge-operator: Bridge-operator manages linux bridge in Kubernetes cluster node.]]
- [[https://github.com/k8snetworkplumbingwg/multus-cni][k8snetworkplumbingwg/multus-cni: A CNI meta-plugin for multi-homed pods in Kubernetes]]
  - [[https://github.com/foundation-model-stack/multi-nic-cni][foundation-model-stack/multi-nic-cni - Multi-NIC CNI is the CNI plugin operating on top of Multus CNI]]
- [[https://github.com/k8snetworkplumbingwg/whereabouts][k8snetworkplumbingwg/whereabouts: A CNI IPAM plugin that assigns IP addresses cluster-wide]]
- [[https://github.com/k8tz/k8tz][k8tz/k8tz: Kubernetes admission controller and a CLI tool to inject timezones into Pods and CronJobs]]
- [[https://github.com/kadalu/kadalu][kadalu/kadalu: A lightweight Persistent storage solution for Kubernetes / OpenShift / Nomad using GlusterFS in background.]]
- [[https://github.com/kakao/detek][kakao/detek: Extensible problem-detecting CLI tool for reliable Kubernetes cluster operations and rapid problem detections.]]
- [[https://github.com/karmab/kcli][karmab/kcli: Management tool for libvirt/aws/gcp/kubevirt/openstack/ovirt/vsphere/packet]]
- [[https://github.com/kdash-rs/kdash][kdash-rs/kdash: A simple and fast dashboard for Kubernetes]]
- [[https://github.com/kedacore/keda][kedacore/keda: KEDA is a Kubernetes-based Event Driven Autoscaling component. It provides event driven scale for any container running in Kubernetes]]
- [[https://keda.sh/][KEDA | Kubernetes Event-driven Autoscaling]]
  - [[https://kubevirt.io/user-guide/virtual_machines/replicaset/][VirtualMachineInstanceReplicaSet - KubeVirt user guide]]
  - [[https://dev.to/anirudhgarg_99/scale-up-and-down-a-http-triggered-function-app-in-kubernetes-using-keda-4m42][Scale a HTTP Triggered app up and down in Kubernetes using KEDA and Prometheus - DEV Community]]
  - [[https://www.youtube.com/watch?v=nzSyOcBRKhQ][Serverless Track: KEDA - Kubernetes-based Event-Driven Autoscaling - Marek Grabarz - YouTube]]
  - [[https://www.youtube.com/watch?v=DqJTzjtQpOM][Why You Shouldn't Use K8s Autoscaling?!! - YouTube]]
  - [[https://github.com/dailymotion-oss/osiris][dailymotion-oss/osiris: A general purpose, scale-to-zero component for Kubernetes]]
  - [[https://github.com/kedacore/http-add-on][kedacore/http-add-on: Add-on for KEDA to scale HTTP workloads]]
    - [[https://github.com/kedacore/http-add-on/issues/438][Support for scaling custom resources and statefulset · Issue #438 · kedacore/http-add-on]]
  - [[https://habr.com/ru/articles/569760/][Масштабируем приложение в Kubernetes с KEDA (без Prometheus) / Хабр]]
- [[https://github.com/keel-hq/keel][keel-hq/keel: Kubernetes Operator to automate Helm, DaemonSet, StatefulSet & Deployment updates]]
- [[https://github.com/kei6u/kubectl-explore][kei6u/kubectl-explore: A better kubectl explain with the fuzzy finder]]
- [[https://github.com/keptn/keptn][keptn/keptn: Cloud-native application life-cycle orchestration. Keptn automates your SLO-driven multi-stage delivery and operations & remediation of your applications.]]
- [[https://github.com/keyval-dev/odigos][keyval-dev/odigos: Distributed tracing without code changes. 🚀 Instantly monitor any application using OpenTelemetry and eBPF]]
- [[https://github.com/kmdkuk/MCing][kmdkuk/MCing: MCing is a Kubernetes operator for Minecraft server.]]
- [[https://github.com/komodorio/helm-dashboard][komodorio/helm-dashboard: The missing UI for Helm - visualize your releases]]
- [[https://github.com/konveyor/move2kube][konveyor/move2kube: Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes/Openshift.]]
- [[https://github.com/kragniz/tor-ingress-controller][kragniz/tor-ingress-controller: An ingress controller to expose kubernetes services as onion services on the tor network]]
- [[https://github.com/kris-nova/naml][kris-nova/naml: Not another markup language. Framework for replacing Kubernetes YAML with Go.]]
- [[https://github.com/ksync/ksync][ksync/ksync: Sync files between your local system and a kubernetes cluster.]]
- [[https://github.com/kubean-io/kubean][kubean-io/kubean: Kubernetes lifecycle management operator based on kubespray.]]
- [[https://github.com/kubearmor/KubeArmor][kubearmor/KubeArmor: Cloud-native Runtime Security Enforcement System. Workload hardening and implementing least-permissive policies made easy.]]
- [[https://github.com/kubebadges/kubebadges][kubebadges/kubebadges: KubeBadges is an open-source Kubernetes status display tool designed to provide concise, intuitive status badges for services within Kubernetes.]]
- [[https://github.com/kubecfg/kubecfg][kubecfg/kubecfg: A tool for managing complex enterprise Kubernetes environments as code.]]
- [[https://github.com/kubecolor/kubecolor][kubecolor/kubecolor: Colorize your kubectl output]]
- [[https://github.com/kubefarm/dnsmasq-controller][kubefarm/dnsmasq-controller: Run DNS- and DHCP-server inside Kubernetes cluster]]
- [[https://github.com/kubefin/kubefin][kubefin/kubefin: Unified cost allocation insights and optimization in multi-cloud, multi-cluster for Kubernetes]]
- [[https://github.com/kubenav/kubenav][kubenav/kubenav: kubenav is the navigator for your Kubernetes clusters right in your pocket.]]
- [[https://github.com/KubeNetworks/kubevpn][KubeNetworks/kubevpn: KubeVPN is Cloud Native Dev Environment, connect to kubernetes cluster network, you can access remote kubernetes cluster network, remote kubernetes cluster service can also access your local service. and more, you can run your kubernetes pod on local Docker container with same environment、volume、and network.]]
- [[https://github.com/kubeovn/kube-ovn][kubeovn/kube-ovn: A Kubernetes Network Fabric for Enterprises that is Rich in Functions and Easy in Operations (Project under CNCF)]]
- [[https://github.com/kubepug/kubepug][kubepug/kubepug: Kubernetes PreUpGrade (Checker)]]
- [[https://github.com/kuberhealthy/kuberhealthy][kuberhealthy/kuberhealthy: A Kubernetes operator for running synthetic checks as pods. Works great with Prometheus!]]
- [[https://kompose.io/][Kubernetes + Compose = Kompose]]
- [[https://github.com/kubernetes-csi/csi-driver-iscsi][kubernetes-csi/csi-driver-iscsi]]
- [[https://github.com/kubernetes-csi/csi-driver-nfs][kubernetes-csi/csi-driver-nfs: This driver allows Kubernetes to access NFS server on Linux node.]]
- [[https://github.com/kubernetes-csi/csi-driver-smb][kubernetes-csi/csi-driver-smb: This driver allows Kubernetes to access SMB Server on both Linux and Windows nodes.]]
- [[https://github.com/kubernetes/kube-state-metrics][kubernetes/kube-state-metrics: Add-on agent to generate and expose cluster-level metrics.]]
- [[https://www.synacktiv.com/en/publications/kubernetes-namespaces-isolation-what-it-is-what-it-isnt-life-universe-and-everything.html][Kubernetes namespaces isolation - what it is, what it isn't, life,]]
- [[https://github.com/kubernetes/node-problem-detector][kubernetes/node-problem-detector: This is a place for various problem detectors running on the Kubernetes nodes.]]
- [[https://github.com/kubernetes-sigs/cluster-api][kubernetes-sigs/cluster-api: Home for Cluster API, a subproject of sig-cluster-lifecycle]]
- [[https://github.com/kubernetes-sigs/cluster-api][kubernetes-sigs/cluster-api: Home for Cluster API, a subproject of sig-cluster-lifecycle]]
- [[https://github.com/kubernetes-sigs/cluster-api-provider-kubevirt][kubernetes-sigs/cluster-api-provider-kubevirt: Cluster API Provider for KubeVirt]]
- [[https://github.com/kubernetes-sigs/community-images][kubernetes-sigs/community-images: kubectl plugin that displays images running in a Kubernetes cluster that were pulled from community owned repositories and warn the user to switch repositories if needed]]
- [[https://github.com/kubernetes-sigs/descheduler][kubernetes-sigs/descheduler: Descheduler for Kubernetes]]
- [[https://github.com/kubernetes-sigs/hierarchical-namespaces][kubernetes-sigs/hierarchical-namespaces: Home of the Hierarchical Namespace Controller (HNC). Adds hierarchical policies and delegated creation to Kubernetes namespaces for improved in-cluster multitenancy.]]
- [[https://github.com/kubernetes-sigs/hydrophone][kubernetes-sigs/hydrophone: Hydrophone is a lightweight Kubernetes conformance tests runner]]
- [[https://github.com/kubernetes-sigs/ingress2gateway][kubernetes-sigs/ingress2gateway: Convert Ingress resources to Gateway API resources]]
- [[https://github.com/kubernetes-sigs/jobset][kubernetes-sigs/jobset: JobSet: An API for managing a group of Jobs as a unit]]
- [[https://github.com/kubernetes-sigs/kernel-module-management][kubernetes-sigs/kernel-module-management: The kernel module management operator builds, signs and loads kernel modules in Kubernetes clusters..]]
- [[https://github.com/kubernetes-sigs/kubefed][kubernetes-sigs/kubefed: Kubernetes Cluster Federation]]
- [[https://github.com/kubernetes-sigs/kube-scheduler-simulator][kubernetes-sigs/kube-scheduler-simulator: A web-based simulator for the Kubernetes scheduler]]
- [[https://github.com/kubernetes-sigs/kueue][kubernetes-sigs/kueue: Kubernetes-native Job Queueing]]
- [[https://github.com/kubernetes-sigs/kui][kubernetes-sigs/kui: A hybrid command-line/UI development experience for cloud-native development]]
- [[https://github.com/kubernetes-sigs/kustomize][kubernetes-sigs/kustomize: Customization of kubernetes YAML configurations]]
- [[https://github.com/kubernetes-sigs/kwok][kubernetes-sigs/kwok: Simulate thousands of fake kubelets, on a laptop with minimum resource footprint.]]
- [[https://github.com/kubernetes-sigs/metrics-server][kubernetes-sigs/metrics-server: Scalable and efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines.]]
- [[https://github.com/kubernetes-sigs/network-policy-api?tab=readme-ov-file][kubernetes-sigs/network-policy-api: This repo addresses further work involving Kubernetes network security beyond the initial NetworkPolicy resource]]
- [[https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner][kubernetes-sigs/nfs-subdir-external-provisioner: Dynamic sub-dir volume provisioner on a remote NFS server.]]
- [[https://github.com/kubernetes-sigs/node-feature-discovery][kubernetes-sigs/node-feature-discovery: Node feature discovery for Kubernetes]]
- [[https://github.com/kubernetes-sigs/prometheus-adapter][kubernetes-sigs/prometheus-adapter: An implementation of the custom.metrics.k8s.io API using Prometheus]]
- [[https://github.com/kubernetes-sigs/secrets-store-csi-driver][kubernetes-sigs/secrets-store-csi-driver: Secrets Store CSI driver for Kubernetes secrets - Integrates secrets stores with Kubernetes via a CSI volume.]]
- [[https://github.com/kubernetes-sigs/security-profiles-operator][kubernetes-sigs/security-profiles-operator: The Kubernetes Security Profiles Operator]]
- [[https://github.com/kubero-dev/kubero][kubero-dev/kubero: A free and self-hosted Heroku PaaS alternative for Kubernetes that implements GitOps]]
- [[https://github.com/kubeshop/monokle][kubeshop/monokle: 🧐 Monokle Desktop empowers you to better create, understand, and deploy YAML manifests with a visual UI that also provides policy validation and cluster insights.]]
- [[https://github.com/kubeshop/testkube][kubeshop/testkube: ☸️ Kubernetes-native testing framework for test execution and orchestration]]
- [[https://github.com/kubesimplify/ksctl][kubesimplify/ksctl: Cloud Agnostic Kubernetes Management (Core)]]
- [[https://github.com/kubesphere/kube-events][kubesphere/kube-events: K8s Event Exporting, Filtering and Alerting in Multi-Tenant Environment]]
- [[https://github.com/kubetail-org/kubetail?tab=readme-ov-file][kubetail-org/kubetail: Web-based, real-time log viewer for Kubernetes clusters]]
- [[https://github.com/kubevious/kubevious][kubevious/kubevious: Kubevious - application centric Kubernetes UI and continuous assurance provider]]
- [[https://github.com/kube-vip/kube-vip][kube-vip/kube-vip: Kubernetes Control Plane Virtual IP and Load-Balancer]]
- [[https://kubevirt.io/][KubeVirt.io Building a virtualization API for Kubernetes]]
- [[https://www.cncf.io/blog/2022/07/11/kubevirt-kube-ovn-networking-for-cloud-native-virtualization/][KubeVirt + Kube-OVN: Networking for cloud native virtualization | Cloud Native Computing Foundation]]
- [[https://github.com/kubevoy/kubevoy][kubevoy/kubevoy: Slack bot for monitoring resources for kubernetes, It's customizable across multiple channels]]
- [[https://github.com/kubewharf/kelemetry][kubewharf/kelemetry: Global control plane tracing for Kubernetes]]
- [[https://github.com/kubewharf/kubeadmiral][kubewharf/kubeadmiral: Multi-Cluster Kubernetes Orchestration]]
- [[https://github.com/kubewharf/kubegateway][kubewharf/kubegateway: Specific Layer7 Gateway for kube-apiserver]]
- [[https://github.com/kumahq/kuma][kumahq/kuma: 🐻 The multi-zone service mesh for containers, Kubernetes and VMs. Built with Envoy. CNCF Sandbox Project.]]
- [[https://github.com/kvaps/kubectl-build][kvaps/kubectl-build: Build dockerfiles directly in your Kubernetes cluster.]]
- [[https://github.com/kvaps/kubectl-node-shell][kvaps/kubectl-node-shell: Exec into node via kubectl]]
- [[https://github.com/kyverno/kyverno][kyverno/kyverno: Kubernetes Native Policy Management]]
- [[https://github.com/Ladicle/kubectl-rolesum][Ladicle/kubectl-rolesum: Summarize Kubernetes RBAC roles for the specified subjects.]]
- [[https://github.com/larryTheSlap/dumpy][larryTheSlap/dumpy: kubectl plugin that performs tpcdump network captures on containers inside kubernetes]]
- [[https://github.com/lastbackend/lastbackend][lastbackend/lastbackend: System for containerized apps management. From build to scaling.]]
- [[https://github.com/lensapp/lens][lensapp/lens: Lens - The Kubernetes IDE]]
- [[https://github.com/libopenstorage/stork][libopenstorage/stork: Stork - Storage Orchestration Runtime for Kubernetes]]
- [[https://github.com/liggitt/audit2rbac][liggitt/audit2rbac: Autogenerate RBAC policies based on Kubernetes audit logs]]
- [[https://github.com/lightrun-platform/koolkits][lightrun-platform/koolkits: 🧰 Opinionated, language-specific, batteries-included debug container images for Kubernetes.]]
- [[https://github.com/LINBIT/linstor-server][LINBIT/linstor-server: High Performance Software-Defined Block Storage for container, cloud and virtualisation. Fully integrated with Docker, Kubernetes, Openstack, Proxmox etc.]]
- local development
  - Telepresence
  - garden.io
  - tilt.dev
  - okteto.com
  - devspace
  - skaffold
- [[https://github.com/loft-sh/loft][loft-sh/loft: Namespace & Virtual Cluster Manager for Kubernetes - Lightweight Virtual Clusters, Self-Service Provisioning for Engineers and 70% Cost Savings with Sleep Mode]]
- [[https://github.com/loft-sh/vcluster][loft-sh/vcluster: vcluster - Create fully functional virtual Kubernetes clusters - Each vcluster runs inside a namespace of the underlying k8s cluster. It's cheaper than creating separate full-blown clusters and it offers better multi-tenancy and isolation than regular namespaces.]]
- [[https://github.com/loft-sh/vcluster][loft-sh/vcluster: vcluster - Create fully functional virtual Kubernetes clusters - Each vcluster runs inside a namespace of the underlying k8s cluster. It's cheaper than creating separate full-blown clusters and it offers better multi-tenancy and isolation than regular namespaces.]]
- [[https://github.com/loggie-io/loggie][loggie-io/loggie: A lightweight, cloud-native data transfer agent and aggregator]]
- [[https://longhorn.io/][Longhorn - Cloud native distributed block storage for Kubernetes]]
- [[https://github.com/longhorn/longhorn][longhorn/longhorn: Cloud-Native distributed storage built on and for Kubernetes]]
- [[https://github.com/lucky-sideburn/kubeinvaders][lucky-sideburn/kubeinvaders: Gamified Chaos Engineering Tool for Kubernetes]]
- [[https://github.com/madhuakula/kubernetes-goat][madhuakula/kubernetes-goat: Kubernetes Goat is a "Vulnerable by Design" cluster environment to learn and practice Kubernetes security using an interactive hands-on playground 🚀]]
- [[https://github.com/mattfenwick/kubectl-schema][mattfenwick/kubectl-schema: kubectl plugin to query kubernetes openapi schemas]]
- [[https://github.com/memphisdev/memphis][memphisdev/memphis: Memphis.dev is a highly scalable and effortless data streaming platform]]
- [[https://meshery.io/][Meshery The Kubernetes and Cloud Native Manager - an extensible developer platform | Meshery]]
- [[https://github.com/metalbear-co/mirrord][metalbear-co/mirrord: Connect your local process and your cloud environment, and run local code in cloud conditions.]]
- [[https://github.com/metallb/metallb][metallb/metallb: A network load-balancer implementation for Kubernetes using standard routing protocols]]
- [[https://github.com/metal-stack/csi-driver-lvm][metal-stack/csi-driver-lvm]]
- [[https://opensource.com/article/22/5/migrating-databases-kubernetes-using-konveyor][Migrate databases to Kubernetes using Konveyor | Opensource.com]]
  - [[https://github.com/konveyor/tackle-diva][konveyor/tackle-diva: Tackle Data-intensive Validity Analyzer]]
- [[https://opensource.com/article/21/6/migrate-vms-kubernetes-forklift][Migrate virtual machines to Kubernetes with this new tool | Opensource.com]]
- [[https://github.com/MoJo2600/pihole-kubernetes][MoJo2600/pihole-kubernetes: PiHole on kubernetes]]
- [[https://artifacthub.io/packages/helm/si-gitops/mok][mok 0.9.7 · sheogorath/si-gitops: Mail on Kubernetes (MoK) is a project to deploy a functional mailserver that runs without a database server on Kubernetes, taking advantage of configmaps and secret.]]
- [[https://github.com/MoserMichael/s9k][MoserMichael/s9k: dashboard/web app for managing kubernetes clusters, with similar functionality as k9s]]
- [[https://github.com/msfidelis/kubedump][msfidelis/kubedump: :whale: :rocket: Simple tool to dump and restore kubernetes resources]]
- [[https://github.com/mycrEEpy/failover-service-operator][mycrEEpy/failover-service-operator: Kubernetes operator for providing active-passive services on top of headless services for statefulsets.]]
- [[https://github.com/NCCloud/mayfly][NCCloud/mayfly: An operator to manage ephemeral Kubernetes resources.  Mayfly is a Kubernetes operator that enables you to create temporary resources on the cluster that will expire after a certain period of time.]]
- [[https://github.com/NCCloud/watchtower][NCCloud/watchtower: Effortlessly monitor your Kubernetes resource activity]]
- [[https://github.com/nmstate/kubernetes-nmstate][nmstate/kubernetes-nmstate: Declarative node network configuration driven through Kubernetes API.]]
- [[https://github.com/Nordix/Meridio][Nordix/Meridio: Facilitator of attraction and distribution of external traffic within Kubernetes via secondary networks]]
- [[https://github.com/octarinesec/kube-scan][octarinesec/kube-scan: kube-scan: Octarine k8s cluster risk assessment tool]]
- [[https://github.com/okteto/okteto][okteto/okteto: Develop your applications directly in your Kubernetes Cluster]]
- [[https://github.com/omrikiei/ktunnel][omrikiei/ktunnel: A cli that exposes your local resources to kubernetes]]
- [[https://github.com/onedr0p/home-ops][onedr0p/home-ops: Wife approved HomeOps driven by Kubernetes and GitOps using Flux]]
- [[https://github.com/opcr-io/policy][opcr-io/policy: CLI for building OPA policies into OCI images]]
- [[https://github.com/openclarity/vmclarity][openclarity/vmclarity: VMClarity is an open source tool for agentless detection and management of Virtual Machine Software Bill Of Materials (SBOM) and security threats such as vulnerabilities, exploits, malware, rootkits, misconfigurations and leaked secrets.]]
- [[https://github.com/opencost/opencost][opencost/opencost: Cross-cloud cost allocation models for Kubernetes workloads]]
- [[https://github.com/openebs/openebs][openebs/openebs: Leading Open Source Container Attached Storage, built using Cloud Native Architecture, simplifies running Stateful Applications on Kubernetes.]]
- [[https://github.com/openebs/zfs-localpv][openebs/zfs-localpv: CSI Driver for dynamic provisioning of Persistent Local Volumes for Kubernetes using ZFS.]]
- [[https://github.com/openfga/openfga][openfga/openfga: A high performance and flexible authorization/permission engine built for developers and inspired by Google Zanzibar]]
- [[https://github.com/openkruise/kruise][openkruise/kruise: Automate application management on Kubernetes (project under CNCF)]]
- [[https://github.com/open-policy-agent/gatekeeper][open-policy-agent/gatekeeper: Gatekeeper - Policy Controller for Kubernetes]]
- [[https://github.com/openshift/cluster-kube-scheduler-operator][openshift/cluster-kube-scheduler-operator: Installs and maintains the kube-scheduler on a cluster.]]
- [[https://github.com/ori-edge/k8s_gateway][ori-edge/k8s_gateway: A CoreDNS plugin to resolve all types of external Kubernetes resources]]
- [[https://github.com/otterize/network-mapper][otterize/network-mapper: Map Kubernetes in-cluster traffic and export as text, intents, or an image]]
- [[https://github.com/PaladinCloud/CE][PaladinCloud/CE: Paladin Cloud is an extensible, Security-as-Code platform.]]
- [[https://github.com/paralus/paralus][paralus/paralus: All-in-one Kubernetes access manager. User-level credentials, RBAC, SSO, audit logs.]]
- [[https://github.com/parca-dev/parca][parca-dev/parca: Continuous profiling for analysis of CPU and memory usage, down to the line number and throughout time. Saving infrastructure cost, improving performance, and increasing reliability.]]
- [[https://github.com/patrickdappollonio/kubectl-slice][patrickdappollonio/kubectl-slice: Split multiple Kubernetes files into smaller files with ease. Split multi-YAML files into individual files.]]
- [[https://github.com/pete911/kubectl-image][pete911/kubectl-image: list container images running in kubernetes cluster]]
- [[https://github.com/pete911/kubectl-prom][pete911/kubectl-prom: kubectl plugin to query prometheus]]
- [[https://github.com/pete911/kubectl-rr][pete911/kubectl-rr: kubernetes resource recommendation]]
- [[https://github.com/pete911/vault-auth-kubernetes][pete911/vault-auth-kubernetes: fully automated vault authentication backend for kubernetes service accounts]]
- [[https://github.com/piraeusdatastore/linstor-csi][piraeusdatastore/linstor-csi: CSI plugin for LINSTOR]]
- [[https://github.com/pixel-point/kube-forwarder][pixel-point/kube-forwarder: Easy to use Kubernetes port forwarding manager]]
- [[https://github.com/pixie-io/pixie][pixie-io/pixie: Instant Kubernetes-Native Application Observability]]
- [[https://github.com/planetlabs/draino][planetlabs/draino: Automatically cordon and drain Kubernetes nodes based on node conditions]]
- [[https://github.com/porter-dev/porter][porter-dev/porter: Kubernetes powered PaaS that runs in your own cloud.]]
- [[https://github.com/portworx/torpedo][portworx/torpedo: A test suite to qualify storage providers for stateful containers running in a cluster.]]
- [[https://github.com/poseidon/scuttle][poseidon/scuttle: Kubelet graceful node drain/delete and spot termination watcher]]
- [[https://cncf.pravega.io/][Pravega – A Reliable Stream Storage System]]
- [[https://habr.com/ru/company/nixys/blog/682612/][Pre-Commit хуки, о которых DevOps-инженер должен знать, чтобы управлять Kubernetes / Хабр]]
- [[https://github.com/projectcalico/calico][projectcalico/calico: Cloud native networking and network security]]
- [[https://github.com/project-copacetic/copacetic][project-copacetic/copacetic: 📦 CLI tool for directly patching container images using reports from vulnerability scanners]]
- [[https://github.com/prometheus-operator/prometheus-operator][prometheus-operator/prometheus-operator: Prometheus Operator creates/configures/manages Prometheus clusters atop Kubernetes]]
- [[https://github.com/pusher/k8s-auth-example][pusher/k8s-auth-example: Example Kubernetes Authentication helper. Performs OIDC login and configures Kubectl appropriately.]]
- [[https://github.com/pyrra-dev/pyrra][pyrra-dev/pyrra: Making SLOs with Prometheus manageable, accessible, and easy to use for everyone!]]
- [[https://github.com/pytorch/serve][pytorch/serve: Serve, optimize and scale PyTorch models in production]]
- [[https://github.com/Qovery/pleco][Qovery/pleco: Automatically removes Cloud managed services and Kubernetes resources based on tags with TTL]]
- [[https://github.com/rancher/k3s][rancher/k3s: Lightweight Kubernetes]]
- [[https://github.com/rancher/kim][rancher/kim: In ur kubernetes, buildin ur imagez]]
- [[https://github.com/rancher-sandbox/rancher-desktop][rancher-sandbox/rancher-desktop: Container Management and Kubernetes on the Desktop]]
- [[https://github.com/reactive-tech/kubegres][reactive-tech/kubegres: Kubegres is a Kubernetes operator allowing to create a cluster of PostgreSql instances and manage databases replication, failover and backup.]]
- [[https://github.com/ricoberger/vault-secrets-operator][ricoberger/vault-secrets-operator: Create Kubernetes secrets from Vault for a secure GitOps based workflow.]]
- [[https://github.com/rlipscombe/erlang-cluster][rlipscombe/erlang-cluster: Erlang nodes running in a cluster on the nodes in a Kubernetes cluster :)]]
- [[https://github.com/robert-nemet/klock][robert-nemet/klock: Lock for K8s]]
- [[https://github.com/robscott/kube-capacity][robscott/kube-capacity: A simple CLI that provides an overview of the resource requests, limits, and utilization in a Kubernetes cluster]]
- [[https://github.com/robusta-dev/krr][robusta-dev/krr: Prometheus-based Kubernetes Resource Recommendations]]
- [[https://github.com/robusta-dev/kubernetes-chatgpt-bot][robusta-dev/kubernetes-chatgpt-bot: A ChatGPT bot for Kubernetes issues.]]
- [[https://github.com/rook/rook][rook/rook: Storage Orchestration for Kubernetes]]
- [[https://github.com/rook/rook][rook/rook: Storage Orchestration for Kubernetes]]
- [[https://github.com/RunLLM/aqueduct][RunLLM/aqueduct: Aqueduct allows you to run LLM and ML workloads on any cloud infrastructure.]]
- [[https://charlottemach.com/2020/11/03/windows-kubevirt-k3s.html][Running a Windows VM on KubeVirt on K3s · cookies and containers]]
- [[https://github.com/sapcc/kube-parrot][sapcc/kube-parrot: A Kubernetes Controller that Dynamically Announces Routes with BGP]]
- [[https://github.com/SAP/redis-operator][SAP/redis-operator: Manage Redis clusters in Kubernetes]]
- [[https://github.com/saschagrunert/kubernix][saschagrunert/kubernix: Single dependency Kubernetes clusters for local testing, experimenting and development]]
- [[https://github.com/saschagrunert/kubernix][saschagrunert/kubernix: Single dependency Kubernetes clusters for local testing, experimenting and development]]
- [[https://github.com/sbstp/kubie][sbstp/kubie: A more powerful alternative to kubectx and kubens]]
- [[https://schemahero.io/][SchemaHero - A modern approach to database schema migrations]]
- [[https://github.com/seacrew/helm-compose][seacrew/helm-compose: Helm Compose is a helm plugin for managing multiple releases of one or many charts within a single configuration file.]]
- [[https://github.com/sealerio/sealer][sealerio/sealer: A tool to seal application's all dependencies and Kubernetes into CloudImage, distribute this application anywhere via CloudImage, and run it within any cluster in one command.]]
- [[https://github.com/senthilrch/kube-fledged][senthilrch/kube-fledged: A kubernetes operator for creating and managing a cache of container images directly on the cluster worker nodes, so application pods start almost instantly]]
- [[https://github.com/shibumi/kubectl-htpasswd][shibumi/kubectl-htpasswd: kubectl plugin for generating nginx-ingress compatible basic-auth secrets on kubernetes clusters]]
- [[https://shipwright.io/][Shipwright - extensible framework for building container images on Kubernetes]]
- [[https://github.com/Shopify/kubeaudit][Shopify/kubeaudit: kubeaudit helps you audit your Kubernetes clusters against common security controls]]
- [[https://github.com/sighupio/permission-manager][sighupio/permission-manager: Permission Manager is a project that brings sanity to Kubernetes RBAC and Users management, Web UI FTW]]
- [[https://github.com/slok/simple-ingress-external-auth][slok/simple-ingress-external-auth: A very simple external authentication service for Kubernetes ingresses (ingress-nginx, traefik...)]]
- [[https://github.com/soluble-ai/kubetap][soluble-ai/kubetap: Kubectl plugin to interactively proxy Kubernetes Services with ease]]
- [[https://github.com/Soluto/kamus][Soluto/kamus: An open source, git-ops, zero-trust secret encryption and decryption solution for Kubernetes applications]]
- [[https://github.com/sozercan/kubectl-ai][sozercan/kubectl-ai: ✨ Kubectl plugin for OpenAI GPT]]
- [[https://github.com/spacecloud-io/space-cloud][spacecloud-io/space-cloud: Open source Firebase + Heroku to develop, scale and secure serverless apps on Kubernetes]]
- [[https://github.com/spacecloud-io/space-cloud][spacecloud-io/space-cloud: Open source Firebase + Heroku to develop, scale and secure serverless apps on Kubernetes]]
- [[https://github.com/spidernet-io/spiderpool][spidernet-io/spiderpool: underlay network solution of cloud native, for bare metal, VM and public cloud]]
- [[https://github.com/spotahome/redis-operator][spotahome/redis-operator: Redis Operator creates/configures/manages high availability redis with sentinel automatic failover atop Kubernetes.]]
- [[https://github.com/squat/generic-device-plugin][squat/generic-device-plugin: A Kubernetes device plugin to schedule generic Linux devices]]
- [[https://github.com/squat/kilo][squat/kilo: Kilo is a multi-cloud network overlay built on WireGuard and designed for Kubernetes (k8s + wg = kg)]]
- [[https://github.com/stackrox/kube-linter][stackrox/kube-linter: KubeLinter is a static analysis tool that checks Kubernetes YAML files and Helm charts to ensure the applications represented in them adhere to best practices.]]
- [[https://github.com/stackrox/stackrox][stackrox/stackrox: The StackRox Kubernetes Security Platform performs a risk analysis of the container environment, delivers visibility and runtime alerts, and provides recommendations to proactively improve security by hardening the environment.]]
- [[https://github.com/stakater/application][stakater/application: Generic helm chart for all kind of applications]]
- [[https://github.com/stakater/IngressMonitorController][stakater/IngressMonitorController: A Kubernetes controller to watch ingresses and create liveness alerts for your apps/microservices in UptimeRobot, StatusCake, Pingdom, etc. – [✩Star] if you're using it!]]
- [[https://github.com/stakater/Reloader][stakater/Reloader: A Kubernetes controller to watch changes in ConfigMap and Secrets and do rolling upgrades on Pods with their associated Deployment, StatefulSet, DaemonSet and DeploymentConfig – [✩Star] if you're using it!]]
- [[https://github.com/stashed/stash][stashed/stash: 🛅 Backup your Kubernetes Stateful Applications]]
- [[https://github.com/stern/stern][stern/stern: ⎈ Multi pod and container log tailing for Kubernetes -- Friendly fork of https://github.com/wercker/stern]]
- [[https://github.com/stevelacy/kuberhaus][stevelacy/kuberhaus: Kubernetes resource dashboard with node/pod layout and resource requests]]
- [[https://github.com/stolostron/multicluster-observability-operator][stolostron/multicluster-observability-operator: Operator for Multi-Cluster Monitoring with Thanos.]]
- [[https://github.com/sustainable-computing-io/kepler][sustainable-computing-io/kepler: Kepler (Kubernetes-based Efficient Power Level Exporter) uses eBPF to probe energy related system stats and exports as Prometheus metrics]]
- [[https://github.com/tdihp/dspcap][tdihp/dspcap: A humble bash script set that uses daemonset to capture tcpdump from all k8s nodes, then collect the captures.]]
- [[https://github.com/teamcode-inc/kubeorbit][teamcode-inc/kubeorbit: Test your application on Kubernetes in a brand new simple way⚡]]
- [[https://github.com/team-soteria/rback][team-soteria/rback: RBAC in Kubernetes visualizer]]
- [[https://habr.com/ru/company/nixys/blog/481992/][Tekton Pipeline — Kubernetes-нативные pipelines / Хабр]]
- [[https://github.com/teletrace/teletrace][teletrace/teletrace: Open-Source Tracing Platform]]
- [[https://opensource.com/article/21/6/chaos-mesh-kubernetes][Test your Kubernetes experiments with an open source web interface | Opensource.com]]
- [[https://github.com/theelderbeever/kubectl-guardrails][theelderbeever/kubectl-guardrails: Introduce a critical checkpoint for potentially hazardous operations]]
- [[https://github.com/timebertt/kubernetes-controller-sharding][timebertt/kubernetes-controller-sharding: Horizontally Scalable Kubernetes Controllers: distribute reconciliation of Kubernetes objects across multiple controller instances]]
- [[https://tinkerbell.org/][Tinkerbell - Bare Metal Automation]]
- [[https://github.com/tinyzimmer/android-farm-operator][tinyzimmer/android-farm-operator: A Kubernetes operator for OpenSTF and Android Farm Deployments]]
- [[https://github.com/tkestack/tke][tkestack/tke: Native Kubernetes container management platform supporting multi-tenant and multi-cluster]]
- [[https://github.com/toboshii/hajimari][toboshii/hajimari: Hajimari is a beautiful & customizable browser startpage/dashboard with Kubernetes application discovery.]]
- [[https://github.com/tohjustin/kube-lineage][tohjustin/kube-lineage: A CLI tool to display all dependencies or dependents of an object in a Kubernetes cluster.]]
- [[https://github.com/tommy351/pullup][tommy351/pullup: Update Kubernetes resources by webhooks.]]
- [[https://github.com/topolvm/pvc-autoresizer][topolvm/pvc-autoresizer: Auto-resize PersistentVolumeClaim objects based on Prometheus metrics]]
- [[https://github.com/traefik/mesh][traefik/mesh: Traefik Mesh - Simpler Service Mesh]]
- [[https://github.com/tricorder-observability/starship][tricorder-observability/starship: Starship: next-generation Observability platform built with eBPF+WASM]]
- [[https://github.com/Trois-Six/k8s-diagrams][Trois-Six/k8s-diagrams: Create diagrams from the Kubernetes API with go-diagrams.]]
- [[https://github.com/troytop/cfk8sterm][troytop/cfk8sterm: A ttyd web terminal image on Alpine with cf, kubectl, and other tools]]
- [[https://github.com/txn2/kubefwd][txn2/kubefwd: Bulk port forwarding Kubernetes services for local development.]]
- [[https://www.getunleash.io/][Unleash: Open-Source Feature Management for Enterprises]]
- [[https://github.com/up9inc/mizu][up9inc/mizu: API traffic viewer for Kubernetes enabling you to view all API communication between microservices. Think TCPDump and Wireshark re-invented for Kubernetes]]
- [[https://developers.redhat.com/blog/2020/11/18/using-multus-and-datavolume-in-kubevirt#using_multus_in_kubevirt][Using Multus and DataVolume in KubeVirt | Red Hat Developer]]
- [[https://github.com/utkuozdemir/pv-migrate][utkuozdemir/pv-migrate: CLI tool to easily migrate Kubernetes persistent volumes]]
- [[https://github.com/vfx1b/kubectl-really-get-all][vfx1b/kubectl-really-get-all: A kubectl get plugin that allows you to list every resource in your cluster]]
- [[https://github.com/vmware-tanzu/octant][vmware-tanzu/octant: Highly extensible platform for developers to better understand the complexity of Kubernetes clusters.]]
- [[https://github.com/vmware-tanzu/velero][vmware-tanzu/velero: Backup and migrate Kubernetes applications and their persistent volumes]]
- [[https://github.com/wavyland/wavy/tree/main][wavyland/wavy: Wavy is a toolset for running GUI applications on Kubernetes]]
- [[https://github.com/weaveworks/kured][weaveworks/kured: Kubernetes Reboot Daemon]]
- [[https://github.com/weaveworks/tf-controller][weaveworks/tf-controller: A GitOps Terraform controller for Kubernetes]]
- [[https://github.com/we-dcode/kubetunnel][we-dcode/kubetunnel: Develop microservices locally while being connected to your Kubernetes environment]]
- [[https://scribe.bus-hit.me/flant-com/kubernetes-operator-in-python-451f2d2e33f3][Writing a Kubernetes Operator in Python without frameworks and SDK]]
- [[https://github.com/XenitAB/node-ttl][XenitAB/node-ttl: Enforces a time to live (TTL) on Kubernetes nodes and evicts nodes which have expired.]]
- [[https://github.com/XenitAB/spegel][XenitAB/spegel: Stateless cluster local OCI registry mirror.]]
- [[https://github.com/xpipe-io/xpipe][xpipe-io/xpipe: Your entire server infrastructure at your fingertips]]
- [[https://github.com/yahoo/k8s-namespace-guard][yahoo/k8s-namespace-guard: K8s - Admission controller for guarding namespace]]
- [[https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml][YAML and Kubernetes syntax support - Visual Studio Marketplace]]
- [[https://github.com/yannh/kubeconform][yannh/kubeconform: A FAST Kubernetes manifests validator, with support for Custom Resources!]]
- [[https://github.com/yashbhutwala/kubectl-df-pv][yashbhutwala/kubectl-df-pv: kubectl plugin - giving admins df (disk free) like utility for persistent volumes]]
- [[https://github.com/yokawasa/kubectl-plugin-ssh-jump][yokawasa/kubectl-plugin-ssh-jump: A kubectl plugin to access nodes or remote services using a SSH jump Pod]]
- [[https://github.com/yonahd/kor][yonahd/kor: A Golang Tool to discover unused Kubernetes Resources]]
- [[https://github.com/yunionio/cloudpods][yunionio/cloudpods: A cloud-native open-source unified multi-cloud and hybrid-cloud platform. 开源、云原生的多云管理及混合云融合平台]]
- [[https://github.com/zalando/skipper][zalando/skipper: An HTTP router and reverse proxy for service composition, including use cases like Kubernetes Ingress]]
- [[https://github.com/zegl/kube-score][zegl/kube-score: Kubernetes object analysis with recommendations for improved reliability and security]]
- [[https://github.com/zekker6/hosts-generator][zekker6/hosts-generator: Tool which is able to generate hosts file content for services discovered from different sources]]
- [[https://habr.com/ru/post/540220/][Настройка отказоустойчивого кластера Kubernetes на серверах с публичной и приватной сетью с помощью Kubeadm / Хабр]]
- [[https://habr.com/ru/company/flant/blog/524196/#][Обзор k9s — продвинутого терминального интерфейса для Kubernetes]]
- [[https://habr.com/ru/company/flant/blog/575972/][Обзор Kalm — веб-интерфейса для деплоя приложений и управления ими в Kubernetes / Хабр]]
- [[https://habr.com/ru/post/542834/][Первый взгляд на Tekton Pipelines / Хабр]]
- [[https://habr.com/ru/company/flant/blog/676678/][Снапшоты в Kubernetes: что это и как ими пользоваться / Хабр]]

** [[https://github.com/WoozyMasta/kube-dump][WoozyMasta/kube-dump: Backup a Kubernetes cluster as a yaml manifest]]
#+begin_src shell
  docker run \
         --network host \
         --tty \
         --interactive \
         --rm \
         --volume "${HOME}/.kube:/.kube" \
         --volume "${HOME}/ansible-out/files/kubernetes:/dump" \
         woozymasta/kube-dump:latest \
         dump-namespaces -d /dump --kube-config /.kube/config
#+end_src

** minikube
- [[https://habr.com/ru/company/vk/blog/648117/][Как работать с Minikube: рекомендации и полезные советы / Хабр]]
- [[https://github.com/kubevirt/demo][kubevirt/demo: Easy to use KubeVirt demo based on minikube.]]

** kubenav
#+begin_example
  oleg@guixsd ~/bin$ /home/oleg/src/git.savannah.gnu.org/git/guix/pre-inst-env guix describe
  Git checkout:
    repository: /home/oleg/src/git.savannah.gnu.org/git/guix/
    branch: master
    commit: ced80c94b3402f8f34c1e45b10685f7dfc589a13
#+end_example

#+begin_src bash
  wget https://github.com/kubenav/kubenav/releases/download/v4.2.3/kubenav-x86_64.AppImage

  /home/oleg/src/git.savannah.gnu.org/git/guix/pre-inst-env guix shell gcc-toolchain \
  --container \
  --network \
  --emulate-fhs \
  --development ungoogled-chromium \
  --preserve='^DISPLAY$' \
  --preserve='^XAUTHORITY$' \
  --expose=$XAUTHORITY \
  --preserve='^DBUS_' \
  --expose=/var/run/dbus \
  --expose=/sys/dev \
  --expose=/sys/devices \
  --expose=/dev/dri \
  -- /bin/sh -c 'LD_LIBRARY_PATH=/gnu/store/l684qgqlrqkbsh8jffp9d8ag6vrpcwgs-gcc-11.3.0-lib/lib:/lib:/lib/nss ./kubenav-x86_64.AppImage --appimage-extract-and-run'
#+end_src

** crossplane
- [[https://github.com/crossplane-contrib/provider-ansible][crossplane-contrib/provider-ansible: Crossplane Ansible provider]]
- [[https://github.com/crossplane/provider-template][crossplane/provider-template: A template for building @crossplane providers.]]
- [[https://github.com/crossplane/terrajet][crossplane/terrajet: Generate Crossplane Providers from any Terraform Provider]]

* [[https://blog.cloudowski.com/articles/how-to-modify-containers-wihtout-rebuilding/][How to modify containers without rebuilding their image]]


How to modify containers without rebuilding their image

6 minute read

Containers are a beautiful piece of technology that ease the development of modern applications and also the maintenance of modern environments. One thing that draws many people to them is how they reduce the time required to set up a service, or a whole environment, with everything included. It is possible mainly because there are so many container images available and ready to use. You will probably need to build your own container images with your applications, but many containers in your environment will use prebuilt images prepared by someone else. It’s especially worth considering for software that is provided by the software vendor or a trusted group of developers like it has been done in the case of “official” images published on Docker Hub. In both cases, it makes your life easier by letting someone else take care of updates, packaging new versions, and making sure it works.
But what if you want to change something in those images? Maybe it’s a minor change or something bigger that is specific for your particular usage of the service. The first instinct may tell you to rebuild that image. This, however, brings some overhead - these images will have to be published, rebuilt when new upstream versions are published, and you lose most of the benefits that come with those prebuilt versions.
There is an alternative to that - actually, I found four of them which I will describe below. These solutions will allow you to keep all the benefits and adjust the behavior of running containers in a seamless way.
Method 1 - init-containers

Init-containers were created to provide additional functionality to the main container (or containers) defined in a Pod. They are executed before the main container and can use a different container image. In case of any failure, they will prevent the main container from starting. All logs can be easily retrieved and troubleshooting is fairly simple - they are fetched just like any other container defined in a Pod by providing its name. This methods is quiote popular among services such as databases to initialize and configure them based on configuration parameters.
Example

The following example uses a dedicated empty volume for storing data initialized by an init-container. In this specific case, it’s just a simple “echo” command, but in a real-world scenario, this can be a script that does something more complex.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx-init
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      initContainers:
        - name: prepare-webpage
          image: busybox:1.28
          command: ["sh", "-c"]
          args: [
              "set -x;
              echo '<h2>Page prepared by an init container</h2>' > /web/index.html;
              echo 'Init finished successfully'
              ",
            ]
          volumeMounts:
            - mountPath: /web
              name: web
      containers:
        - image: nginx:1.19
          name: nginx
          volumeMounts:
            - mountPath: /usr/share/nginx/html/
              name: web
          ports:
            - containerPort: 80
              name: http
      volumes:
        - name: web
          emptyDir: {}

Method 2 - post-start hook

A Post-start hook can be used to execute some action just after the main container starts. It can be either a script executed in the same context as the container or an HTTP request that is executed against a defined endpoint. In most cases, it would probably be a shell script. Pod stays in the ContainerCreating state until this script ends. It can be tricky to debug since there are no logs available. There are more caveats and this should be used only for simple, non-invasive actions. The best feature of this method is that the script is executed when the service in the main container starts and can be used to interact with the service (e.g. by executing some API requests). With a proper readinessProbe configuration, this can give a nice way of initializing the application before any requests are allowed.
Example

In the following example a post-start hook executes the echo command, but again - this can be anything that uses the same set of files available on the container filesystem in order to perform some sort of initialization.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx-hook
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - image: nginx:1.19
          name: nginx
          ports:
            - containerPort: 80
              name: http
          lifecycle:
            postStart:
              exec:
                command:
                  [
                    "sh",
                    "-c",
                    "sleep 5;set -x; echo '<h2>Page prepared by a PostStart hook</h2>' > /usr/share/nginx/html/index.html",
                  ]

Method 3 - sidecar container

This method leverages the concept of the Pod where multiple containers run at the same time sharing IPC and network kernel namespaces. It’s been widely used in the Kubernetes ecosystem by projects such as Istio, Consul Connect, and many others. The assumption here is that all containers run simultaneously which makes it a little bit tricky to use a sidecar container to modify the behaviour of the main container. But it’s doable and it can be used to interact with the running application or a service. I’ve been using this feature with the Jenkins helm chart where there’s a sidecar container responsible for reading ConfigMap objects with Configuration-as-Code config entries.
Example

Nothing new here, just the “echo” command with a little caveat - since sidecar containers must obey restartPolicy setting, they must run after they finish their actions and thus it uses a simple while infinite loop. In more advanced cases this would be rather some small daemon (or a loop that checks some state) that runs like a service.


apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx-sidecar
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - image: nginx:1.19
          name: nginx
          volumeMounts:
            - mountPath: /usr/share/nginx/html/
              name: web
          ports:
            - containerPort: 80
              name: http
        - name: prepare-webpage
          image: busybox:1.28
          command: ["sh", "-c"]
          args: [
              "set -x;
              echo '<h2>Page prepared by a sidecar container</h2>' > /web/index.html;
              while :;do sleep 9999;done
              ",
            ]
          volumeMounts:
            - mountPath: /web
              name: web
      volumes:
        - name: web
          emptyDir: {}

Method 4 - entrypoint

The last method uses the same container image and is similar to the Post-start hook except it runs before the main app or service. As you probably know in every container image there is an ENTRYPOINT command defined (explicitly or implicitly) and we can leverage it to execute some arbitrary scripts. It is often used by many official images and in this method we will just prepend our own script to modify the behavior of the main container. In more advanced scenarios you could actually provide a modified version of the original entrypoint file.
Example

This method is a little bit more complex and involves creating a ConfigMap with a script content that is executed before the main entrypoint. Our script for modifying nginx entrypoint is embedded in the following ConfigMap

apiVersion: v1
kind: ConfigMap
metadata:
  name: scripts
data:
  prestart-script.sh: |-
    #!/usr/bin/env bash

    echo '<h2>Page prepared by a script executed before entrypoint container</h2>' > /usr/share/nginx/html/index.html

    exec /docker-entrypoint.sh nginx -g "daemon off;" # it's "ENTRYPOINT CMD" extracted from the main container image definition

One thing that is very important is the last line with exec. It executes the original entrypoint script and must match it exactly as it is defined in the Dockerfile. In this case it requires additional arguments that are defined in the CMD.

Now let’s define the Deployment object

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx-script
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - image: nginx:1.19
          name: nginx
          command: ["bash", "-c", "/scripts/prestart-script.sh"]
          ports:
            - containerPort: 80
              name: http
          volumeMounts:
            - mountPath: /scripts
              name: scripts
      volumes:
        - name: scripts
          configMap:
            name: scripts
            defaultMode: 0755 # <- this is important

That is pretty straightforward - we override the entrypoint with command and we also must make sure our script is mounted with proper permissions (thus defaultMode needs to be defined).
Comparison table

Containers are about reusability and often it’s much easier to make small adjustments without rebuilding the whole container image and take over the responsibility of publishing and maintaining it. It’s just an implementation of the KISS principle.

* Windows
- [[https://tensorworks.com.au/blog/bringing-full-gpu-support-to-windows-containers-in-kubernetes/][Bringing full GPU support to Windows containers in Kubernetes | TensorWorks]]
- [[https://github.com/MicrosoftDocs/SupportArticles-docs/blob/main/support/windows-server/containers/support-for-windows-containers-docker-on-premises-scenarios.md][SupportArticles-docs/support-for-windows-containers-docker-on-premises-scenarios.md at main · MicrosoftDocs/SupportArticles-docs]]
- [[https://thomasvanlaere.com/posts/2021/06/exploring-windows-containers/][Thomas Van Laere | Exploring Windows Containers]]
- [[https://samcogan.com/using-host-process-containers-to-interact-with-kubernetes-windows-nodes/][Using Host Process Containers to Interact with Kubernetes Windows Nodes - samcogan.com]]
** [[https://www.jamessturtevant.com/posts/Windows-Containers-on-Windows-10-without-Docker-using-Containerd/][Windows Containers on Windows 10 without Docker (using Containerd) · James Sturtevant]]
*** [[https://github.com/microsoft/Windows-Containers/issues/210][How to use containerd with crictl on Windows Server 2022 · Issue #210 · microsoft/Windows-Containers]]
All features:
: Add-WindowsFeature Containers,Hyper\-V,Hyper\-V\-Tools,Hyper\-V\-PowerShell \-Restart \-IncludeManagementTools

Minimum is:
: Add-WindowsFeature Containers -IncludeManagementTools

** Issues
- [[https://github.com/rancher/rke2/issues/3915][kubelet fails to pull docker.io/rancher/pause:3.6 on Windows 11 Pro node · Issue #3915 · rancher/rke2]]
  - [[https://github.com/containerd/containerd/pull/8137][Allow containerd on Windows 11 to use Windows Server 2022 images by hach-que · Pull Request #8137 · containerd/containerd]]

* Installing Cilium Service Mesh with external Kubernetes Control Plane (illumos)

Tony Norlin on 2023-06-02
Installing Cilium Service Mesh with the Kubernetes Control Plane externally (illumos)

In this part we will install the components and quickly check out some of the features in the Cilium Service Mesh to get a glimpse of the current state.
Hubble UI, as it can look with an external Control Plane, two clusters and a vm connected to each other — Cluster Mesh

My employer (Conoa) run a weekly meetup channel where we (the tech people) have an opportunity to talk about whatever interests us in the cloud native space.

I was asked if I could arrange a talk during the last Thursday of may about something that have caught my interest and I agreed to do it if I could choose to talk about Cilium (for those that know me, it was perhaps no bigger surprise as I happen to like this project and the folks behind it).

I had almost a week to prepare a demo (mostly based on the https://github.com/isovalent/cilium-grafana-observability-demo and from what I’ve seen in the awesome labs available at https://isovalent.com/resource-library/labs/ — unfortunately there are not so much fresh code in GH as I would like) about the Service Mesh (in Swedish).

Unfortunately, with limited time I was only showing a subset of what I initially had in mind, but at least I managed to put up a demo and the Demo Gods were quite nice to me:

The demo preparations was done in my ordinary environment, which happens to run an external Control Plane (in my port of Kubernetes to illumos), placed in another network segment (VLAN) than the Data Plane.

While I had in mind to run my demos in that environment, I felt that it would perhaps have been counter-intuitive to demonstrate an environment that lacks components that most(?) Kubernetes users would perhaps expect to be there:

$ kubectl get pod -n kube-system        
NAME                               READY   STATUS                 RESTARTS   AGE
cilium-bdvbd                       1/1     Running                0          21h
cilium-m6dgf                       1/1     Running                0          21h
cilium-operator-78ff8866bf-k98zq   1/1     Running                0          21h
cilium-operator-78ff8866bf-tjvhx   1/1     Running                0          21h
coredns-5f47698cfc-5nkr7           1/1     Running                0          26h
coredns-5f47698cfc-qhnl9           1/1     Running                0          26h
hubble-relay-5447546447-sqcsm      1/1     Running                0          21h
hubble-ui-694cf76f4c-m4fg4         2/2     Running                0          21h

With that in mind it still wouldn’t stop me from at least preparing and testing out the concepts in my “managed” Kubernetes solution, if it works there, it works almost anywhere?

The steps involved will still be the same, except for the BGP configuration (which will be environment specific anyway) where I try to steer traffic more directly between the network zones (VLANs), without passing through other network zones.

The output in Hubble, however, will be a bit different as it will display more components.
Creating a demo environment
Prerequisites

    Working control plane (see my earlier articles on how to set it up, it still holds up until now in concepts. There are some differences where this article installs components, just skip those parts in the old articles.).

$ kubectl version --output=yaml
clientVersion:
  buildDate: "2023-04-14T18:51:06Z"
  compiler: gc
  gitCommit: dc6760b58d10b77ce10082dcfbdb4c4c9f3d61df
  gitTreeState: clean
  gitVersion: v1.27.1-1+dc6760b58d10b7
  goVersion: go1.20.2
  major: "1"
  minor: 27+
  platform: illumos/amd64
kustomizeVersion: v5.0.1
serverVersion:
  buildDate: "2023-04-14T18:52:09Z"
  compiler: gc
  gitCommit: dc6760b58d10b77ce10082dcfbdb4c4c9f3d61df
  gitTreeState: clean
  gitVersion: v1.27.1-1+dc6760b58d10b7
  goVersion: go1.20.2
  major: "1"
  minor: 27+
  platform: illumos/amd64

    Worker nodes with Linux (I’ve opted for Ubuntu 22.04 LTS) with Kubernetes v1.27.x and CRI (I’ve chosen CRI-O) set up.
    BGP
    Prometheus Operator CRD in order to utilize Service Monitors (to facilitate scraping)
    Gateway API spec (v0.5.1) CRD

Working control plane — and worker nodes

This (friend links, no paywall) are the concepts for getting the control plane up and running, as well as bringing the worker nodes up.

    Build a managed Kubernetes cluster from scratch — part 1 The adventures in the Home Lab environment proceed (when time allows), and with concepts taken from the the earlier…medium.com

    Build a managed Kubernetes cluster from scratch — part 2 Its been a while since part one of this series where I intend to describe how one can build a managed cluster from…medium.com

    Build a managed Kubernetes cluster from scratch — part 3 As the cluster now has a solid foundation with an external Certificate Authority and the Control Plane is in place, it…medium.com

    Build a managed Kubernetes cluster from scratch — part 4 Implementing a first stage of Service Meshmedium.com

    Build a managed Kubernetes cluster from scratch — part 5 Will this finally be the last part of this series?medium.com

BGP

This configuration is in no way “production ready” as it has no security, but for demonstration purposes it show how BGP can be implemented and enable it in the cluster.

To explain how the configuration ended up in this way, some background is needed:

In order to have the control plane being able to talk with the internal services I’ve had some challenges:

#1: I looked into on how to integrate with the VXLAN, but I saw no easy way to implement it. #2: Next up I had the admission controllers listening on host ports and patched the webhooks to talk url to a external load balancer which pointed to each worker node instead of the ordinaryservice, which mostly by the way worked rather good on various projects. I had Longhorn working stable. Then came Longhorn v1.3+ which relied on even more webhooks and I realised that it wouldn’t be realistic to maintain. #3: VTEP, still VXLAN, but it would at least be a defined state. However good, it would still be a single worker node (if I’ve understood the concept correctly). #4: Current state. I simply announce the ClusterIP to the kube-apiserver and route it back to the workers. What’s not so good is that each individual worker node needs to be defined.

This is the configuration in its current state:

K8S_CLUSTERIP_CIDR=
K8S_ROUTER_ASN=
K8S_LB_CIDR=
WORKER1_NODE_IP=
WORKER2_NODE_IP=
WORKER3_NODE_IP=
WORKER_NODE_CIDR=
LOCAL_ROUTER_ASN=
LOCAL_ROUTER_ID=
LOCAL_ROUTER_NAME=
UPSTREAM_ROUTER_ASN=
UPSTREAM_ROUTER_ID=

cat << EOF > /etc/frr/frr.conf
frr version 7.5
frr defaults traditional
hostname ${LOCAL_ROUTER_NAME}
log syslog
no ipv6 forwarding
service integrated-vtysh-config
!
ip route ${K8S_CLUSTERIP_CIDR} ${WORKER1_NODE_IP}
ip route ${K8S_CLUSTERIP_CIDR} ${WORKER2_NODE_IP}
ip route ${K8S_CLUSTERIP_CIDR} ${WORKER3_NODE_IP}
!
router bgp ${LOCAL_ROUTER_ASN}
 bgp router-id ${LOCAL_ROUTER_ID}
 bgp log-neighbor-changes
 bgp bestpath as-path multipath-relax
 neighbor ${UPSTREAM_ROUTER_ID} remote-as ${UPSTREAM_ROUTER_ASN}
 neighbor K8S peer-group
 neighbor K8S remote-as ${K8S_ROUTER_ASN}
 neighbor K8S capability extended-nexthop
 neighbor K8S update-source ${LOCAL_ROUTER_ID}
 neighbor ${WORKER_NODE_CIDR} peer-group K8S
 bgp listen range ${WORKER_NODE_CIDR} peer-group K8S
!
 address-family ipv4 unicast
  redistribute connected
  neighbor K8S route-map IMPORT in
  neighbor K8S route-map EXPORT out
  network ${K8S_LB_CIDR}
  network ${K8S_CLUSTERIP_CIDR}
  network ${WORKER_NODE_CIDR}
  neighbor ${UPSTREAM_ROUTER_ID} soft-reconfiguration inbound
  neighbor ${UPSTREAM_ROUTER_ID} route-map ALLOW-ALL in
  neighbor ${UPSTREAM_ROUTER_ID} route-map ALLOW-ALL out
  neighbor K8S route-map ALLOW-ALL in
  neighbor K8S route-map ALLOW-ALL out
 exit-address-family
!
route-map ALLOW-ALL permit 100
!
line vty
!
EOF

After the basic definition, we simply create static routes to the worker nodes. Then (as I have several BGP peers in my home infrastructure), I declare BGP peers both upstream (my router) and downstream (the kubernetes cluster). Lastly, among the (no) filters, I define what networks I chose to announce further.

Prometheus Operator CRD

As we will deploy Cilium with resources of kind ServiceMonitors that define what Prometheus Operator should scrape, we should create those CRD beforehand.

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm template kube-prometheus prometheus-community/kube-prometheus-stack --include-crds \
  | yq 'select(.kind == "CustomResourceDefinition") * {"metadata": {"annotations": {"meta.helm.sh/release-name": "kube-prometheus", "meta.helm.sh/release-namespace": "monitoring"}}}' \
  | kubectl create -f -

Gateway API CRD v0.5.1

While it appears not strictly necessary to install before Cilium (or rather, when we deploy a Gateway or HTTPRoute), but it makes no sense to delay what’s necessary.

    Gateway API Support - Cilium 1.13.3 documentation helm upgrade cilium cilium/cilium --version 1.13.3 \ --namespace kube-system \ --reuse-values \ --set…docs.cilium.io

kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v0.5.1/config/crd/standard/gateway.networking.k8s.io_gatewayclasses.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v0.5.1/config/crd/standard/gateway.networking.k8s.io_gateways.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v0.5.1/config/crd/standard/gateway.networking.k8s.io_httproutes.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v0.5.1/config/crd/experimental/gateway.networking.k8s.io_referencegrants.yaml

Installing Cilium v1.13.x

We are deploying Cilium with a multitude of options set. - Enable the GoBGP based BGP Control Plane. - Enable Cilium Ingress Controller. - Enable Cilium Gateway API. - Define the Cluster CIDR. - Install Prometheus Operator ServiceMonitor. - Install Hubble HTTP L7 Dashboard with a defined set of metrics enabled (including policy visualization for Cilium Network Policies). - Enable “strict” mode (kube-proxy free installation) - The observant will see bpf.lbExternalClusterIP. While an anti-pattern, it is a life saver for me that Cilium provides this fantastic possibility as it do enable me to segment the Control Plane to live outside of Data Plane.

CLUSTERCIDR=10.0.0.0/16
K8SAPISERVER=10.12.13.14

helm install cilium cilium/cilium --version 1.13.3 \
--namespace kube-system \
--set bgpControlPlane.enabled=true \
--set ingressController.enabled=true \
--set bpf.lbExternalClusterIP=true \
--set gatewayAPI.enabled=true \
--set bpf.masquerade=true \
--set cluster.id=1 \
--set cluster.name=democluster1 \
--set ipam.mode=kubernetes \
--set ipv4NativeRoutingCIDR=${CLUSTERCIDR} \
--set k8sServiceHost=${K8SAPISERVER} \
--set k8sServicePort=6443 \
--set kubeProxyReplacement=strict \
--set tunnel=vxlan \
--set operator.prometheus.enabled=true \
--set operator.prometheus.serviceMonitor.enabled=true \
--set prometheus.enabled=true \
--set prometheus.serviceMonitor.enabled=true \
--set hubble.relay.enabled=true \
--set hubble.ui.enabled=true \
--set hubble.metrics.dashboards.enabled=true \
--set hubble.metrics.dashboards.namespace=monitoring \
--set hubble.metrics.dashboards.annotations.grafana_folder=Hubble \
--set hubble.metrics.enableOpenMetrics=true \
--set hubble.metrics.enabled="{dns,drop,tcp,flow:sourceContext=workload-name|reserved-identity;destinationContext=workload-name|reserved-identity,port-distribution,icmp,kafka:labelsContext=source_namespace\,source_workload\,destination_namespace\,destination_workload\,traffic_direction;sourceContext=workload-name|reserved-identity;destinationContext=workload-name|reserved-identity,policy:sourceContext=app|workload-name|pod|reserved-identity;destinationContext=app|workload-name|pod|dns|reserved-identity;labelsContext=source_namespace\,destination_namespace,httpV2:exemplars=true;labelsContext=source_ip\,source_namespace\,source_workload\,destination_ip\,destination_namespace\,destination_workload\,traffic_direction}", \
--set hubble.enabled=true \
--set hubble.metrics.serviceMonitor.enabled=true

Install ExternalDNS

ExternalDNS should support Gateway API as of now but I initially had no success on getting the --source=gateway-httproute to work properly, it turned out to be due to missing rbac permissions (the namespaces resource was missing but should be in place in v0.13.5).

The configuration is really out of scope here (as it depends on your DNS hosting), but I show a configuration that has been working for me with RFC2136.

helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/

Then define the values that are appropriate for your environment.

PROVIDER=rfc2136
DNSHOST=10.53.0.2 # Your internal DNS resolver
DNSZONE=your.domain.com # The zone that DNS manages
TSIGSECRET=<TSIG Secret at the DNS server, check named.conf>
TSIGALGO=hmac-sha256 # TSIG algorithm chosen at DNS server
TSIGKEY=externaldns # The TSIG name chosen at DNS server
DOMAINFILTER=your.domain.com # Which sub domains the ExternalDNS handles

$ cat <<EOF | helm upgrade --install -n external-dns external-dns \
external-dns/external-dns --create-namespace  -f -
---
serviceAccount:
  create: true

rbac:
  create: true

securityContext:
  runAsNonRoot: true
  runAsUser: 65534
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop: ["ALL"]

sources:
  - service
  - ingress
  - httprouter

registry: txt
txtOwnerId: "k8s"
txtPrefix: "external-dns-"

domainFilters: 
  - ${DOMAINFILTER}

provider: ${PROVIDER}

deploymentStrategy:
  type: Recreate

extraArgs: 
  - --rfc2136-host=${DNSHOST}
  - --rfc2136-port=53
  - --rfc2136-zone=${DNSZONE}
  - --rfc2136-tsig-secret=${TSIGSECRET}
  - --rfc2136-tsig-secret-alg=${TSIGALGO}
  - --rfc2136-tsig-keyname=${TSIGKEY}
  - --rfc2136-tsig-axfr
EOF

Installation of Cert Manager

Cert Manager is out of scope, as it depends on how your domain is hosted, but to facilitate TLS certificates in the cluster it is handy to deal it with Cert Manager.

Basically the installation is like this (and then a Issuer/ClusterIssuer needs to be set up). The extra arguments are needed for GatewayAPI (as it is still considered “experimental”) and for split DNS setup (it you, like me, have internal and external DNS servers serving the clients).

helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
--namespace cert-manager \
--create-namespace \
--set "extraArgs={\
--feature-gates=ExperimentalGatewayAPISupport=true,\
--dns01-recursive-nameservers-only,\
--dns01-recursive-nameservers=8.8.8.8:53\,1.1.1.1:53}" \
--set installCRDs=true \
--set webhook.hostNetwork=true

    Helm cert-manager provides Helm charts as a first-class method of installation on both Kubernetes and OpenShift. Be sure…cert-manager.io

https://cert-manager.io/docs/installation/helm/#steps

Choose a ACME provider of choice (DNS-01 is to prefer as the cluster won’t need to be externally exposed) https://cert-manager.io/docs/configuration/acme/dns01/#supported-dns01-providers and set up the challenge method.

A ClusterIssuer can look like this:

TSIGNAME= # name of the TSIG key
VALIDTSIGKEY= # a valid TSIG key to the dns server
CLUSTERISSUER=acme-prod # a name to refer your ClusterIssuer
SOADNS= # the IP of the primary DNS
TSIGALGO=HMACSHA512 # Choose something strong here

cat <<EOF |kubectl apply -f 
---
apiVersion: v1
data:
  ${TSIGNAME}: ${VALIDTSIGKEY}
kind: Secret
metadata:
  name: ${TSIGNAME}
  namespace: cert-manager
type: Opaque
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: ${CLUSTERISSUER}
spec:
  acme:
    email: ${VALIDEMAIL}
    preferredChain: ""
    privateKeySecretRef:
      name: certmanager-keyref
    server: https://acme-v02.api.letsencrypt.org/directory
    solvers:
    - dns01:
        rfc2136:
          nameserver: ${SOADNS}
          tsigAlgorithm: ${TSIGALGO}
          tsigKeyName: ${TSIGNAME}
          tsigSecretSecretRef:
            key: ${TSIGNAME}
            name: ${TSIGNAME}
EOF

Deploy Istio Bookinfo sample application

The Bookinfo sample application created by the Istio project is great to test out the abilities of the Gateway API, let’s deploy it:

kubectl apply -f \
https://raw.githubusercontent.com/istio/istio/\   
release-1.13/samples/bookinfo/platform/kube/bookinfo.yaml

Configure Cilium Gateway API

In order to deploy the Gateway, we need to define a IP pool for the Service LoadBalancer and declare how to route/announce the IP through the network.

The concept around the BGP Control Plane, with LB Ipam, creates possibilities for having different routes on different nodes. I’m longing for native multi homing in Cilium but this could be a kind of (limited) alternative to route work load on node level. Btw, I’ve seen some happenings on the multi homing in the project so I feel optimistic that good things will happen with multi homing in the future.

Not really necessary in this specific environment (as the control plane is external and not affected), but needed in my demo environment where control plane and data plane live together, the BGP configuration below will only be applied to nodes with label bgp=worker:

Values from FRR declaration above
K8S_ROUTER_ASN= The AS Number defined for the Kubernetes cluster
LOCAL_ROUTER_ASN= AS Number defined for the router
LOCAL_ROUTER_HOSTCIDR= the /32 CIDR of the router

cat <<EOF |. kubectl apply -f -
---
apiVersion: cilium.io/v2alpha1
kind: CiliumLoadBalancerIPPool
metadata:
  name: ippool
spec:
  cidrs:
  - cidr: 10.245.12.0/24
  disabled: false
---
apiVersion: cilium.io/v2alpha1
kind: CiliumBGPPeeringPolicy
metadata:
  name: rack0
spec:
  nodeSelector:
    matchLabels:
      bgp: worker
  virtualRouters:
  - exportPodCIDR: true
    localASN: ${K8S_ROUTER_ASN}
    neighbors:
    - peerASN: ${LOCAL_ROUTER_ASN}
      peerAddress: ${LOCAL_ROUTER_HOSTCIDR}
    serviceSelector:
      matchExpressions:
      - key: somekey
        operator: NotIn
        values:
        - never-used-value
EOF

Test out TLS terminated HTTPRoutes

To test out the functionality of TLS terminated Gateway HTTPRoute resources this stanza can be used:

HTTPROUTEDOMAIN1=bookinfo.c1demo.ploio.net
HTTPROUTEDOMAIN2=hipstershop.c1dem.ploio.net
CLUSTERISSUER=acme-prod #Defined when choosing a provider for cert-manager


$ cat <<EOF | kubectl apply -f 
---
apiVersion: gateway.networking.k8s.io/v1beta1
kind: Gateway
metadata:
  name: tls-gateway
  annotations:
    cert-manager.io/cluster-issuer: ${CLUSTERISSUER}
spec:
  gatewayClassName: cilium
  listeners:
  - name: https-1
    protocol: HTTPS
    port: 443
    hostname: "${HTTPROUTEDOMAIN1}"
    tls:
      certificateRefs:
      - kind: Secret
        name: demo-cert
  - name: https-2
    protocol: HTTPS
    port: 443
    hostname: "${HTTPROUTEDOMAIN2}"
    tls:
      certificateRefs:
      - kind: Secret
        name: demo-cert
---
apiVersion: gateway.networking.k8s.io/v1beta1
kind: HTTPRoute
metadata:
  name: https-app-route-1
spec:
  parentRefs:
  - name: tls-gateway
  hostnames:
  - "${HTTPROUTEDOMAIN1}"
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /details
    backendRefs:
    - name: details
      port: 9080
---
apiVersion: gateway.networking.k8s.io/v1beta1
kind: HTTPRoute
metadata:
  name: https-app-route-2
spec:
  parentRefs:
  - name: tls-gateway
  hostnames:
  - "${HTTPROUTEDOMAIN2}"
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /
    backendRefs:
    - name: productpage
      port: 9080
EOF

With this deployed there are one gateway and two httproutes in place,

$ kubectl get gateway                                               
NAME          CLASS    ADDRESS        READY   AGE
tls-gateway   cilium   10.245.12.100   True    38m

$ kubectl get httproutes.gateway.networking.k8s.io
NAME                HOSTNAMES                                    AGE
https-app-route-1   ["bookinfo.c1demo.ploio.net"]      37m
https-app-route-2   ["hipstershop.c1demo.ploio.net"]   37m

$ kubectl get certificates.cert-manager.io
NAME        READY   SECRET      AGE
demo-cert   True    demo-cert   29m

The https-app-route-2 (“hipstershop”) will take us straight to / of the “productpage” Service on port 9080:
The https-app-route-2

The https-app-route-2 (“bookinfo”), on the other hand, will only route on the /details (and below) and direct the HTTP traffic to the details Service on port 9080, thus a 404 on request to /:

$ curl -i https://bookinfo.c1demo.ploio.net/
HTTP/1.1 404 Not Found
date: Sat, 03 Jun 2023 14:01:43 GMT
server: envoy
content-length: 0

A request to /details will take us to the right path:

$ curl -i https://bookinfo.c1demo.ploio.net/details
HTTP/1.1 400 Bad Request
content-type: application/json
server: envoy
date: Sat, 03 Jun 2023 14:03:39 GMT
content-length: 45
x-envoy-upstream-service-time: 2

{"error":"please provide numeric product id"}%

Result when a valid path is fetched:
Output from details Service, through a TLS terminated HTTPRoute

Deploy HTTP Gateway with HTTPRoute

In the same fashion, we will deploy a demo application to explore the the functionality of modifying headers and load balancing. We’ll do this with a simple HTTP Gateway HTTPRoute resource (no TLS termination and no hostname directive, accepting traffic directly to IP):

---
apiVersion: gateway.networking.k8s.io/v1beta1
kind: Gateway
metadata:
  annotations:
  name: my-gateway
  namespace: default
spec:
  gatewayClassName: cilium
  listeners:
  - allowedRoutes:
      namespaces:
        from: Same
    name: web-gw
    port: 80
    protocol: HTTP
---
apiVersion: gateway.networking.k8s.io/v1beta1
kind: HTTPRoute
metadata:
  name: http-app-1
  namespace: default
spec:
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: my-gateway
    namespace: default
  rules:
  - filters:
    - requestHeaderModifier:
        add:
        - name: some-header
          value: This is fantastic
        set:
        - name: user-agent
          value: Mozilla/5.0 (compatible; Konqueror/3.5; SunOS) KHTML/3.5.0 (like
            Gecko)
      type: RequestHeaderModifier
    matches:
    - path:
        type: PathPrefix
        value: /
  - backendRefs:
    - group: ""
      kind: Service
      name: echo-1
      port: 8080
      weight: 100
    - group: ""
      kind: Service
      name: echo-2
      port: 8080
      weight: 0
    matches:
    - path:
        type: PathPrefix
        value: /
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: echo-1
  name: echo-1
  namespace: default
spec:
  ports:
  - name: high
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: echo-1
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: echo-2
  name: echo-2
  namespace: default
spec:
  ports:
  - name: high
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: echo-2
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
  labels:
    app: echo-1
  name: echo-1
  namespace: default
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: echo-1
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: echo-1
    spec:
      containers:
      - env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        image: gcr.io/kubernetes-e2e-test-images/echoserver:2.2
        imagePullPolicy: IfNotPresent
        name: echo-1
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
  labels:
    app: echo-2
  name: echo-2
  namespace: default
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: echo-2
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: echo-2
    spec:
      containers:
      - env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        image: gcr.io/kubernetes-e2e-test-images/echoserver:2.2
        imagePullPolicy: IfNotPresent
        name: echo-2
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30

The result is a Gateway resource and HTTPRoute (without a hostname):

$ kubectl get httproutes.gateway.networking.k8s.io http-app-1 
NAME         HOSTNAMES   AGE
http-app-1               41h

$ kubectl get gateway my-gateway 
NAME         CLASS    ADDRESS        READY   AGE
my-gateway   cilium   10.245.12.48   True    41h

If we inspect the spec of HTTPRoute deployed, we can see that weight is 100 on echo-1 and 0 on echo-2, which means that all traffic is to be sent to echo-1. At the same time, we will change the request headers:

kubectl get httproutes.gateway.networking.k8s.io http-app-1 -o yaml | yq .spec
parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: my-gateway
    namespace: default
rules:
  - filters:
      - requestHeaderModifier:
          add:
            - name: some-header
              value: This is fantastic
          set:
            - name: user-agent
              value: Mozilla/5.0 (compatible; Konqueror/3.5; SunOS) KHTML/3.5.0 (like Gecko)
        type: RequestHeaderModifier
    matches:
      - path:
          type: PathPrefix
          value: /
  - backendRefs:
      - group: ""
        kind: Service
        name: echo-1
        port: 8080
        weight: 100
      - group: ""
        kind: Service
        name: echo-2
        port: 8080
        weight: 0
    matches:
      - path:
          type: PathPrefix
          value: /

This is the corresponding output, where the request headers are changed, and the response comes from echo-1:

$ curl 10.245.12.48                                                            


Hostname: echo-1-78b66687b5-wzhbb

Pod Information:
 node name: worker2
 pod name: echo-1-78b66687b5-wzhbb
 pod namespace: default
 pod IP: 10.0.1.230

Server values:
 server_version=nginx: 1.12.2 - lua: 10010

Request Information:
 client_address=10.0.0.108
 method=GET
 real path=/
 query=
 request_version=1.1
 request_scheme=http
 request_uri=http://10.245.12.48:8080/

Request Headers:
 accept=*/*  
 host=10.245.12.48  
 some-header=This is fantastic  
 user-agent=Mozilla/5.0 (compatible; Konqueror/3.5; SunOS) KHTML/3.5.0 (like Gecko)  
 x-forwarded-proto=http  
 x-request-id=c66720e5-d45b-40c5-943a-6377ffb4454c  

Request Body:
 -no body in request-

If we scale up the amount requests, the pattern should be visible — everything is sent to echo-1:

$ :> gwapi.out && for i in {1..100}; do curl 10.248.8.169 >> gwapi.out &>/dev/null; done

$ grep -c Hostname gwapi.out                                      
100

$ grep -c "Hostname: echo-1" gwapi.out                            
100
 
$ grep -c "Hostname: echo-2" gwapi.out                                                  
0

If we change the weights to 50 on each, the load should be spread out rather equally:

$ kubectl patch --type merge httproutes.gateway.networking.k8s.io http-app-1 -p '
{
  "spec": {
    "rules": [
      {
        "backendRefs": [
          {
            "name": "echo-1",
            "port": 8080,
            "weight": 50
          },
          {
            "name": "echo-2",
            "port": 8080,
            "weight": 50
          }
        ]
      }
    ]
  }
}'
httproute.gateway.networking.k8s.io/http-app-1 patched

$ :> gwapi.out && for i in {1..100}; do curl 10.248.8.169 >> gwapi.out &>/dev/null; done

$ grep -c Hostname gwapi.out                                                  
100

$ grep -c "Hostname: echo-1" gwapi.out
52

$ grep -c "Hostname: echo-2" gwapi.out
48

And finally, patching the echo-2 to have all the load is reflected when we curl the resource:

$ kubectl patch --type merge httproutes.gateway.networking.k8s.io http-app-1 -p '
{
  "spec": {
    "rules": [
      {
        "backendRefs": [
          {
            "name": "echo-1",
            "port": 8080,
            "weight": 0
          },
          {
            "name": "echo-2",
            "port": 8080,
            "weight": 100
          }
        ]
      }
    ]
  }
}'
httproute.gateway.networking.k8s.io/http-app-1 patched

$ :> gwapi.out && for i in {1..100}; do curl 10.248.8.169 >> gwapi.out &>/dev/null; done

$ grep -c "Hostname: echo-1" gwapi.out
0

$ grep -c "Hostname: echo-2" gwapi.out
100

Install OpenTelemetry Operator and OTel Collector

For this, we will install the operator and collector with values from the Isovalent Cilium Grafana Observability Demo repo.

First we install the OpenTelemetry Operator:

$ cat <<EOF | helm upgrade opentelemetry-operator\
 open-telemetry/opentelemetry-operator\
 --install --namespace opentelemetry-operator\
 --create-namespace --version 0.15.0 -f -
---
admissionWebhooks:
  create: false

manager:
  serviceMonitor:
    enabled: true
  env:
    ENABLE_WEBHOOKS: "false"
EOF

Install the OpenTelemetry Collector

cat <<EOF | kubectl apply -n opentelemetry-operator -f -
---
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel
spec:
  mode: daemonset
  hostNetwork: true
  #image: otel/opentelemetry-collector-contrib:0.60.0
  image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.60.0
  config: |
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
          thrift_compact:
            endpoint: 0.0.0.0:6831
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    processors:
      batch: {}
      memory_limiter:
        check_interval: 5s
        limit_mib: 409
        spike_limit_mib: 128

    exporters:
      logging:
        loglevel: info
      otlp:
        endpoint: tempo.tempo.svc.cluster.local:4317
        tls:
          insecure: true

    service:
      telemetry:
        logs:
          level: info
          encoding: console
      pipelines:
        traces:
          receivers:
            - otlp
            - jaeger
          processors:
            - memory_limiter
            - batch
          exporters:
            - logging
            - otlp
EOF

With this in place we chould be able to collect traces.
Deploy Grafana Tempo

We will use Grafana Tempo as our front end for OpenTelemetry for having a look at the Hubble HTTP L7 traces.

$ cat << EOF > tempo-values.yaml
---
fullnameOverride: tempo

tempo:
  searchEnabled: true
EOF

$ helm upgrade tempo grafana/tempo --install \
--namespace tempo --create-namespace \
--create-namespace --version 0.16.2 -f tempo-values.yaml

Deploy Kube Prometheus Stack

Prometheus Operator will be installed with the kube-prometheus-stack helm template with some configuration for exemplars handling, dashboards and datasources:

GRAFANAFQDN=grafana.c1demo.ploio.net
GRAFANAPW=password # set something sensible here

cat <<EOF > prometheus-values.yaml
---
# nameOverride: prometheus-k8s
fullnameOverride: prometheus-k8s

prometheus:
  prometheusSpec:
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false

    enableRemoteWriteReceiver: true
    enableFeatures:
      - exemplar-storage
    externalLabels:
      cluster: kind
  ingress:
    enabled: false
    ingressClassName: cilium

defaultRules:
  rules:
    kubeProxy: false

alertmanager:
  ingress:
    enabled: false
    ingressClassName: cilium


kubeApiServer:
  tlsConfig:
    serverName: kubernetes
    insecureSkipVerify: true
grafana:
  enabled: true
  image:
    tag: 9.2.0

  serviceMonitor:
    enabled: true
  grafana.ini:
    server:
      domain: ${GRAFANAFQDN}
      root_url: "%(protocol)s://%(domain)s"
    feature_toggles:
      enable: 'tempoApmTable tempoBackendSearch'
  ingress:
    enabled: true
    ingressClassName: cilium
    hosts:
      - ${GRAFANAFQDN}
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'cilium'
        orgId: 1
        folder: 'cilium'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/cilium

  dashboards:
    cilium:
      hubble:
        gnetId: 16613
        revision: 1
        datasource: Prometheus
      cilium-agent:
        gnetId: 16611
        revision: 1
        datasource: Prometheus
      cilium-operator:
        gnetId: 16612
        revision: 1
        datasource: Prometheus
      cilium-policies:
        gnetId: 18015
        revision: 4
        datasource:
        - name: DS_PROMETHEUS
          value: prometheus

  persistence:
    enabled: false

  adminUser: admin
  adminPassword: ${GRAFANAPW}


grafana:
  sidecar:
    skipTlsVerify: true
    dashboards:
      folderAnnotation: grafana_folder
      provider:
        foldersFromFilesStructure: true

    datasources:
      exemplarTraceIdDestinations:
        datasourceUid: tempo
        traceIdLabelName: traceID

  additionalDataSources:
    - name: Tempo
      type: tempo
      uid: tempo
      url: http://tempo.tempo:3100
      access: proxy
      jsonData:
        httpMethod: GET
        tracesToMetrics:
          datasourceUid: 'prometheus'
          tags: [{ key: 'service.name', value: 'service' }, { key: 'job' }]
          queries:
            - name: 'Sample query'
              query: 'sum(rate(tempo_spanmetrics_latency_bucket{$__tags}[5m]))'
        serviceMap:
          datasourceUid: 'prometheus'
        search:
          hide: false
        nodeGraph:
          enabled: true
EOF

$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
$ helm repo update
$ helm upgrade kube-prometheus prometheus-community/kube-prometheus-stack --install --namespace monitoring --create-namespace --version 46.5.0 --values prometheus-values.yaml

Cilium Policy Verdicts Dashboard

Also, to visualise Cilium Network Policies, we installed the Policy Verdicts dashboard. This dashboard can really help us to tune the network policies to a zero trust level.

In my demo I deployed it manually as I had trouble to get it working from Grafana dashboard marketplace (for some reason it was unavailable at the moment), but normally as above in the helm values it would be installed at the same time as the others.
Deploy of OpenEBS for Persistent Storage

Some of the demo components really insisted for a PVC and I checked out OpenEBS and it turned out to be rather sleek (lets see in time if its a good as well):

$ kubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml

$ kubectl patch storageclass openebs-hostpath -p '
{"metadata": 
{"annotations":
{"storageclass.kubernetes.io/is-default-class":"true"}
}
}
'

The star of this show — “tenants app”

Either git clone the original demo app directly from Isovalent GH to play with some of the values, or clone my fork for some immediate action:

$ helm repo add minio https://operator.min.io
$ helm repo add strimzi https://strimzi.io/charts
$ helm repo add elastic https://helm.elastic.co
$ git clone https://github.com/tnorlin/cilium-grafana-observability-demo.git
$ cilium-grafana-observability-demo
$ helm dep build ./helm/jobs-app
$ helm upgrade jobs-app ./helm/jobs-app \
  --install \
  --wait \
  --create-namespace \
  --namespace tenant-jobs \
  -f helm/jobs-app-values.yaml

The components should have stabilised after a couple of minutes and in Hubble a view similar to this should be visible:
The sample “tenants-app” deployed, as shown in Hubble UI.

To show Hubble, we can deploy a Cilium Ingress:

CLUSTERISSUER=acme-prod #Defined when choosing a provider for cert-manager
HUBBLEFQDN= 

cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: $(CLUSTERISSUER}
    external-dns.alpha.kubernetes.io/hostname: ${HUBBLEFQDN}
  name: hubble-ingress
  namespace: kube-system
spec:
  ingressClassName: cilium
  rules:
  - host: ${HUBBLEFQDN}
    http:
      paths:
      - backend:
          service:
            name: hubble-ui
            port:
              number: 80
        path: /
        pathType: Prefix
  tls:
  - hosts:
    - ${HUBBLEFQDN}
    secretName: hubbleui-tls-cert
EOF

Grafana

In Grafana we should have a bunch of dashboards, but one dashboard is particular of interest, the Hubble L7 HTTP dashboard, which show HTTP metrics along with examplars (the green “squares” in the bottom graph) from Grafana Tempo:
Grafana Hubble L7 HTTP Metrics dashboard.

Hoovering an examplar would show something like this:
Grafana dashboard with Promethus (Hubble) and Tempo as a source.

Example of a trace that got wrong:
Grafana Tempo with a broken request.

Hubble Policy Verdicts

The Cilium Policy Verdicts dashboard, showing output from Hubble policy verdicts. This dashboard can be very helpful in the process of getting the network policies in a good shape by catching the traffic that doesn’t hit a defined rule:
Cilium Policy Verdicts dashbord, green.

The policy verdicts can also be showed with the Hubble cli tool:

hubble observe --type policy-verdict -n tenant-jobs --last 5              
Jun  3 17:34:08.849: tenant-jobs/strimzi-cluster-operator-6d4865c4d6-pnxwt:54352 (ID:109299) -> tenant-jobs/jobs-app-kafka-0:9091 (ID:126417) policy-verdict:L3-L4 INGRESS ALLOWED (TCP Flags: SYN)
Jun  3 17:34:08.910: tenant-jobs/strimzi-cluster-operator-6d4865c4d6-pnxwt:54354 (ID:109299) -> tenant-jobs/jobs-app-kafka-0:9091 (ID:126417) policy-verdict:L3-L4 INGRESS ALLOWED (TCP Flags: SYN)
Jun  3 17:34:09.067: tenant-jobs/strimzi-cluster-operator-6d4865c4d6-pnxwt:54356 (ID:109299) -> tenant-jobs/jobs-app-kafka-0:9091 (ID:126417) policy-verdict:L3-L4 INGRESS ALLOWED (TCP Flags: SYN)
Jun  3 17:34:09.100: tenant-jobs/strimzi-cluster-operator-6d4865c4d6-pnxwt:54358 (ID:109299) -> tenant-jobs/jobs-app-kafka-0:9091 (ID:126417) policy-verdict:L3-L4 INGRESS ALLOWED (TCP Flags: SYN)
Jun  3 17:35:10.079: tenant-jobs/jobs-app-entity-operator-6c69b669b6-gz7l8:56702 (ID:69972) -> tenant-jobs/jobs-app-kafka-0:9091 (ID:126417) policy-verdict:L3-L4 INGRESS ALLOWED (TCP Flags: SYN)
Jun  3 17:35:44.034: tenant-jobs/strimzi-cluster-operator-6d4865c4d6-pnxwt:59206 (ID:109299) -> 10.20.14.20:6443 (ID:16777217) policy-verdict:all EGRESS ALLOWED (TCP Flags: SYN)
Jun  3 17:35:49.889: 10.0.1.248:59578 (host) -> tenant-jobs/jobs-app-entity-operator-6c69b669b6-gz7l8:8080 (ID:69972) policy-verdict:L4-Only INGRESS ALLOWED (TCP Flags: SYN)
Jun  3 17:35:49.889: 10.0.1.248:55716 (host) -> tenant-jobs/jobs-app-entity-operator-6c69b669b6-gz7l8:8081 (ID:69972) policy-verdict:L4-Only INGRESS ALLOWED (TCP Flags: SYN)
Jun  3 17:35:49.889: 10.0.248:55714 (host) -> tenant-jobs/jobs-app-entity-operator-6c69b669b6-gz7l8:8081 (ID:69972) policy-verdict:L4-Only INGRESS ALLOWED (TCP Flags: SYN)
Jun  3 17:35:49.889: 10.0.1.248:59572 (host) -> tenant-jobs/jobs-app-entity-operator-6c69b669b6-gz7l8:8080 (ID:69972) policy-verdict:L4-Only INGRESS ALLOWED (TCP Flags: SYN)

And that was about what I had time to demo during my show time (we have limited amount of time ~35 minutes for presentation, demo and Q&A).
Cilium Mesh (almost there)

Wait, there’s more to it, I was out of time during the demo, but my preparations went a bit further. I had installed another (ordinary) kubernetes Cluster with Cluster Mesh enabled and also connected an external workload (a VM) to the cluster.

Output from the vm running docker:

root@c1demovm1:~# cilium status
KVStore:                 Ok         etcd: 1/1 connected, lease-ID=7c02888260c64b1d, lock lease-ID=7c02888260c64b1f, has-quorum=true: https://clustermesh-apiserver.cilium.io:2379 - 3.5.4 (Leader)
Kubernetes:              Disabled   
Host firewall:           Disabled
CNI Chaining:            none
CNI Config file:         CNI configuration file management disabled
Cilium:                  Ok   1.13.3 (v1.13.3-36cb0eed)
NodeMonitor:             Listening for events on 4 CPUs with 64x4096 of shared memory
Cilium health daemon:    Ok   
IPAM:                    IPv4: 1/2 allocated from 10.190.1.0/30, IPv6: 1/4294967294 allocated from f00d::a14:0:0:0/96
IPv6 BIG TCP:            Disabled
BandwidthManager:        Disabled
Host Routing:            Legacy
Masquerading:            IPTables [IPv4: Enabled, IPv6: Enabled]
Controller Status:       17/17 healthy
Proxy Status:            OK, ip 10.190.1.2, 0 redirects active on ports 10000-20000
Global Identity Range:   min 256, max 65535
Hubble:                  Disabled
Encryption:              Disabled
Cluster health:             Probe disabled

In one of the clusters, the vm is visible

$ kubectl get ciliumnode                                      
NAME           CILIUMINTERNALIP   INTERNALIP     AGE
c1demovm1      10.190.1.2         10.20.21.20    103s
worker1        10.0.0.114         172.22.5.31    2d4h
worker3        10.0.1.248         172.22.5.33    2d4h

The vm can do DNS lookups in the cluster

root@c1demovm1:~# nslookup -norecurse hubble-ui.kube-system.svc.cluster.local
Server:  10.192.0.10
Address: 10.192.0.10#53

Name: hubble-ui.kube-system.svc.cluster.local
Address: 10.195.247.135

Also, the vm can connect to resources in the cluster

curl echo-1.default.svc.cluster.local:8080


Hostname: echo-1-78b66687b5-wzhbb

Pod Information:
 node name: worker3
 pod name: echo-1-78b66687b5-wzhbb
 pod namespace: default
 pod IP: 10.0.1.230

Server values:
 server_version=nginx: 1.12.2 - lua: 10010

Request Information:
 client_address=10.190.1.2
 method=GET
 real path=/
 query=
 request_version=1.1
 request_scheme=http
 request_uri=http://echo-1.default.svc.cluster.local:8080/

Request Headers:
 accept=*/*  
 host=echo-1.default.svc.cluster.local:8080  
 user-agent=curl/7.81.0  

Request Body:
 -no body in request-

During KubeConEU, Liz Rice showed us an amazing teaser about the future Cilium Mesh and this little piece (screen shot) captured my interest.

This functionality is not in the mainline Cilium (not at least from what I’ve seen), but hopefully something that will land in the next release of Cilium OSS (v1.14) — cilium endpoint add --name= --labels= --ip=. Without the possibility to declare new endpoints, I’ve yet to find good ways to integrate the vm into the cluster (but the other way around seem to work well).

Well, that’s about it for this time, in the next part I planned to dive a little deeper into some of the parts as there are more to it, especially when it comes to Policy Verdicts and Cluster Mesh.

What did you think about this article? Did I get anything wrong or bad? Spelling mistakes? Will you try out some of the functionality yourself?

Please react|comment|share if you liked the article or else found it useful. I hope it will inspire you to test out at least some of the features.

I’ve had hopes to create an inspiring bare metal cluster with the Turing PI v2 and a couple of Raspberry Pi CM4, hopefully with some kind of a node auto scaler — but you’ll have to bare with my modest demo environment as we’ll have to wait until Q4 for the CM4 to restock…

* ChatGPT

** How to tell kubernetes deployment not to run pod on specific node?

There are a few ways to tell Kubernetes not to run a pod on a specific node:

    Node Selector: You can use a node selector to specify which nodes the pod can run on. You can label the node you want to exclude, and then add a node selector to the pod specification that excludes that label. For example, to exclude a node with a label node-type=exclude, you can add the following to the pod spec:

python

nodeSelector:
  node-type: "not-exclude"

    Node Affinity: Node affinity allows you to specify rules for the scheduling of pods based on the labels of the nodes. You can use node affinity to exclude a node by setting a negative match expression. For example, to exclude a node with a label node-type=exclude, you can add the following to the pod spec:

yaml

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: node-type
              operator: NotIn
              values:
                - exclude

    Taints and tolerations: Taints are used to mark a node as unsuitable for certain pods. Tolerations are used to allow pods to be scheduled on nodes with specific taints. You can use taints and tolerations to exclude a node by tainting the node and adding a corresponding toleration to the pod. For example, to exclude a node with a taint of exclude=true:NoSchedule, you can add the following to the node:

java

kubectl taint nodes <node-name> exclude=true:NoSchedule

And then add the following to the pod spec:

vbnet

tolerations:
  - key: exclude
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"

These are just a few examples of how you can exclude a node from running a pod in Kubernetes. The approach you choose will depend on your specific use case and requirements.

** How much does Helm give overhead in Kubernetes?

Helm is a package manager for Kubernetes that helps you manage and deploy applications to your Kubernetes cluster. While Helm does add some overhead to your Kubernetes deployment, the benefits it provides can far outweigh the costs.

The amount of overhead that Helm adds to your Kubernetes deployment will depend on a variety of factors, including the complexity of your application, the size of your cluster, and the resources available to you.

Here are some potential areas where Helm might add overhead to your Kubernetes deployment:

    Resource usage: Helm uses additional resources in your cluster to manage the deployment of your applications. This includes additional Kubernetes objects such as releases, charts, and config maps. Depending on the size and complexity of your deployment, this could impact the performance of your cluster.

    Learning curve: If you're new to Helm, there is a learning curve to understanding how to use it effectively. This can require additional time and resources upfront to get up to speed.

    Maintenance: Helm requires ongoing maintenance to ensure that your deployments are up to date and secure. This includes updating Helm charts and releases, as well as monitoring and troubleshooting any issues that arise.

Despite these potential overheads, there are many benefits to using Helm in your Kubernetes deployment. Here are some potential benefits:

    Reusability: Helm charts can be easily shared and reused across multiple teams and projects, reducing the amount of duplication and increasing efficiency.

    Standardization: Helm provides a standard way of packaging and deploying applications to Kubernetes, making it easier to manage and maintain your deployments.

    Versioning: Helm allows you to version your deployments, making it easier to roll back to a previous version if needed.

    Security: Helm includes security features such as RBAC and TLS encryption to help secure your deployments.

Ultimately, whether or not Helm is worth the overhead for your Kubernetes deployment will depend on your specific use case and requirements. However, for many organizations, the benefits of using Helm outweigh the costs.

** How much objects Kubernetes cluster can handle?

The number of objects that a Kubernetes cluster can handle depends on several factors, including the size and configuration of the cluster, the types of objects being managed, and the resources available to the cluster.

In general, Kubernetes can handle a large number of objects. The Kubernetes API server, which is responsible for managing Kubernetes objects, is designed to scale horizontally, meaning that additional API servers can be added to handle additional load.

However, as the number of objects in a Kubernetes cluster grows, the performance of the cluster can be affected. This is because the Kubernetes API server must maintain an index of all the objects in the cluster, and this index can become slower to search and update as the number of objects grows.

To mitigate this performance impact, there are several best practices that can be followed:

    Limit the number of objects: To reduce the load on the Kubernetes API server, it is recommended to limit the number of objects in the cluster. This can be achieved by deleting unnecessary objects, consolidating objects where possible, and using efficient object designs.

    Optimize object design: When designing objects, it is important to consider the performance impact of each object. For example, using labels and selectors effectively can reduce the number of objects that need to be indexed.

    Scale the API server: If the number of objects in the cluster is expected to grow significantly, it may be necessary to add additional API server instances to handle the load.

    Use caching and indexing: To improve the performance of the Kubernetes API server, caching and indexing can be used. For example, a caching layer like memcached can be used to cache frequently accessed objects, and indexing can be used to speed up searches.

Ultimately, the maximum number of objects that a Kubernetes cluster can handle will depend on the specific configuration and resources available to the cluster, as well as the design and usage of the objects in the cluster.

** How to tell Kubernetes do not run pods on node unless specified.

To prevent Kubernetes from scheduling pods on specific nodes by default, you can add a taint to those nodes and then add a corresponding toleration to your pod specifications.

Here are the general steps to follow:

    Add a taint to the nodes that you want to restrict. A taint is a label that tells Kubernetes not to schedule pods on a node unless the pod specifies a matching toleration. For example, you can add a taint with the following command:

    php

kubectl taint nodes <node-name> key=value:taint-effect

Replace <node-name> with the name of the node that you want to restrict, key and value with a key-value pair to identify the taint, and taint-effect with NoSchedule, NoExecute, or PreferNoSchedule depending on the effect you want the taint to have. For example:

kubectl taint nodes node1 special=restricted:NoSchedule

This command adds a taint to the node1 node with the special=restricted key-value pair and the NoSchedule effect.

Add a toleration to your pod specifications. A toleration is a specification that tells Kubernetes to allow a pod to be scheduled on a node with a matching taint. To add a toleration to your pod specifications, add the following tolerations section to your pod specification:

vbnet

tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"

Replace key and value with the key-value pair that matches the taint you added to the node, and effect with the effect that you specified when adding the taint (NoSchedule, NoExecute, or PreferNoSchedule). For example:

vbnet

    tolerations:
    - key: "special"
      operator: "Equal"
      value: "restricted"
      effect: "NoSchedule"

    This tolerations section specifies that the pod can be scheduled on a node with a taint that has the special=restricted key-value pair and the NoSchedule effect.

By adding a taint and a corresponding toleration, you can prevent Kubernetes from scheduling pods on nodes unless the pods explicitly specify that they can tolerate the taint. This can be useful for reserving nodes for specific workloads or for testing purposes.

** How to configure Kubernetes deployment not to run same pod on the same host?

To configure a Kubernetes deployment to avoid running the same pod on the same host, you can use a pod anti-affinity rule. A pod anti-affinity rule specifies that pods should not be scheduled on the same node if they meet certain criteria, such as having the same label or belonging to the same replica set.

Here are the general steps to follow:

    Add a label to the pods that you want to avoid scheduling on the same node. For example, you can add a label with the following command:

    css

kubectl label pods <pod-name> key=value

Replace <pod-name> with the name of the pod that you want to label, and key and value with a key-value pair to identify the label. For example:

perl

kubectl label pods my-app-1234-5678 app=my-app

This command adds a label with the app=my-app key-value pair to the my-app-1234-5678 pod.

Add a pod anti-affinity rule to your deployment specifications. A pod anti-affinity rule specifies that pods should not be scheduled on the same node if they have a certain label. To add a pod anti-affinity rule to your deployment specifications, add the following podAntiAffinity section to your deployment specification:

yaml

    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - my-app
        topologyKey: "kubernetes.io/hostname"

    This podAntiAffinity section specifies that pods with the app=my-app label should not be scheduled on the same node. The topologyKey field specifies the level at which the rule applies; in this case, it applies at the node level (kubernetes.io/hostname).

By adding a pod anti-affinity rule, you can ensure that Kubernetes does not schedule multiple pods with the same label on the same node. This can be useful for ensuring high availability and avoiding resource contention.

** But it will block creating new pods with Pending status. How to avoid this?

If you want to avoid blocking the creation of new pods with a Pending status while also ensuring that no two pods with the same label are scheduled on the same node, you can use a combination of pod anti-affinity and pod disruption budgets.

Here are the general steps to follow:

    Add a label to the pods that you want to avoid scheduling on the same node, as described in the previous answer.

    Add a pod anti-affinity rule to your deployment specifications, as described in the previous answer.

    Add a pod disruption budget to your deployment specifications. A pod disruption budget specifies the maximum number of pods that can be unavailable at any given time. To add a pod disruption budget to your deployment specifications, add the following podDisruptionBudget section to your deployment specification:

    yaml

    podDisruptionBudget:
      selector:
        matchLabels:
          app: my-app
      maxUnavailable: 1

    This podDisruptionBudget section specifies that no more than one pod with the app=my-app label can be unavailable at any given time.

With this combination of pod anti-affinity and pod disruption budgets, Kubernetes will avoid scheduling multiple pods with the same label on the same node while also allowing new pods to be created even if other pods are in a Pending state. If a node becomes unavailable, Kubernetes will ensure that at least one pod with the app=my-app label is still available by either moving the pod to a different node or creating a new pod to replace it.

** Can Kustomize change object kind and how if it can?

No, Kustomize cannot change the kind of a Kubernetes object. The kind field is a fundamental part of the object's identity and is defined by the Kubernetes API, so it cannot be modified by Kustomize.

Kustomize is designed to modify the contents of existing Kubernetes object manifests, rather than changing their fundamental structure or identity. The tool provides a number of mechanisms for making targeted modifications to the fields, labels, annotations, and other attributes of objects in a declarative, repeatable way.

However, it is possible to achieve similar functionality to changing the kind of an object by using the transformers feature of Kustomize. Transformers can be used to modify the YAML structure of a Kubernetes object manifest, which could include modifying the kind field.

Here is an example of using a transformer to change the kind field of a Deployment object to a StatefulSet:

yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-container
          image: my-image
          ports:
            - containerPort: 80
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- deployment.yaml
transformers:
- deployment-to-statefulset.yaml

In this example, the deployment.yaml file defines a Deployment object with the name my-deployment. The kustomization.yaml file includes a transformers section that specifies a transformer file deployment-to-statefulset.yaml.

The deployment-to-statefulset.yaml file includes the following transformer:

yaml

apiVersion: builtin
kind: Filter
metadata:
  name: deployment-to-statefulset
spec:
  filters:
  - path: kind
    kind: EditTransformer
    version: v1
    options:
      value: StatefulSet

This transformer modifies the kind field of any object that matches its filter to StatefulSet. When you apply the Kustomization using the kubectl apply command, Kustomize will modify the kind field of the Deployment object to StatefulSet. Note that this is a non-standard and potentially risky operation, so it should be used with caution and only when absolutely necessary.

** I tried it, but got output `error: unable to load builtin Filter.builtin.[noGrp]`.

The error message unable to load builtin Filter.builtin.[noGrp] typically indicates that the version of Kustomize you are using is not compatible with the syntax of the Filter transformer in your deployment-to-statefulset.yaml file.

The Filter transformer was introduced in Kustomize version 3.2.0, so if you are using an earlier version of Kustomize, you may encounter this error. To fix the error, you can either upgrade your version of Kustomize to at least version 3.2.0, or you can modify the syntax of the transformer to be compatible with the version of Kustomize you are using.

If you are already using Kustomize 3.2.0 or later, it's possible that the Filter transformer is not installed or enabled in your installation of Kustomize. In this case, you can try reinstalling or updating Kustomize to ensure that the Filter transformer is available.

Alternatively, you can try using a different transformer that is compatible with the version of Kustomize you are using, such as the patch transformer or the kustomize transformer. These transformers provide similar functionality to the Filter transformer and should work with any version of Kustomize.
wigust@yahoo.com
Could you describe how to use `patch` suggested in the previous message?

Sure! The patch transformer in Kustomize can be used to modify specific fields of Kubernetes objects by applying JSON or YAML patches to their manifest files. Here's an example of how to use the patch transformer to change the kind field of a Deployment object to a StatefulSet:

    Create a Deployment manifest file, for example deployment.yaml, with the following contents:

    yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-container
          image: my-image
          ports:
            - containerPort: 80

Create a patch file, for example deployment-patch.yaml, that specifies the changes you want to make to the kind field:

yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  template:
    metadata:
      annotations:
        kustomize.k8s.io/patch: |
          - op: replace
            path: /kind
            value: StatefulSet

This patch file modifies the kind field of the my-deployment object to StatefulSet. Note that the kustomize.k8s.io/patch annotation is used to apply the patch to the object.

Create a kustomization.yaml file that specifies the resources and transformers to apply:

makefile

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- deployment.yaml
transformers:
- deployment-patch.yaml

This kustomization.yaml file specifies the deployment.yaml file as the input resource, and the deployment-patch.yaml file as the transformer to apply.

Run kubectl apply to apply the changes:

php

    kubectl apply -k <directory containing kustomization.yaml>

    This command applies the changes to the Kubernetes cluster. The deployment.yaml file is modified by the deployment-patch.yaml transformer, and the resulting manifest is deployed to the cluster.

That's it! The patch transformer can be used to modify any field of a Kubernetes object, so it's a powerful tool for customizing Kubernetes deployments. Note that the syntax of the patch file is based on JSON patches, so you'll need to be familiar with this format to use the patch transformer effectively.

** I apologize for the confusion. You are correct, the previous solution was not correct for converting a Deployment to a DaemonSet. Here is an updated solution:

    Create a kustomization.yaml file:

typescript

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
- deployment.yaml

patches:
- path: deployment.yaml
  kind: Deployment
  version: apps/v1
  patch: |-
    - op: remove
      path: /spec/replicas
    - op: add
      path: /spec/template/spec/nodeSelector
      value:
        node-type: worker
    - op: add
      path: /spec/template/spec/tolerations/-
      value:
        key: node-type
        operator: Equal
        value: worker
        effect: NoSchedule
    - op: add
      path: /spec/template/spec/hostNetwork
      value: true
    - op: add
      path: /spec/template/spec/dnsPolicy
      value: ClusterFirstWithHostNet
    - op: replace
      path: /spec/strategy/type
      value: Recreate
    - op: replace
      path: /kind
      value: DaemonSet

This Kustomization defines a Deployment resource and a patch that replaces the Deployment with a DaemonSet. The DaemonSet is scheduled to run one pod per node that has the label node-type=worker. The patch also removes the replicas field, as it is not needed for DaemonSet.

    Create a deployment.yaml file:

yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-container
        image: nginx
        ports:
        - containerPort: 80

    Run kubectl apply -k . to apply the Kustomization to the cluster.

This should create a DaemonSet with one pod per node labeled with node-type=worker. Each pod runs a single container with the nginx image.

** What is init container?

An init container is a special type of container that runs before the main container of a pod starts. Its primary purpose is to perform any initialization or pre-processing tasks that are required before the main container starts running. The init container can run a command or a script, and can also perform various other tasks such as installing software, downloading data, or configuring the environment.

Init containers can be used to handle a wide range of scenarios, such as:

    Waiting for a database or other service to become available before starting the main container
    Setting up a shared volume or configuration file that is required by multiple containers in the same pod
    Running security checks or configuration validation before starting the main container
    Initializing a database or other data store before it is accessed by the main container

Init containers are defined in the same YAML file as the main container, but in a separate container specification. They are run sequentially, with the next init container starting only after the previous one has completed successfully. If an init container fails, the pod is restarted until the init container succeeds.

** How to set /etc/hostname inside a pod?

You can set the hostname inside a pod by using an init container to modify the /etc/hostname file. Here's an example of how to do it:

    Create a ConfigMap that contains the hostname value you want to set:

yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: hostname-configmap
data:
  hostname: my-hostname

    Create a Pod with an init container that sets the hostname:

yaml

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  initContainers:
  - name: set-hostname
    image: busybox
    command: ["/bin/sh", "-c", "echo $(hostname) > /etc/hostname"]
    volumeMounts:
    - name: hostname-volume
      mountPath: /etc/hostname
  containers:
  - name: my-container
    image: my-image
    volumeMounts:
    - name: hostname-volume
      mountPath: /etc/hostname
    env:
    - name: MY_HOSTNAME
      valueFrom:
        configMapKeyRef:
          name: hostname-configmap
          key: hostname
  volumes:
  - name: hostname-volume
    configMap:
      name: hostname-configmap

In this example, the set-hostname init container runs the echo $(hostname) > /etc/hostname command to set the hostname to the value of the hostname of the container, which is passed to the MY_HOSTNAME environment variable of the main container using a ConfigMap. The volumeMounts and volumes sections ensure that the /etc/hostname file is mounted in both containers.

** What ports required for Kubelet?

Kubelet, the Kubernetes node agent, requires several ports to be open for communication with the Kubernetes control plane and other nodes in the cluster. Here are the default ports required by Kubelet:

    TCP port 10250: The Kubelet API server port. It is used for communication between the Kubelet and other Kubernetes components such as the kube-apiserver, kubelet's http endpoint, and kubelet's read-only port.

    TCP port 10255: The read-only port for the Kubelet's HTTP endpoint. It provides access to the health check, metrics, and log files for the Kubelet.

    TCP port 10256: The Kubelet's cadvisor port. It provides access to the container metrics for the Kubelet.

It's worth noting that these ports can be customized by passing command-line flags to the Kubelet or by setting configuration options in the Kubelet's configuration file. Also, some Kubernetes distributions may use different ports by default, so it's always a good idea to consult the documentation or vendor-specific guides for the specific distribution being used.

** How to deny specific query type, e.g. POST?

To deny a specific query type, such as POST, for an ingress resource in Kubernetes, you can use an Nginx configuration snippet that specifies the limit_except directive. Here is an example:

yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      limit_except GET HEAD {
        deny all;
      }
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /specific-path
            pathType: Prefix
            backend:
              service:
                name: example-service
                port:
                  name: http

In this example, the configuration-snippet annotation specifies a custom Nginx configuration snippet that uses the limit_except directive to deny all requests except for GET and HEAD for the specified location (in this case, /specific-path).

You can modify the example to deny other query types, such as PUT or DELETE, by adding them to the list inside the limit_except block:

yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      limit_except GET HEAD PUT DELETE {
        deny all;
      }
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /specific-path
            pathType: Prefix
            backend:
              service:
                name: example-service
                port:
                  name: http

In this modified example, the limit_except block specifies that requests for the /specific-path location must be either GET, HEAD, PUT, or DELETE. Any other type of request will be denied.

** What is CPU Requests Commitment?

CPU requests commitment is a concept in Kubernetes, which is a popular container orchestration platform used for deploying and managing containerized applications. In Kubernetes, each container that runs on a node can make resource requests, which specify the minimum amount of CPU and memory that the container requires to run properly.

CPU requests commitment refers to the amount of CPU resources that a Kubernetes node guarantees to a particular container over a certain period of time. When a container is created in Kubernetes, it is scheduled to run on a node that has sufficient available resources to meet the container's resource requests. Once the container is running, the node guarantees that the requested CPU resources will be available to the container for the duration of its runtime.

If a container exceeds its CPU requests commitment, Kubernetes may throttle the container's CPU usage or evict the container from the node altogether. This helps ensure that other containers on the node are not starved of resources and can operate properly.

Overall, CPU requests commitment is an important concept in Kubernetes for ensuring that containerized applications have the necessary resources to run smoothly, while also preventing resource contention and ensuring that other applications on the same node are not negatively affected.

** What is Memory Usage (WSS) in Kubernetes?

Memory usage (Working Set Size or WSS) in Kubernetes refers to the amount of memory currently being used by a container in a pod. It includes all the memory that a process has accessed recently and is therefore likely to need again in the near future.

In Kubernetes, the memory usage of a container is tracked by the kubelet, which periodically collects memory usage statistics from the container runtime and reports them to the Kubernetes API server. The Kubernetes API server can then use this information to make scheduling decisions and to enforce resource limits and requests.

Memory usage is an important metric for Kubernetes because it can affect the performance and stability of the entire cluster. If a container consumes too much memory, it can cause the node it's running on to become unstable, potentially leading to other pods on the same node being evicted or even causing the node to crash.

Therefore, monitoring and managing memory usage is an important part of optimizing the performance and stability of Kubernetes clusters. Kubernetes provides various tools for monitoring and managing resource usage, including resource quotas, resource limits, and container metrics exposed by kubelet.

** What does mean `Lost` status in PVC?

In Kubernetes, Lost status of a Persistent Volume Claim (PVC) indicates that Kubernetes has lost the connection with the underlying storage.

This can happen when the underlying storage is deleted or when the PVC's storage class is no longer available. When a PVC is in the Lost status, Kubernetes is no longer able to track the state of the underlying storage and the PVC cannot be used by any pods.

To fix this issue, you will need to determine the cause of the underlying storage becoming unavailable and address it accordingly. This may involve creating a new PVC, fixing the storage class or restoring the storage to its previous state. Once the underlying storage is available again, Kubernetes should be able to restore the PVC to its previous state.

** I need a serviceaccount which could modify nodes labels.

To allow a service account to modify node labels, you need to grant it the appropriate permissions using a ClusterRole and a ClusterRoleBinding. Here is an example of how to create a service account with the necessary permissions:

    First, create a file named node-labeler-clusterrole.yaml with the following contents:

yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: node-labeler
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch", "update"]

This ClusterRole grants permission to list and watch nodes, as well as get and update their labels.

    Next, create a file named node-labeler-clusterrolebinding.yaml with the following contents:

yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: node-labeler
subjects:
- kind: ServiceAccount
  name: node-labeler
  namespace: default
roleRef:
  kind: ClusterRole
  name: node-labeler
  apiGroup: rbac.authorization.k8s.io

This ClusterRoleBinding grants the node-labeler service account in the default namespace access to the node-labeler ClusterRole.

    Create a service account named node-labeler:

bash

kubectl create serviceaccount node-labeler

    Finally, apply the ClusterRole and ClusterRoleBinding to the cluster:

bash

kubectl apply -f node-labeler-clusterrole.yaml
kubectl apply -f node-labeler-clusterrolebinding.yaml

After following these steps, you can use the node-labeler service account to modify the labels of nodes in the cluster.

** How to expose statefulset as a service?

To expose a StatefulSet as a Service in Kubernetes, you can create a Headless Service. A Headless Service is a service that does not provide a stable IP address for accessing the pods. Instead, it returns the IP addresses of the individual pods directly.

Here's an example YAML definition of a Headless Service that exposes a StatefulSet:

yaml

apiVersion: v1
kind: Service
metadata:
  name: my-statefulset-service
  labels:
    app: my-statefulset
spec:
  clusterIP: None
  selector:
    app: my-statefulset
  ports:
    - name: http
      port: 80
      protocol: TCP

In the above example:

    The clusterIP field is set to None to create a Headless Service.
    The selector field selects the pods of the StatefulSet to expose.
    The ports field specifies the port to expose.

After applying this Service definition, you can access the individual pods of the StatefulSet directly using their DNS names, which will be in the following format:

typescript

$(podname).$(servicename).$(namespace).svc.cluster.local

Where:

    $(podname) is the name of the pod
    $(servicename) is the name of the Headless Service
    $(namespace) is the namespace of the StatefulSet and the Headless Service

For example, if your Headless Service is named my-statefulset-service in the default namespace, and the StatefulSet has two pods named my-statefulset-0 and my-statefulset-1, you can access them at the following DNS names:

arduino

my-statefulset-0.my-statefulset-service.default.svc.cluster.local
my-statefulset-1.my-statefulset-service.default.svc.cluster.local

You can use these DNS names to connect to the individual pods, for example, in your application code or in your other Kubernetes resources.

** How Kubernetes master decides that node is not available?

In Kubernetes, the master node determines whether a node is available or not by regularly sending heartbeats to nodes in the cluster. These heartbeats are known as "node status updates".

If a node fails to respond to multiple consecutive status update requests, the master node marks the node as "NotReady". The master then begins rescheduling any workloads that were running on the failed node onto other healthy nodes in the cluster.

Additionally, Kubernetes provides a number of mechanisms for detecting when a node becomes unavailable due to factors such as network failure or power outage. These mechanisms include node liveness probes, which periodically check whether an application running on a node is still responsive, and node readiness probes, which determine whether a node is ready to accept incoming traffic.

** Does a worker node check master is available in the same way?

Yes, worker nodes in a Kubernetes cluster also check whether the master node is available in a similar way. Worker nodes periodically send "NodeReady" status updates to the master node to indicate that they are still operational.

If the master node fails to respond to multiple consecutive NodeReady status update requests from a particular worker node, the worker node will mark the master node as "NotReady". However, this typically does not trigger any automatic actions since the master node is considered a critical component of the cluster infrastructure.

It's worth noting that while worker nodes can detect when the master node becomes unavailable, they do not have the ability to initiate any failover or recovery actions on their own. All decisions related to rescheduling workloads, scaling the cluster, and handling other types of failures are made by the master node.

** Is it safe to have Kubernetes nodes of a single cluster be in a different geographical places?

While it is technically possible to have Kubernetes nodes of a single cluster located in different geographical locations, it may not always be the best approach from a performance and reliability standpoint.

Having nodes located in different geographical locations can introduce additional network latency and increase the risk of communication failures. This can result in slower application response times and reduced availability of the cluster as a whole.

To mitigate these risks, you can implement measures such as using a highly available control plane with multiple replicas distributed across different data centers, configuring pod anti-affinity rules to ensure that related pods are not scheduled on nodes in different locations, and leveraging network topology aware scheduling to place pods closer to the users or services they interact with.

Ultimately, the decision to deploy Kubernetes nodes in different geographic locations should be based on factors such as the specific requirements of the application being deployed, the available infrastructure, and the overall goals and constraints of the organization.

** Does Kubernetes use mTLS for communication between the components?

Yes, Kubernetes uses mTLS (Mutual Transport Layer Security) to secure communication between its components. With mTLS, each component in the system has a unique certificate and private key that is used to encrypt and authenticate the traffic between them. This helps ensure that only authorized parties can access the data and services within the cluster. Additionally, Kubernetes also provides mechanisms for managing these certificates, such as rotating them periodically and revoking them if they are compromised.

** What mechanisms?

Kubernetes provides a few mechanisms for managing certificates used for mTLS:

    Certificate Authority (CA) - Kubernetes uses a root CA to sign and issue certificates for its components. This CA is typically managed by the cluster administrator, and is used to establish trust between the various components in the cluster.

    API Server Certificates - The API server is the central component of the Kubernetes control plane, and it uses SSL/TLS certificates to secure communication with the Kubernetes API. These certificates are also signed by the cluster's root CA.

    Node Certificates - Each node in the Kubernetes cluster has its own certificate and private key that is used for mTLS communication with the API server and other components. These certificates are also typically signed by the cluster's root CA.

    Service Account Tokens - Kubernetes uses service account tokens to authenticate requests between components within the cluster. These tokens are issued by the API server, and are signed by a keypair that is rotated periodically.

Overall, these mechanisms provide a secure foundation for communication within Kubernetes clusters, and help ensure that only authorized parties can access the data and services within the cluster.

** Does both mTLS participants should use same CA?

In general, both mTLS (mutual Transport Layer Security) participants do not necessarily need to use the same Certificate Authority (CA). Each participant can obtain their own certificate from a different CA, as long as they trust each other's CA. However, it is generally recommended for both participants to use certificates issued by the same trusted CA in order to ensure smooth and interoperable operation of the mTLS connection.

Using certificates from the same trusted CA helps to avoid potential issues that can arise when different CAs issue certificates with different policies or validation procedures. It can also simplify the management of the certificates, as there will be a single trusted root CA that needs to be configured on both sides.

Ultimately, the decision of whether to use the same CA or different CAs depends on the specific requirements of the mTLS deployment and the security policies in place.

** kube-controller-manager

Kube-controller-manager is one of the core components of Kubernetes, which runs on the control plane nodes and manages various controllers that regulate the state of the cluster. It is responsible for ensuring that the desired state of the cluster matches the actual state by monitoring and reconciling the state of various resources such as pods, services, endpoints, replicasets, and deployments.

The kube-controller-manager includes several controllers, such as the Replication Controller, Endpoints Controller, Namespace Controller, Service Account and Token Controllers, and many more. Each controller watches a specific set of resources and takes appropriate actions to maintain the desired state.

In addition to the built-in controllers, it is also possible to write custom controllers using the Kubernetes API that can manage custom resources or perform custom logic based on events in the cluster.

The kube-controller-manager is designed to be highly available, so multiple instances of the controller manager can be run in a cluster, each running on a different node. This ensures that if one node fails, another controller manager can take over its responsibilities and maintain the desired state of the cluster.

** ClusterFirstWithHostNet

ClusterFirstWithHostNet is a Kubernetes DNS policy that specifies how DNS queries should be resolved within a cluster. With this policy, DNS queries originating from containers in a pod are first sent to the cluster's DNS resolver, which then forwards the query to the node's DNS resolver if it cannot resolve the query itself. This allows containers to resolve DNS queries using both the cluster's DNS and the host's DNS.

The ClusterFirstWithHostNet policy is used when the pod's hostNetwork field is set to true. When this field is set, the pod shares the host's network namespace, which means it can access the host's DNS resolver.

It's important to note that using the ClusterFirstWithHostNet policy can pose security risks, as it allows pods to access the host's DNS resolver. Therefore, it should only be used when necessary and with caution

** How to configure sudo to become any user except root?

By default, the sudo command allows users to execute commands with the privileges of the root user. However, you can configure sudo to allow a user to become any other user except root by editing the /etc/sudoers file.

To do this, you need to add a line to the /etc/sudoers file that grants the necessary permissions. Here's an example of how to grant the bob user permission to run commands as any user except root:

bob ALL=(ALL,!root) ALL

This line allows the user bob to execute any command as any user except root. The !root part of the line specifies that the root user is excluded. The ALL at the end of the line allows the user to execute commands on all hosts and terminals.

It's important to note that editing the /etc/sudoers file should only be done with extreme caution, as incorrect syntax or permissions can cause serious system issues. It's recommended to use the visudo command to edit the file, as it performs syntax checks before committing changes.

** Ubuntu 14.04 kubelet version

The kubelet is not officially supported on Ubuntu 14.04 as it reached end-of-life in April 2019 and is no longer receiving security updates. However, if you still need to install Kubernetes on Ubuntu 14.04, the latest version of kubelet that you can install is v1.15.11. You can download the binary from the Kubernetes release page for v1.15.11 and install it manually. Keep in mind that using an outdated operating system is not recommended, as it may expose your system to vulnerabilities and other security risks.

** List kubelet health checks.

Kubelet is responsible for managing and monitoring the state of each node in a Kubernetes cluster. Here are some of the health checks that Kubelet performs:

    Docker health check: Kubelet checks if Docker is running and healthy.
    Node network check: Kubelet ensures that the node's IP address is reachable from other nodes in the cluster.
    Node disk space check: Kubelet monitors the amount of disk space available on the node and alerts if it falls below a certain threshold.
    Node memory check: Kubelet monitors the amount of memory available on the node and alerts if it falls below a certain threshold.
    Pod health check: Kubelet periodically checks the health of all pods running on the node and restarts any that have failed or become unresponsive.
    Kubelet API server check: Kubelet ensures that it can communicate with the Kubernetes API server and that the API server is healthy.

These health checks help ensure the overall health and availability of the Kubernetes cluster.

** kubernetes rbac readonly user for specific namespaces

To create a Kubernetes RBAC readonly user for specific namespaces, you would need to follow these steps:

    Create a new Kubernetes role that has read-only permissions for the resources you want to allow the user to access. For example, if you want the user to be able to read ConfigMaps, you could create a role with the following YAML definition:

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: readonly-role
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch

    Bind the new role to a new Kubernetes role binding that maps the role to the user or group you want to grant access to. For example, if you want to grant access to a user named "readonly-user" in the "mynamespace" namespace, you could create a role binding with the following YAML definition:

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: readonly-role-binding
  namespace: mynamespace
subjects:
- kind: User
  name: readonly-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: readonly-role
  apiGroup: rbac.authorization.k8s.io

This binds the "readonly-role" to the "readonly-user" in the "mynamespace" namespace.

    Repeat this process for any additional namespaces you want to grant read-only access to.

Note that this setup only provides read-only access to specific resources in the designated namespaces. If you want to restrict access further, you may need to create additional roles and bindings with more specific permissions as needed.

** In what Kubernetes namespace does Helm install chart dependencies charts?

When Helm installs a chart, including its dependencies, it creates a new release in a specific Kubernetes namespace. By default, Helm installs the chart and its dependencies in the same namespace specified during the installation process.

However, it's important to note that Helm behaves differently based on the Helm version and the specific chart dependency management method being used. Here are the two common scenarios:

1. Helm v2: In Helm v2, the Chart.yaml file of the chart being installed can specify dependencies. When installing a chart with dependencies, Helm will install all the charts in the same namespace as specified in the =--namespace=  flag during the installation command. If no namespace is provided, Helm will install the chart and its dependencies in the namespace set as the current context.
  
2. Helm v3: Helm v3 introduces a significant change in how chart dependencies are managed. In Helm v3, chart dependencies are managed through a separate file called =requirements.yaml=. When you install a chart with dependencies using Helm v3, it creates a new release for each chart, and each release is installed into the same namespace as specified during the installation command using the =--namespace=  flag. If no namespace is provided, Helm will install the chart and its dependencies in the namespace set as the current context.
  

In both Helm v2 and Helm v3, you have control over which namespace the chart and its dependencies are installed into by specifying the desired namespace during the installation command using the =--namespace=  flag.
