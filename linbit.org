- [[https://vitobotta.com/2019/08/07/linstor-storage-with-kubernetes/][Linstor storage with Kubernetes]]

* Learning
** [[https://docs.piraeus.daocloud.io/books/linstor-10-user-guide/page/212-adding-and-removing-disks][2.12. Adding and remov... | Piraeus]]
2.12.1. Migrating disks

In order to move a resource between nodes without reducing redundancy at any point, LINSTOR’s disk migrate feature can be used. First create a diskless resource on the target node, and then add a disk using the --migrate-from option. This will wait until the data has been synced to the new disk and then remove the source disk.

For example, to migrate a resource backups from ‘alpha’ to ‘bravo’:

: linstor resource create bravo backups --drbd-diskless
: linstor resource toggle-disk bravo backups --storage-pool pool_ssd --migrate-from alpha

** [[https://deckhouse.io/documentation/v1/modules/041-linstor/faq.html][The linstor module: FAQ | Deckhouse]]
The linstor module: FAQ

    What is difference between LVM and LVMThin?
    Performance and reliability notes, comparison to Ceph
    What to use in which situation?
    Changing the default StorageClass
    How to add existing LVM or LVMThin pool?
    How to configure Prometheus to use LINSTOR for storing data?
    linstor-node cannot start because the drbd module cannot be loaded
    How to evict resources from a node?
    Pod cannot start with the FailedMount error
        Pod is stuck in the ContainerCreating phase
        Pod cannot start due to missing CSI driver
        Errors like Input/output error

This feature is actively developed. It might significantly change in the future.
What is difference between LVM and LVMThin?

Briefly:

    LVM is simpler and has performance comparable to native drives;
    LVMThin allows you to use snapshots and overprovisioning, but twice as slow.

Performance and reliability notes, comparison to Ceph

    You may be interested in our article “Comparing Ceph, LINSTOR, Mayastor, and Vitastor storage performance in Kubernetes”.

We take a practical view of the issue. A difference of several tens of percent — in practice it never matters. The difference is several times or more important.

Comparison factors:

    Sequential read and write: do not matter, because on any technology they always run into the network (which is 10Gb/s, which is 1Gb/s). From a practical point of view, this indicator can be completely ignored;
    Random read and write (which is 1Gb/s, which is 10Gb/s):
        DRBD + LVM 5 times better (latency — 5 times less, IOPS — 5 times more) than Ceph RBD;
        DRBD + LVM is 2 times better than DRBD + LVMThin.
    If one of the replicas is located on local storage, then the read speed will be approximately equal to the storage device speed;
    If there are no replicas located on local storage, then the write speed will be approximately equal to half the network bandwidth for two replicas, or ⅓ network bandwidth for three replicas;
    With a large number of clients (more than 10, with iodepth 64), Ceph starts to fall behind more (up to 10 times) and consume much more CPU.

All in all, in practice, it doesn’t matter how many knobs you have for tuning, only three factors are significant:

    Read locality — if all reading is performed locally, then it works at the speed (throughput, IOPS, latency) of the local disk (the difference is practically insignificant);
    1 network hop when writing — in DRBD, the replication is performed by the client, and in Ceph, by server, so Ceph latency for writing always has at least x2 from DRBD;
    Complexity of code — latency of calculations on the datapath (how much assembler code is executed for each io operation), DRBD + LVM is simpler than DRBD + LVMThin, and much simpler than Ceph RBD.

What to use in which situation?

By default, we use two replicas (the third is an automatically created diskless replica used for quorum). This approach guarantees protection against split-brain and a sufficient level of storage reliability, but the following features must be taken into account:

    When one of the replicas (replica A) is unavailable, data is written only to a single replica (replica B). It means that:
        If at this moment the second replica (replica B) is also turned off, writing and reading will be unavailable;
        If at the same time the second replica (replica B) is irretrievably lost, then the data will be partially lost (there is only the old, outdated replica A);
        If the old replica (replica A) was also irretrievably lost, the data will be completely lost.
    When the second replica is turned off, in order to turn it back on (without operator intervention), both replicas must be available (in order to correctly work out the split-brain);
    Enabling a third replica solves both problems (at least two copies of data at any given time), but increases the overhead (network, disk).

It is strongly recommended to have one replica locally. This doubles the possible write bandwidth (with two replicas) and significantly increases the read speed. But if this is not the case, then everything still continues to work normally (but reading over the network, and double network utilization for writing).

Depending on the task, choose one of the following:

    DRBD + LVM — faster (x2) and more reliable (LVM is simpler);
    DRBD + LVMThin — support for snapshots and the possibility of overcommitment.

Changing the default StorageClass

List the StorageClasses in your cluster:

kubectl get storageclass

Mark the default StorageClass as non-default:

kubectl annotate storageclass local-path storageclass.kubernetes.io/is-default-class-

Mark a StorageClass as default:

kubectl annotate storageclass linstor-data-r2 storageclass.kubernetes.io/is-default-class=true

How to add existing LVM or LVMThin pool?

    The general method is described in`LINSTOR storage configuration page. Unlike commands listed below it will automatically configure the StorageClasses as well.

Example of adding an existing LVM pool:

linstor storage-pool create lvm node01 lvmthin linstor_data

Example of adding an existing LVMThin pool:

linstor storage-pool create lvmthin node01 lvmthin linstor_data/data

You can also add pools with some volumes have already been created. LINSTOR will just create new ones nearby.
How to configure Prometheus to use LINSTOR for storing data?

To configure Prometheus to use LINSTOR for storing data:

    Configure storage-pools and StorageClass;

    Specify the longtermStorageClass and storageClass parameters in the prometheus module configuration. E.g.:

    Example:

    prometheus: |
      longtermStorageClass: linstor-data-r2
      storageClass: linstor-data-r2

    Wait for the restart of Prometheus Pods.

linstor-node cannot start because the drbd module cannot be loaded

Check the status of the linstor-node Pods:

kubectl get pod -n d8-linstor -l app.kubernetes.io/instance=linstor,\
app.kubernetes.io/managed-by=piraeus-operator,app.kubernetes.io/name=piraeus-node

If you see that some of them get stuck in Init:CrashLoopBackOff state, check the logs of kernel-module-injector container:

kubectl logs -n d8-linstor linstor-node-xxwf9 -c kernel-module-injector

The most likely reasons why it cannot load the kernel module:

    You may already have an in-tree kernel version of the DRBDv8 module loaded when LINSTOR requires DRBDv9. Check loaded module version: cat /proc/drbd. If the file is missing, then the module is not loaded and this is not your case.

    You have Secure Boot enabled. Since the DRBD module we provide is compiled dynamically for your kernel (similar to dkms), it has no digital sign. We do not currently support running the DRBD module with a Secure Boot configuration.

How to evict resources from a node?

To do this, just run the command:

linstor node evacuate <node_name>

It will move resources to other free nodes and replicate them.
Pod cannot start with the FailedMount error
Pod is stuck in the ContainerCreating phase

If the Pod is stuck in the ContainerCreating phase, and you see the following errors in kubectl describe pod:

rpc error: code = Internal desc = NodePublishVolume failed for pvc-b3e51b8a-9733-4d9a-bf34-84e0fee3168d: checking
for exclusive open failed: wrong medium type, check device health

… it means that device is still mounted on one of the other nodes.

To check it, use the following command:

linstor resource list -r pvc-b3e51b8a-9733-4d9a-bf34-84e0fee3168d

The InUse flag will indicate which node the device is being used on.
Pod cannot start due to missing CSI driver

An example error in kubectl describe pod:

kubernetes.io/csi: attachment for pvc-be5f1991-e0f8-49e1-80c5-ad1174d10023 failed: CSINode b-node0 does not
contain driver linstor.csi.linbit.com

Check the status of the linstor-csi-node Pods:

kubectl get pod -n d8-linstor -l app.kubernetes.io/component=csi-node,app.kubernetes.io/instance=linstor,\
app.kubernetes.io/managed-by=piraeus-operator,app.kubernetes.io/name=piraeus-csi

Most likely they are stuck in the Init state, waiting for the node to change its status to Online in LINSTOR. Run the following command to check the list of nodes:

linstor node list

If you see any nodes in the EVICTED state, then they have been unavailable for 2 hours, to return them to the cluster, run:

linstor node rst <name>

Errors like Input/output error

Such errors usually occur at the stage of creating the file system (mkfs).

Check dmesg on the node where your Pod is running:

dmesg | grep 'Remote failed to finish a request within'

If you get any output (there are lines with the “Remote failed to finish a request within …” parts in the dmesg output), then most likely, your disk subsystem is too slow for the normal functioning of DRBD.

* Cheat sheet

- list volumes
  : linstor volume list --all

- delete volume
  : linstor volume-definition delete pvc-0d2864b4-a71e-4073-b132-a58875433a75 0

- list volume definitions
  : linstor volume-definition list

- list resource groups
  : linstor resource-group list

- list resources
  : linstor resource list

- list resource definitions
  : linstor resource-definition list

- manually create lvm thin volume
  : lvcreate -V 14G --thin -n pvc-2923a7b0-20c9-4676-bdcc-5998196980dc_00000 vg0/pool0

- drbd
  : kubectl exec -n piraeus -it pod/piraeus-piraeus-op-ns-node-gb756 -- /bin/bash

- show error report
  : linstor error-reports show 63A7A9DF-F3736-000318

- list nodes
  : linstor node list

- drbd status
#+begin_example
  root@kube1:/# drbdadm status
  pvc-bfd7e627-5114-4130-b0e3-15d97ce38106 role:Secondary
    disk:UpToDate
    kube2 role:Secondary
      peer-disk:UpToDate
    kube7 role:Primary
      peer-disk:Diskless
#+end_example

- list pools
  : storage-pool list

- show resource
  : linstor r l -r pvc-64fe679b-b317-482e-a922-5058921c88e8

- LINSTOR provides various commands to check the state of your cluster. These
  commands start with a ‘list-‘ prefix and provide various filtering and
  sorting options. The ‘–groupby’ option can be used to group and sort the
  output in multiple dimensions.
  : linstor node list
  : linstor storage-pool list --groupby Size

* Restore DRBD
** 
root@kube2:/# drbdadm status
pvc-e5750c31-d73d-48e0-9b70-a03fc492e41f role:Secondary
  disk:Inconsistent
  kube1 role:Secondary
    peer-disk:UpToDate
  kube6 connection:Connecting

pvc-ee82abb3-06bc-41be-9e09-3894cab9fd38 role:Secondary
  disk:Inconsistent
  kube1 role:Secondary
    peer-disk:UpToDate
  kube8 connection:Connecting

pvc-f12a4435-c5af-43b0-943b-b43302964354 role:Secondary
  disk:Inconsistent
  kube1 role:Secondary
    peer-disk:UpToDate
  kube6 connection:Connecting

** 
root@kube2:/# drbdadm -- disconnect all
root@kube2:/# drbdadm status
pvc-e5750c31-d73d-48e0-9b70-a03fc492e41f role:Secondary
  disk:Inconsistent quorum:no
  kube1 connection:StandAlone
  kube6 connection:StandAlone

pvc-ee82abb3-06bc-41be-9e09-3894cab9fd38 role:Secondary
  disk:Inconsistent quorum:no
  kube1 connection:StandAlone
  kube8 connection:StandAlone

pvc-f12a4435-c5af-43b0-943b-b43302964354 role:Secondary
  disk:Inconsistent quorum:no
  kube1 connection:StandAlone
  kube6 connection:StandAlone

** 
drbdadm -- --discard-my-data connect all

* Learning

- [[https://www.admin-magazine.com/Articles/Storage-cluster-management-with-LINSTOR/(offset)/3][LINSTOR » ADMIN Magazine]]
- [[https://pub.nethence.com/storage/drbd-linstor][DRBD9 and LINSTOR the easy way]]

** [[https://vitobotta.com/2019/08/07/linstor-storage-with-kubernetes/][Linstor storage with Kubernetes]]

Linstor storage with Kubernetes
Published Wednesday, Aug 07 2019
Jump to comments

In the previous post I shared some notes and benchmarks for a number of storage products for Kubernetes, both free and paid. I also mentioned that I had basically given up on Kubernetes because of various problems with these storage solutions. However a reader suggested I also try Linstor, yet another open source solution with optional paid support I had never heard of. Because of the various issues experienced with the others I was kinda skeptical, but after trying it I must say I like it! It’s fast and replication based on DRBD works very well. I only had one issue (so far) with volumes not detaching correctly with the 0.6.4 version of the CSI plugin, but the developer promptly made a new version (0.7.0) available that seems to have fixed it. I wish Linstor had off site backups based on snapshots… but other than that it’s looking good, so there might still be hope for me with Kubernetes after all… will keep testing and see how it goes. I really hope I won’t find any other issues with it and that I can actually use it and forget about Heroku!

Anyway the documentation is vast and while it’s good, I found some places that seemed out of date, so I thought a post on how to quickly install and use Linstor on Kubernetes might be useful. I assume that you just want the storage set up and configure the CSI support in Kubernetes so that you can dynamically provision volumes with a storage class. Also, the instructions below are for Ubuntu; I tried with CentOS but it seems that while the DRBD kernel module is available for this distro, other packages are not available. Linbit (the company behind Linstor) makes these packages available with a ppa repository, so I got it working on Ubuntu.

Please refer to the Linstor documentation for detailed information on the technology. There’s also a public mailing list where you can ask questions.
Installation on Ubuntu

In my case I have set up a test cluster made of three nodes with a 100GB disk each and connected via a Wireguard VPN, so that all the traffic between the nodes is securely encrypted. This impacts a little on the performance, but while my cloud provider (Hetzner Cloud) now offers private networking, they still recommend encrypting the traffic for sensitive data. The VPN is set up so that the nodes are named linstor-master1, linstor-maste2 and linstor-maste3, and have IPs 192.168.37.1, 192.168.37.2 and 192.168.37.3 respectively. Of course you’ll have to adapt the instructions to your setup.

The first step is to install the kernel headers since the DRBD replication is based on a kernel module that must be built on all the nodes for it work:

apt-get install linux-headers-$(uname -r)

Next you need to add the ppa repository:

add-apt-repository ppa:linbit/linbit-drbd9-stack
apt-get update

On all the nodes you need to install the following packages:

apt install drbd-utils drbd-dkms lvm2

Load the DRBD kernel module:

modprobe drbd

Double check that it is loaded:

lsmod | grep -i drbd

and make sure it is loaded at startup automatically:

echo drbd > /etc/modules-load.d/drbd.conf

A Linstor cluster consists of one active controller, which manages all the information about the cluster, and satellites, that is the nodes that provide storage. On the node that is going to be the controller run:

apt install linstor-controller linstor-satellite  linstor-client

The above will make the controller a satellite as well. In my case the controller is linstor-master1. To start the controller right away and ensure it is started at boot automatically, run:

systemctl enable linstor-controller
systemctl start linstor-controller

On the remaining nodes/satellites, install the following packages:

apt install linstor-satellite  linstor-client

Then start the satellite and ensure it is started at boot:

systemctl enable  linstor-satellite
systemctl start linstor-satellite

Back on the controller, you can now add the satellites, including this node itself:

linstor node create linstor-master1 192.168.37.1
linstor node create linstor-master2 192.168.37.2
linstor node create linstor-master3 192.168.37.3

Give it a few seconds, then check that the nodes are online:

You’ll see something like this:

╭──────────────────────────────────────────────────────────────────╮
┊ Node            ┊ NodeType  ┊ Addresses                 ┊ State  ┊
╞┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄╡
┊ linstor-master1 ┊ SATELLITE ┊ 192.168.37.1:3366 (PLAIN) ┊ Online ┊
┊ linstor-master2 ┊ SATELLITE ┊ 192.168.37.2:3366 (PLAIN) ┊ Online ┊
┊ linstor-master3 ┊ SATELLITE ┊ 192.168.37.3:3366 (PLAIN) ┊ Online ┊
╰──────────────────────────────────────────────────────────────────╯

Next you need to set up the storage. Linstor works with either LVM or ZFS under the hood to manage the storage; not sure of the differences but I am more familiar with LVM so that’s what I’ll use.

First, prepare the physical disk or disks on eanch node - in my case it’s /dev/sdb:

pvcreate /dev/sdb

Create a volume group:

I call the volume group “vg” but you can call it whatever you wish.

Now create a “thin” pool, which will enable both thin provisioning (i.e. the ability to create volumes bigger than the actual storage available, so you can then add storage as needed) and snapshots:

lvcreate -l 100%FREE  --thinpool vg/lvmthinpool

The command above will create a logical volume that spans the entire disk.

It’s time to create a storage pool on each node, so back on the controller run:

linstor storage-pool create lvmthin linstor-master1 linstor-pool vg/lvmthinpool
linstor storage-pool create lvmthin linstor-master2 linstor-pool vg/lvmthinpool
linstor storage-pool create lvmthin linstor-master3 linstor-pool vg/lvmthinpool

I am calling the pool “linstor-pool”. Check that the pools have been created:

linstor storage-pool list

You’ll see something like this:

╭───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
┊ StoragePool  ┊ Node            ┊ Driver   ┊ PoolName       ┊ FreeCapacity ┊ TotalCapacity ┊ SupportsSnapshots ┊ State ┊
╞┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄╡
┊ linstor-pool ┊ linstor-master1 ┊ LVM_THIN ┊ vg/lvmthinpool ┊    99.80 GiB ┊     99.80 GiB ┊ true              ┊ Ok    ┊
┊ linstor-pool ┊ linstor-master2 ┊ LVM_THIN ┊ vg/lvmthinpool ┊    99.80 GiB ┊     99.80 GiB ┊ true              ┊ Ok    ┊
┊ linstor-pool ┊ linstor-master3 ┊ LVM_THIN ┊ vg/lvmthinpool ┊    99.80 GiB ┊     99.80 GiB ┊ true              ┊ Ok    ┊
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

At this point the main setup of Linstor is complete.
Kubernetes

To enable Kubernetes to dynamically provision volumes, you’ll need to install the CSI plugin and create a storage class. At the moment of this writing the latest version is 0.7.0, but check here what is the latest image available.

Run the following to install:

TAG=v0.7.0
CONTROLLER_IP=192.168.37.1

curl https://raw.githubusercontent.com/LINBIT/linstor-csi/$TAG/examples/k8s/deploy/linstor-csi-1.14.yaml | sed "s/linstor-controller.example.com/$CONTROLLER_IP/g" | kubectl apply -f -

Of course change the tag with your version and the controller IP with the iP of your controller. Wait that the pods are up and running:

watch kubectl -n kube-system get all

The final step for the installation is the storage class:

REPLICAS=3

cat <<EOF | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: linstor
provisioner: linstor.csi.linbit.com
parameters:
  autoPlace: "$REPLICAS"
  storagePool: "linstor-pool"
EOF 

Set the number of replicas to the number of nodes. autoPlace ensures that the volumes are automatically placed/distributed across the nodes/pools.

Finally, to test that the provisioning is working, create a pvc:

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  storageClassName: linstor
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
EOF

kubectl get pvc

If all is good, in a few seconds you’ll see that the pvc is bound:

NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
test-pvc   Bound    pvc-af6991ee-b922-11e9-bbca-9600002d2434   1Gi        RWO            linstor        10s

You can check on the controller with Linstor as well by running:

linstor volume list

You’ll see something like this:

╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
┊ Node            ┊ Resource                                 ┊ StoragePool  ┊ VolumeNr ┊ MinorNr ┊ DeviceName    ┊ Allocated ┊ InUse  ┊    State ┊
╞┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄╡
┊ linstor-master1 ┊ pvc-a8d679a9-b918-11e9-bbca-9600002d2434 ┊ linstor-pool ┊ 0        ┊ 1001    ┊ /dev/drbd1001 ┊ 1.00 GiB  ┊ Unused ┊ UpToDate ┊
┊ linstor-master2 ┊ pvc-a8d679a9-b918-11e9-bbca-9600002d2434 ┊ linstor-pool ┊ 0        ┊ 1001    ┊ /dev/drbd1001 ┊ 1.00 GiB  ┊ Unused ┊ UpToDate ┊
┊ linstor-master3 ┊ pvc-a8d679a9-b918-11e9-bbca-9600002d2434 ┊ linstor-pool ┊ 0        ┊ 1001    ┊ /dev/drbd1001 ┊ 1.00 GiB  ┊ Unused ┊ UpToDate ┊
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Since you are at it, you may also want to run a simple benchmark to see how the setup performs by creating a pvc and a job:

cat <<EOF | kubectl apply -f -
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: dbench-linstor
spec:
  storageClassName: linstor
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: batch/v1
kind: Job
metadata:
  name: dbench-linstor
spec:
  template:
    spec:
      containers:
      - name: dbench
        image: sotoaster/dbench:latest
        imagePullPolicy: IfNotPresent
        env:
          - name: DBENCH_MOUNTPOINT
            value: /data
          - name: FIO_SIZE
            value: 1G
        volumeMounts:
        - name: dbench-pv
          mountPath: /data
      restartPolicy: Never
      volumes:
      - name: dbench-pv
        persistentVolumeClaim:
          claimName: dbench-linstor
  backoffLimit: 4
EOF

Wait for the job pod to be ready and then check the logs with:

You’ll see something like this at the end:

==================
= Dbench Summary =
==================
Random Read/Write IOPS: 7495/4468. BW: 300MiB/s / 68.4MiB/s
Average Latency (usec) Read/Write: 945.99/
Sequential Read/Write: 301MiB/s / 62.6MiB/s
Mixed Random Read/Write IOPS: 7214/2401

In my case every metric is identical to what I get when benchmarking the disk directly, apart from the write spees, which are lower due to replication and VPN encryption. Otherwise they would be identical as well. Linstor really has no overhead basically, and that’s great.
Conclusion

Setting up Linstor may not be as straightforward as applying one yaml or two like with most of its competitors, but the setup is not difficult at all and can be automated with Ansible etc. So far I’ve only found that single issue which has already been fixed, so like I said I hope I won’t find any others. I would still like to use self-managed Kubernetes instead of Heroku. Hope this post was useful and saved you some time.

* Kubernetes

** Storage pool creation

cfdisk /dev/vdb
pvcreate /dev/vdb2
vgcreate vg0 /dev/vdb2
lvcreate -l 100%FREE -Zn --type thin-pool --thinpool pool0 vg0

linstor storage-pool create lvmthin kube10 linstor-pool vg0/pool0
linstor storage-pool create lvmthin kube2 pool0 vg0/pool0

* Backup

- [[https://github.com/kvaps/linstor-backup-script][kvaps/linstor-backup-script: Script for export linstor configuration as simple commands]]

* Kubernetes

** benchmark

#+begin_src yaml
  kind: PersistentVolumeClaim
  apiVersion: v1
  metadata:
    name: dbench-linstor
  spec:
    storageClassName: linstor-pool0
    accessModes:
      - ReadWriteMany
    resources:
      requests:
        storage: 5Gi
  ---
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: dbench-linstor
  spec:
    template:
      spec:
        tolerations:
          - operator: Exists
        # nodeSelector:
        #   kubernetes.io/hostname: kube2
        containers:
        - name: dbench
          image: sotoaster/dbench:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: DBENCH_MOUNTPOINT
              value: /data
            - name: FIO_SIZE
              value: 1G
          volumeMounts:
          - name: dbench-pv
            mountPath: /data
        restartPolicy: Never
        volumes:
        - name: dbench-pv
          persistentVolumeClaim:
            claimName: dbench-linstor
    backoffLimit: 4
#+end_src

** snapshots

#+begin_src yaml
  kind: PersistentVolumeClaim
  apiVersion: v1
  metadata:
    name: bash-linstor
  spec:
    storageClassName: linstor-pool0
    accessModes:
      - ReadWriteMany
    resources:
      requests:
        storage: 5Gi
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: bash-linstor
  spec:
    selector:
      matchLabels:
        app: bash-linstor
    template:
      metadata:
        labels:
          app: bash-linstor
      spec:
        containers:
        - name: bash
          image: nixery.dev/shell/coreutils
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: bash-pv
            mountPath: /data
          command: ["/bin/sh"]
          args: ["-c", "while true; do sleep 60; done"]
        volumes:
        - name: bash-pv
          persistentVolumeClaim:
            claimName: bash-linstor
#+end_src

#+begin_src yaml
  apiVersion: snapshot.storage.k8s.io/v1
  kind: VolumeSnapshot
  metadata:
    name: my-first-linstor-snapshot
    namespace: linstor-bash
  spec:
    volumeSnapshotClassName: linstor0
    source:
      persistentVolumeClaimName: bash-linstor
#+end_src

#+begin_src yaml
  kind: PersistentVolumeClaim
  apiVersion: v1
  metadata:
    name: bash-linstor-restored
  spec:
    storageClassName: linstor-pool0
    dataSource:
      name: my-first-linstor-snapshot
      kind: VolumeSnapshot
      apiGroup: snapshot.storage.k8s.io
    accessModes:
      - ReadWriteMany
    resources:
      requests:
        storage: 5Gi
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: bash-linstor-restored
  spec:
    selector:
      matchLabels:
        app: bash-linstor-restored
    template:
      metadata:
        labels:
          app: bash-linstor-restored
      spec:
        containers:
        - name: bash
          image: nixery.dev/shell/coreutils
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: bash-pv
            mountPath: /data
          command: ["/bin/sh"]
          args: ["-c", "while true; do sleep 60; done"]
        volumes:
        - name: bash-pv
          persistentVolumeClaim:
            claimName: bash-linstor-restored
#+end_src
