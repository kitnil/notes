- [[https://vitobotta.com/2019/08/07/linstor-storage-with-kubernetes/][Linstor storage with Kubernetes]]

* Learning
- [[https://deckhouse.io/documentation/v1/modules/041-linstor/faq.html][The linstor module: FAQ | Deckhouse]]

* Cheat sheet

- list volumes
  : linstor volume list --all

- delete volume
  : linstor volume-definition delete pvc-0d2864b4-a71e-4073-b132-a58875433a75 0

- list volume definitions
  : linstor volume-definition list

- list resource groups
  : linstor resource-group list

- list resources
  : linstor resource list

- list resource definitions
  : linstor resource-definition list

- manually create lvm thin volume
  : lvcreate -V 14G --thin -n pvc-2923a7b0-20c9-4676-bdcc-5998196980dc_00000 vg0/pool0

- drbd
  : kubectl exec -n piraeus -it pod/piraeus-piraeus-op-ns-node-gb756 -- /bin/bash

- show error report
  : linstor error-reports show 63A7A9DF-F3736-000318

- list nodes
  : linstor node list

- drbd status
#+begin_example
  root@kube1:/# drbdadm status
  pvc-bfd7e627-5114-4130-b0e3-15d97ce38106 role:Secondary
    disk:UpToDate
    kube2 role:Secondary
      peer-disk:UpToDate
    kube7 role:Primary
      peer-disk:Diskless
#+end_example

* Restore DRBD
** 
root@kube2:/# drbdadm status
pvc-e5750c31-d73d-48e0-9b70-a03fc492e41f role:Secondary
  disk:Inconsistent
  kube1 role:Secondary
    peer-disk:UpToDate
  kube6 connection:Connecting

pvc-ee82abb3-06bc-41be-9e09-3894cab9fd38 role:Secondary
  disk:Inconsistent
  kube1 role:Secondary
    peer-disk:UpToDate
  kube8 connection:Connecting

pvc-f12a4435-c5af-43b0-943b-b43302964354 role:Secondary
  disk:Inconsistent
  kube1 role:Secondary
    peer-disk:UpToDate
  kube6 connection:Connecting

** 
root@kube2:/# drbdadm -- disconnect all
root@kube2:/# drbdadm status
pvc-e5750c31-d73d-48e0-9b70-a03fc492e41f role:Secondary
  disk:Inconsistent quorum:no
  kube1 connection:StandAlone
  kube6 connection:StandAlone

pvc-ee82abb3-06bc-41be-9e09-3894cab9fd38 role:Secondary
  disk:Inconsistent quorum:no
  kube1 connection:StandAlone
  kube8 connection:StandAlone

pvc-f12a4435-c5af-43b0-943b-b43302964354 role:Secondary
  disk:Inconsistent quorum:no
  kube1 connection:StandAlone
  kube6 connection:StandAlone

** 
drbdadm -- --discard-my-data connect all
